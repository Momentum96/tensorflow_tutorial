{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 그래프 외부에 출력\n",
    "%matplotlib tk\n",
    "\n",
    "tf.set_random_seed(777)  # 어느 컴퓨터에서 이 코드를 실행해도 학습 방향이 같도록, 다시 수행해도 같도록\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3564, 4, 5)\n",
      "(3564, 1)\n"
     ]
    }
   ],
   "source": [
    "# train Parameters\n",
    "seq_length = 4\n",
    "data_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 892\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1)) # 데이터 일반화\n",
    "scaler2 = MinMaxScaler(feature_range=(0, 1)) # 데이터 일반화\n",
    "\n",
    "# xy = np.loadtxt('./v3data/train_v3_data_cct.csv', delimiter=',')\n",
    "# cct, cas_swr, 446to477, uvb, temp, ptmt (892행 5열)\n",
    "xy = np.loadtxt('./v3data/train_v3_data.csv', delimiter=',')\n",
    "x = scaler.fit_transform(xy[:, 0:-1]) # x = 맨 마지막 ptmt 제외 모든 것\n",
    "y = scaler2.fit_transform(xy[:, [-1]])  # y = ptmt\n",
    "\n",
    "# build a dataset\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "# print(len(x))\n",
    "# print(len(y))\n",
    "# print(x[0])\n",
    "# print(y[0])\n",
    "\n",
    "for i in range(0, len(x) - seq_length): # 한 행씩 dataX, Y에 추가\n",
    "    _x = x[i:i + seq_length]\n",
    "    _y = y[i + seq_length]  # Next close price\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "#     print(np.shape(_x))\n",
    "\n",
    "print(np.shape(dataX))\n",
    "print(np.shape(dataY))\n",
    "\n",
    "data = '2018-03-31'\n",
    "# xy2 = np.loadtxt('./v3data/'+ data +'.csv',delimiter=',')\n",
    "# xy2 = np.loadtxt('./v3data/test_v3_data_cct_180331.csv', delimiter=',')\n",
    "xy2 = np.loadtxt('./v3data/test_v3_data_180331.csv', delimiter=',') # 예측할 날짜\n",
    "\n",
    "# x2 = scaler.fit_transform(xy2) # 데이터 없을때 (데이터란 ptmt 값)\n",
    "x2 = scaler.fit_transform(xy2[:, 0:-1]) # 데이터 있을때\n",
    "y2 = scaler2.fit_transform(xy2[:, [-1]]) #데이터 있을때\n",
    "\n",
    "# print(np.shape(x2))\n",
    "\n",
    "dataX2 = []\n",
    "dataY2 = [] #데이터 있을때\n",
    "\n",
    "for i in range(0, len(x2) - seq_length):\n",
    "    _x2 = x2[i:i+seq_length]\n",
    "    _y2 = y2[i+seq_length] #데이터 있을때\n",
    "    dataX2.append(_x2)\n",
    "    dataY2.append(_y2) #데이터 있을때\n",
    "# train/test split\n",
    "\n",
    "train_size = len(dataX)\n",
    "test_size = len(dataX2)\n",
    "\n",
    "# print(train_size)\n",
    "# print(test_size)\n",
    "\n",
    "trainX, testX = np.array(dataX[0:train_size]),np.array(dataX2[0:test_size])\n",
    "trainY = np.array(dataY[0:train_size])\n",
    "\n",
    "# print(np.shape(trainX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gw/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 2322.46142578125\n",
      "[step: 1] loss: 1800.5382080078125\n",
      "[step: 2] loss: 1398.5225830078125\n",
      "[step: 3] loss: 1089.4876708984375\n",
      "[step: 4] loss: 853.081298828125\n",
      "[step: 5] loss: 674.7769165039062\n",
      "[step: 6] loss: 544.2267456054688\n",
      "[step: 7] loss: 454.0190124511719\n",
      "[step: 8] loss: 398.63165283203125\n",
      "[step: 9] loss: 373.297607421875\n",
      "[step: 10] loss: 372.6789245605469\n",
      "[step: 11] loss: 389.4452819824219\n",
      "[step: 12] loss: 413.897705078125\n",
      "[step: 13] loss: 436.25946044921875\n",
      "[step: 14] loss: 449.849365234375\n",
      "[step: 15] loss: 452.1983642578125\n",
      "[step: 16] loss: 444.20489501953125\n",
      "[step: 17] loss: 428.6481018066406\n",
      "[step: 18] loss: 408.8843688964844\n",
      "[step: 19] loss: 387.9869079589844\n",
      "[step: 20] loss: 368.3077087402344\n",
      "[step: 21] loss: 351.3487548828125\n",
      "[step: 22] loss: 337.8218078613281\n",
      "[step: 23] loss: 327.8025817871094\n",
      "[step: 24] loss: 320.9155578613281\n",
      "[step: 25] loss: 316.5062561035156\n",
      "[step: 26] loss: 313.785888671875\n",
      "[step: 27] loss: 311.9421691894531\n",
      "[step: 28] loss: 310.22216796875\n",
      "[step: 29] loss: 307.99420166015625\n",
      "[step: 30] loss: 304.7916259765625\n",
      "[step: 31] loss: 300.34222412109375\n",
      "[step: 32] loss: 294.5797119140625\n",
      "[step: 33] loss: 287.6343688964844\n",
      "[step: 34] loss: 279.80572509765625\n",
      "[step: 35] loss: 271.5175476074219\n",
      "[step: 36] loss: 263.2586975097656\n",
      "[step: 37] loss: 255.51519775390625\n",
      "[step: 38] loss: 248.69700622558594\n",
      "[step: 39] loss: 243.06776428222656\n",
      "[step: 40] loss: 238.68728637695312\n",
      "[step: 41] loss: 235.38417053222656\n",
      "[step: 42] loss: 232.77694702148438\n",
      "[step: 43] loss: 230.36026000976562\n",
      "[step: 44] loss: 227.6507110595703\n",
      "[step: 45] loss: 224.35494995117188\n",
      "[step: 46] loss: 220.49362182617188\n",
      "[step: 47] loss: 216.41494750976562\n",
      "[step: 48] loss: 212.67385864257812\n",
      "[step: 49] loss: 209.80845642089844\n",
      "[step: 50] loss: 208.08853149414062\n",
      "[step: 51] loss: 207.3372344970703\n",
      "[step: 52] loss: 206.93661499023438\n",
      "[step: 53] loss: 206.0881805419922\n",
      "[step: 54] loss: 204.22862243652344\n",
      "[step: 55] loss: 201.3170166015625\n",
      "[step: 56] loss: 197.78802490234375\n",
      "[step: 57] loss: 194.24221801757812\n",
      "[step: 58] loss: 191.1029815673828\n",
      "[step: 59] loss: 188.435791015625\n",
      "[step: 60] loss: 185.99267578125\n",
      "[step: 61] loss: 183.41473388671875\n",
      "[step: 62] loss: 180.45164489746094\n",
      "[step: 63] loss: 177.08079528808594\n",
      "[step: 64] loss: 173.4879608154297\n",
      "[step: 65] loss: 169.94351196289062\n",
      "[step: 66] loss: 166.650390625\n",
      "[step: 67] loss: 163.6374053955078\n",
      "[step: 68] loss: 160.7510528564453\n",
      "[step: 69] loss: 157.75352478027344\n",
      "[step: 70] loss: 154.4722900390625\n",
      "[step: 71] loss: 150.9051513671875\n",
      "[step: 72] loss: 147.20883178710938\n",
      "[step: 73] loss: 143.5780792236328\n",
      "[step: 74] loss: 140.1016082763672\n",
      "[step: 75] loss: 136.69888305664062\n",
      "[step: 76] loss: 133.1948699951172\n",
      "[step: 77] loss: 129.4914093017578\n",
      "[step: 78] loss: 125.68939971923828\n",
      "[step: 79] loss: 122.02155303955078\n",
      "[step: 80] loss: 118.62029266357422\n",
      "[step: 81] loss: 115.36648559570312\n",
      "[step: 82] loss: 112.0545425415039\n",
      "[step: 83] loss: 108.70514678955078\n",
      "[step: 84] loss: 105.55696105957031\n",
      "[step: 85] loss: 102.68270874023438\n",
      "[step: 86] loss: 99.82594299316406\n",
      "[step: 87] loss: 96.82865905761719\n",
      "[step: 88] loss: 93.91827392578125\n",
      "[step: 89] loss: 91.24836730957031\n",
      "[step: 90] loss: 88.58482360839844\n",
      "[step: 91] loss: 85.82454681396484\n",
      "[step: 92] loss: 83.21248626708984\n",
      "[step: 93] loss: 80.74667358398438\n",
      "[step: 94] loss: 78.18464660644531\n",
      "[step: 95] loss: 75.64842224121094\n",
      "[step: 96] loss: 73.29554748535156\n",
      "[step: 97] loss: 70.91715240478516\n",
      "[step: 98] loss: 68.5262680053711\n",
      "[step: 99] loss: 66.29948425292969\n",
      "[step: 100] loss: 64.05842590332031\n",
      "[step: 101] loss: 61.82151412963867\n",
      "[step: 102] loss: 59.731651306152344\n",
      "[step: 103] loss: 57.60839080810547\n",
      "[step: 104] loss: 55.548728942871094\n",
      "[step: 105] loss: 53.58417510986328\n",
      "[step: 106] loss: 51.5976676940918\n",
      "[step: 107] loss: 49.753662109375\n",
      "[step: 108] loss: 47.9477424621582\n",
      "[step: 109] loss: 46.25702667236328\n",
      "[step: 110] loss: 44.70158004760742\n",
      "[step: 111] loss: 43.234214782714844\n",
      "[step: 112] loss: 41.94502258300781\n",
      "[step: 113] loss: 40.72991943359375\n",
      "[step: 114] loss: 39.67522048950195\n",
      "[step: 115] loss: 38.66852569580078\n",
      "[step: 116] loss: 37.78069305419922\n",
      "[step: 117] loss: 36.920684814453125\n",
      "[step: 118] loss: 36.154701232910156\n",
      "[step: 119] loss: 35.40785598754883\n",
      "[step: 120] loss: 34.749534606933594\n",
      "[step: 121] loss: 34.1215705871582\n",
      "[step: 122] loss: 33.564964294433594\n",
      "[step: 123] loss: 33.04706573486328\n",
      "[step: 124] loss: 32.54977035522461\n",
      "[step: 125] loss: 32.09091567993164\n",
      "[step: 126] loss: 31.621204376220703\n",
      "[step: 127] loss: 31.1514949798584\n",
      "[step: 128] loss: 30.706344604492188\n",
      "[step: 129] loss: 30.27509117126465\n",
      "[step: 130] loss: 29.86027717590332\n",
      "[step: 131] loss: 29.47524642944336\n",
      "[step: 132] loss: 29.126327514648438\n",
      "[step: 133] loss: 28.807905197143555\n",
      "[step: 134] loss: 28.51625633239746\n",
      "[step: 135] loss: 28.26473045349121\n",
      "[step: 136] loss: 28.10991859436035\n",
      "[step: 137] loss: 28.218244552612305\n",
      "[step: 138] loss: 28.987756729125977\n",
      "[step: 139] loss: 28.899200439453125\n",
      "[step: 140] loss: 27.45502281188965\n",
      "[step: 141] loss: 27.00625991821289\n",
      "[step: 142] loss: 27.785011291503906\n",
      "[step: 143] loss: 26.922075271606445\n",
      "[step: 144] loss: 26.212026596069336\n",
      "[step: 145] loss: 26.91735076904297\n",
      "[step: 146] loss: 26.16489028930664\n",
      "[step: 147] loss: 25.590219497680664\n",
      "[step: 148] loss: 26.160356521606445\n",
      "[step: 149] loss: 25.425533294677734\n",
      "[step: 150] loss: 25.008615493774414\n",
      "[step: 151] loss: 25.427133560180664\n",
      "[step: 152] loss: 24.754249572753906\n",
      "[step: 153] loss: 24.418804168701172\n",
      "[step: 154] loss: 24.716341018676758\n",
      "[step: 155] loss: 24.148481369018555\n",
      "[step: 156] loss: 23.8160400390625\n",
      "[step: 157] loss: 24.032318115234375\n",
      "[step: 158] loss: 23.59410858154297\n",
      "[step: 159] loss: 23.22637367248535\n",
      "[step: 160] loss: 23.385183334350586\n",
      "[step: 161] loss: 23.09939956665039\n",
      "[step: 162] loss: 22.687519073486328\n",
      "[step: 163] loss: 22.763553619384766\n",
      "[step: 164] loss: 22.641176223754883\n",
      "[step: 165] loss: 22.217010498046875\n",
      "[step: 166] loss: 22.13960075378418\n",
      "[step: 167] loss: 22.15170669555664\n",
      "[step: 168] loss: 21.84095573425293\n",
      "[step: 169] loss: 21.57644271850586\n",
      "[step: 170] loss: 21.558073043823242\n",
      "[step: 171] loss: 21.48406219482422\n",
      "[step: 172] loss: 21.233469009399414\n",
      "[step: 173] loss: 21.003021240234375\n",
      "[step: 174] loss: 20.92827606201172\n",
      "[step: 175] loss: 20.892623901367188\n",
      "[step: 176] loss: 20.754199981689453\n",
      "[step: 177] loss: 20.543142318725586\n",
      "[step: 178] loss: 20.351905822753906\n",
      "[step: 179] loss: 20.235916137695312\n",
      "[step: 180] loss: 20.17431640625\n",
      "[step: 181] loss: 20.122407913208008\n",
      "[step: 182] loss: 20.06071662902832\n",
      "[step: 183] loss: 19.97250747680664\n",
      "[step: 184] loss: 19.880037307739258\n",
      "[step: 185] loss: 19.776119232177734\n",
      "[step: 186] loss: 19.691024780273438\n",
      "[step: 187] loss: 19.616731643676758\n",
      "[step: 188] loss: 19.59068489074707\n",
      "[step: 189] loss: 19.60245704650879\n",
      "[step: 190] loss: 19.71288299560547\n",
      "[step: 191] loss: 19.84616470336914\n",
      "[step: 192] loss: 20.017189025878906\n",
      "[step: 193] loss: 19.87822151184082\n",
      "[step: 194] loss: 19.47370147705078\n",
      "[step: 195] loss: 18.87190818786621\n",
      "[step: 196] loss: 18.516916275024414\n",
      "[step: 197] loss: 18.533475875854492\n",
      "[step: 198] loss: 18.778709411621094\n",
      "[step: 199] loss: 19.05559539794922\n",
      "[step: 200] loss: 19.101455688476562\n",
      "[step: 201] loss: 18.891765594482422\n",
      "[step: 202] loss: 18.44053077697754\n",
      "[step: 203] loss: 18.065185546875\n",
      "[step: 204] loss: 17.925661087036133\n",
      "[step: 205] loss: 18.008533477783203\n",
      "[step: 206] loss: 18.214662551879883\n",
      "[step: 207] loss: 18.427400588989258\n",
      "[step: 208] loss: 18.595264434814453\n",
      "[step: 209] loss: 18.545244216918945\n",
      "[step: 210] loss: 18.30826759338379\n",
      "[step: 211] loss: 17.898481369018555\n",
      "[step: 212] loss: 17.546472549438477\n",
      "[step: 213] loss: 17.370391845703125\n",
      "[step: 214] loss: 17.38042640686035\n",
      "[step: 215] loss: 17.520801544189453\n",
      "[step: 216] loss: 17.738142013549805\n",
      "[step: 217] loss: 18.0246639251709\n",
      "[step: 218] loss: 18.281736373901367\n",
      "[step: 219] loss: 18.450923919677734\n",
      "[step: 220] loss: 18.25839614868164\n",
      "[step: 221] loss: 17.781047821044922\n",
      "[step: 222] loss: 17.20594024658203\n",
      "[step: 223] loss: 16.89157485961914\n",
      "[step: 224] loss: 16.929967880249023\n",
      "[step: 225] loss: 17.196035385131836\n",
      "[step: 226] loss: 17.51133155822754\n",
      "[step: 227] loss: 17.679094314575195\n",
      "[step: 228] loss: 17.62247657775879\n",
      "[step: 229] loss: 17.29932975769043\n",
      "[step: 230] loss: 16.904809951782227\n",
      "[step: 231] loss: 16.619043350219727\n",
      "[step: 232] loss: 16.539701461791992\n",
      "[step: 233] loss: 16.634967803955078\n",
      "[step: 234] loss: 16.826337814331055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 235] loss: 17.052425384521484\n",
      "[step: 236] loss: 17.24200439453125\n",
      "[step: 237] loss: 17.355541229248047\n",
      "[step: 238] loss: 17.29668617248535\n",
      "[step: 239] loss: 17.078630447387695\n",
      "[step: 240] loss: 16.74207305908203\n",
      "[step: 241] loss: 16.431594848632812\n",
      "[step: 242] loss: 16.242795944213867\n",
      "[step: 243] loss: 16.201478958129883\n",
      "[step: 244] loss: 16.27592658996582\n",
      "[step: 245] loss: 16.426692962646484\n",
      "[step: 246] loss: 16.63567543029785\n",
      "[step: 247] loss: 16.889158248901367\n",
      "[step: 248] loss: 17.1783390045166\n",
      "[step: 249] loss: 17.4116153717041\n",
      "[step: 250] loss: 17.477170944213867\n",
      "[step: 251] loss: 17.22210693359375\n",
      "[step: 252] loss: 16.71563720703125\n",
      "[step: 253] loss: 16.199012756347656\n",
      "[step: 254] loss: 15.93884563446045\n",
      "[step: 255] loss: 15.997575759887695\n",
      "[step: 256] loss: 16.256723403930664\n",
      "[step: 257] loss: 16.542037963867188\n",
      "[step: 258] loss: 16.700408935546875\n",
      "[step: 259] loss: 16.652936935424805\n",
      "[step: 260] loss: 16.41427993774414\n",
      "[step: 261] loss: 16.10359001159668\n",
      "[step: 262] loss: 15.859325408935547\n",
      "[step: 263] loss: 15.759160995483398\n",
      "[step: 264] loss: 15.79743480682373\n",
      "[step: 265] loss: 15.923628807067871\n",
      "[step: 266] loss: 16.08388900756836\n",
      "[step: 267] loss: 16.237821578979492\n",
      "[step: 268] loss: 16.353925704956055\n",
      "[step: 269] loss: 16.409029006958008\n",
      "[step: 270] loss: 16.38158416748047\n",
      "[step: 271] loss: 16.2740478515625\n",
      "[step: 272] loss: 16.103857040405273\n",
      "[step: 273] loss: 15.915670394897461\n",
      "[step: 274] loss: 15.748832702636719\n",
      "[step: 275] loss: 15.630605697631836\n",
      "[step: 276] loss: 15.565248489379883\n",
      "[step: 277] loss: 15.544291496276855\n",
      "[step: 278] loss: 15.556944847106934\n",
      "[step: 279] loss: 15.598721504211426\n",
      "[step: 280] loss: 15.67724895477295\n",
      "[step: 281] loss: 15.815740585327148\n",
      "[step: 282] loss: 16.06489372253418\n",
      "[step: 283] loss: 16.496234893798828\n",
      "[step: 284] loss: 17.205469131469727\n",
      "[step: 285] loss: 18.105396270751953\n",
      "[step: 286] loss: 18.828603744506836\n",
      "[step: 287] loss: 18.4246883392334\n",
      "[step: 288] loss: 16.956743240356445\n",
      "[step: 289] loss: 15.602980613708496\n",
      "[step: 290] loss: 15.59778881072998\n",
      "[step: 291] loss: 16.55647850036621\n",
      "[step: 292] loss: 17.08872413635254\n",
      "[step: 293] loss: 16.520139694213867\n",
      "[step: 294] loss: 15.580429077148438\n",
      "[step: 295] loss: 15.431379318237305\n",
      "[step: 296] loss: 16.01983070373535\n",
      "[step: 297] loss: 16.381677627563477\n",
      "[step: 298] loss: 16.022462844848633\n",
      "[step: 299] loss: 15.444014549255371\n",
      "[step: 300] loss: 15.381149291992188\n",
      "[step: 301] loss: 15.756061553955078\n",
      "[step: 302] loss: 15.940711975097656\n",
      "[step: 303] loss: 15.683801651000977\n",
      "[step: 304] loss: 15.342180252075195\n",
      "[step: 305] loss: 15.326807022094727\n",
      "[step: 306] loss: 15.557459831237793\n",
      "[step: 307] loss: 15.675070762634277\n",
      "[step: 308] loss: 15.536081314086914\n",
      "[step: 309] loss: 15.312821388244629\n",
      "[step: 310] loss: 15.241604804992676\n",
      "[step: 311] loss: 15.343923568725586\n",
      "[step: 312] loss: 15.463738441467285\n",
      "[step: 313] loss: 15.46774959564209\n",
      "[step: 314] loss: 15.353941917419434\n",
      "[step: 315] loss: 15.230859756469727\n",
      "[step: 316] loss: 15.18620777130127\n",
      "[step: 317] loss: 15.226521492004395\n",
      "[step: 318] loss: 15.296146392822266\n",
      "[step: 319] loss: 15.334003448486328\n",
      "[step: 320] loss: 15.318259239196777\n",
      "[step: 321] loss: 15.259695053100586\n",
      "[step: 322] loss: 15.192320823669434\n",
      "[step: 323] loss: 15.141694068908691\n",
      "[step: 324] loss: 15.119484901428223\n",
      "[step: 325] loss: 15.12286376953125\n",
      "[step: 326] loss: 15.142526626586914\n",
      "[step: 327] loss: 15.169757843017578\n",
      "[step: 328] loss: 15.198410987854004\n",
      "[step: 329] loss: 15.228654861450195\n",
      "[step: 330] loss: 15.260177612304688\n",
      "[step: 331] loss: 15.301361083984375\n",
      "[step: 332] loss: 15.354227066040039\n",
      "[step: 333] loss: 15.436188697814941\n",
      "[step: 334] loss: 15.549302101135254\n",
      "[step: 335] loss: 15.725708961486816\n",
      "[step: 336] loss: 15.951167106628418\n",
      "[step: 337] loss: 16.263940811157227\n",
      "[step: 338] loss: 16.548646926879883\n",
      "[step: 339] loss: 16.791833877563477\n",
      "[step: 340] loss: 16.7030029296875\n",
      "[step: 341] loss: 16.34910774230957\n",
      "[step: 342] loss: 15.727839469909668\n",
      "[step: 343] loss: 15.206632614135742\n",
      "[step: 344] loss: 14.992427825927734\n",
      "[step: 345] loss: 15.115974426269531\n",
      "[step: 346] loss: 15.423636436462402\n",
      "[step: 347] loss: 15.687542915344238\n",
      "[step: 348] loss: 15.775753021240234\n",
      "[step: 349] loss: 15.605173110961914\n",
      "[step: 350] loss: 15.318655967712402\n",
      "[step: 351] loss: 15.056708335876465\n",
      "[step: 352] loss: 14.950831413269043\n",
      "[step: 353] loss: 15.009997367858887\n",
      "[step: 354] loss: 15.155593872070312\n",
      "[step: 355] loss: 15.29024887084961\n",
      "[step: 356] loss: 15.331263542175293\n",
      "[step: 357] loss: 15.280906677246094\n",
      "[step: 358] loss: 15.157537460327148\n",
      "[step: 359] loss: 15.029786109924316\n",
      "[step: 360] loss: 14.936954498291016\n",
      "[step: 361] loss: 14.899940490722656\n",
      "[step: 362] loss: 14.913064002990723\n",
      "[step: 363] loss: 14.958955764770508\n",
      "[step: 364] loss: 15.020769119262695\n",
      "[step: 365] loss: 15.083148002624512\n",
      "[step: 366] loss: 15.145041465759277\n",
      "[step: 367] loss: 15.19575023651123\n",
      "[step: 368] loss: 15.24864673614502\n",
      "[step: 369] loss: 15.28935718536377\n",
      "[step: 370] loss: 15.33987808227539\n",
      "[step: 371] loss: 15.376821517944336\n",
      "[step: 372] loss: 15.42776107788086\n",
      "[step: 373] loss: 15.456840515136719\n",
      "[step: 374] loss: 15.495800018310547\n",
      "[step: 375] loss: 15.496143341064453\n",
      "[step: 376] loss: 15.494003295898438\n",
      "[step: 377] loss: 15.439000129699707\n",
      "[step: 378] loss: 15.373367309570312\n",
      "[step: 379] loss: 15.265278816223145\n",
      "[step: 380] loss: 15.157577514648438\n",
      "[step: 381] loss: 15.04265308380127\n",
      "[step: 382] loss: 14.947643280029297\n",
      "[step: 383] loss: 14.872183799743652\n",
      "[step: 384] loss: 14.821792602539062\n",
      "[step: 385] loss: 14.792410850524902\n",
      "[step: 386] loss: 14.77979564666748\n",
      "[step: 387] loss: 14.779452323913574\n",
      "[step: 388] loss: 14.788490295410156\n",
      "[step: 389] loss: 14.806449890136719\n",
      "[step: 390] loss: 14.83498477935791\n",
      "[step: 391] loss: 14.881277084350586\n",
      "[step: 392] loss: 14.953661918640137\n",
      "[step: 393] loss: 15.078645706176758\n",
      "[step: 394] loss: 15.277109146118164\n",
      "[step: 395] loss: 15.629440307617188\n",
      "[step: 396] loss: 16.141855239868164\n",
      "[step: 397] loss: 16.968059539794922\n",
      "[step: 398] loss: 17.73006248474121\n",
      "[step: 399] loss: 18.40216827392578\n",
      "[step: 400] loss: 17.764347076416016\n",
      "[step: 401] loss: 16.475664138793945\n",
      "[step: 402] loss: 15.09012222290039\n",
      "[step: 403] loss: 14.769966125488281\n",
      "[step: 404] loss: 15.460049629211426\n",
      "[step: 405] loss: 16.190811157226562\n",
      "[step: 406] loss: 16.215599060058594\n",
      "[step: 407] loss: 15.406903266906738\n",
      "[step: 408] loss: 14.770552635192871\n",
      "[step: 409] loss: 14.855048179626465\n",
      "[step: 410] loss: 15.355534553527832\n",
      "[step: 411] loss: 15.585878372192383\n",
      "[step: 412] loss: 15.21871566772461\n",
      "[step: 413] loss: 14.781097412109375\n",
      "[step: 414] loss: 14.745501518249512\n",
      "[step: 415] loss: 15.035364151000977\n",
      "[step: 416] loss: 15.215984344482422\n",
      "[step: 417] loss: 15.040263175964355\n",
      "[step: 418] loss: 14.76452922821045\n",
      "[step: 419] loss: 14.685015678405762\n",
      "[step: 420] loss: 14.829256057739258\n",
      "[step: 421] loss: 14.97985553741455\n",
      "[step: 422] loss: 14.946794509887695\n",
      "[step: 423] loss: 14.788676261901855\n",
      "[step: 424] loss: 14.666950225830078\n",
      "[step: 425] loss: 14.682343482971191\n",
      "[step: 426] loss: 14.780231475830078\n",
      "[step: 427] loss: 14.83935832977295\n",
      "[step: 428] loss: 14.807741165161133\n",
      "[step: 429] loss: 14.715662002563477\n",
      "[step: 430] loss: 14.644851684570312\n",
      "[step: 431] loss: 14.637189865112305\n",
      "[step: 432] loss: 14.678614616394043\n",
      "[step: 433] loss: 14.724607467651367\n",
      "[step: 434] loss: 14.735995292663574\n",
      "[step: 435] loss: 14.709516525268555\n",
      "[step: 436] loss: 14.661309242248535\n",
      "[step: 437] loss: 14.620436668395996\n",
      "[step: 438] loss: 14.60277271270752\n",
      "[step: 439] loss: 14.608919143676758\n",
      "[step: 440] loss: 14.628119468688965\n",
      "[step: 441] loss: 14.646730422973633\n",
      "[step: 442] loss: 14.656728744506836\n",
      "[step: 443] loss: 14.653691291809082\n",
      "[step: 444] loss: 14.641654968261719\n",
      "[step: 445] loss: 14.623269081115723\n",
      "[step: 446] loss: 14.60435676574707\n",
      "[step: 447] loss: 14.58712387084961\n",
      "[step: 448] loss: 14.573538780212402\n",
      "[step: 449] loss: 14.563604354858398\n",
      "[step: 450] loss: 14.556883811950684\n",
      "[step: 451] loss: 14.55263900756836\n",
      "[step: 452] loss: 14.550230979919434\n",
      "[step: 453] loss: 14.549336433410645\n",
      "[step: 454] loss: 14.550026893615723\n",
      "[step: 455] loss: 14.553031921386719\n",
      "[step: 456] loss: 14.559769630432129\n",
      "[step: 457] loss: 14.573678970336914\n",
      "[step: 458] loss: 14.599918365478516\n",
      "[step: 459] loss: 14.65164852142334\n",
      "[step: 460] loss: 14.747515678405762\n",
      "[step: 461] loss: 14.940655708312988\n",
      "[step: 462] loss: 15.290730476379395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 463] loss: 15.997016906738281\n",
      "[step: 464] loss: 17.081140518188477\n",
      "[step: 465] loss: 18.9213809967041\n",
      "[step: 466] loss: 19.98626708984375\n",
      "[step: 467] loss: 20.208580017089844\n",
      "[step: 468] loss: 17.335708618164062\n",
      "[step: 469] loss: 14.935018539428711\n",
      "[step: 470] loss: 14.818438529968262\n",
      "[step: 471] loss: 16.406085968017578\n",
      "[step: 472] loss: 17.31136703491211\n",
      "[step: 473] loss: 15.905397415161133\n",
      "[step: 474] loss: 14.60220718383789\n",
      "[step: 475] loss: 14.97530460357666\n",
      "[step: 476] loss: 15.945618629455566\n",
      "[step: 477] loss: 15.800326347351074\n",
      "[step: 478] loss: 14.752339363098145\n",
      "[step: 479] loss: 14.656800270080566\n",
      "[step: 480] loss: 15.363164901733398\n",
      "[step: 481] loss: 15.338955879211426\n",
      "[step: 482] loss: 14.695908546447754\n",
      "[step: 483] loss: 14.563639640808105\n",
      "[step: 484] loss: 15.00700855255127\n",
      "[step: 485] loss: 15.101943969726562\n",
      "[step: 486] loss: 14.66987419128418\n",
      "[step: 487] loss: 14.508296012878418\n",
      "[step: 488] loss: 14.782941818237305\n",
      "[step: 489] loss: 14.893383979797363\n",
      "[step: 490] loss: 14.651808738708496\n",
      "[step: 491] loss: 14.482647895812988\n",
      "[step: 492] loss: 14.623943328857422\n",
      "[step: 493] loss: 14.760353088378906\n",
      "[step: 494] loss: 14.639301300048828\n",
      "[step: 495] loss: 14.481321334838867\n",
      "[step: 496] loss: 14.513688087463379\n",
      "[step: 497] loss: 14.630924224853516\n",
      "[step: 498] loss: 14.625804901123047\n",
      "[step: 499] loss: 14.507649421691895\n",
      "[step: 500] loss: 14.45508098602295\n",
      "[step: 501] loss: 14.514559745788574\n",
      "[step: 502] loss: 14.564963340759277\n",
      "[step: 503] loss: 14.52548599243164\n",
      "[step: 504] loss: 14.453604698181152\n",
      "[step: 505] loss: 14.442743301391602\n",
      "[step: 506] loss: 14.48510456085205\n",
      "[step: 507] loss: 14.504910469055176\n",
      "[step: 508] loss: 14.473087310791016\n",
      "[step: 509] loss: 14.429783821105957\n",
      "[step: 510] loss: 14.42254638671875\n",
      "[step: 511] loss: 14.44575309753418\n",
      "[step: 512] loss: 14.459539413452148\n",
      "[step: 513] loss: 14.444269180297852\n",
      "[step: 514] loss: 14.415959358215332\n",
      "[step: 515] loss: 14.402368545532227\n",
      "[step: 516] loss: 14.409581184387207\n",
      "[step: 517] loss: 14.421488761901855\n",
      "[step: 518] loss: 14.421459197998047\n",
      "[step: 519] loss: 14.407382011413574\n",
      "[step: 520] loss: 14.390923500061035\n",
      "[step: 521] loss: 14.382808685302734\n",
      "[step: 522] loss: 14.384661674499512\n",
      "[step: 523] loss: 14.389979362487793\n",
      "[step: 524] loss: 14.391151428222656\n",
      "[step: 525] loss: 14.385674476623535\n",
      "[step: 526] loss: 14.375880241394043\n",
      "[step: 527] loss: 14.366602897644043\n",
      "[step: 528] loss: 14.361006736755371\n",
      "[step: 529] loss: 14.359474182128906\n",
      "[step: 530] loss: 14.360223770141602\n",
      "[step: 531] loss: 14.360905647277832\n",
      "[step: 532] loss: 14.359994888305664\n",
      "[step: 533] loss: 14.356917381286621\n",
      "[step: 534] loss: 14.352263450622559\n",
      "[step: 535] loss: 14.346771240234375\n",
      "[step: 536] loss: 14.341339111328125\n",
      "[step: 537] loss: 14.336453437805176\n",
      "[step: 538] loss: 14.332374572753906\n",
      "[step: 539] loss: 14.329073905944824\n",
      "[step: 540] loss: 14.326406478881836\n",
      "[step: 541] loss: 14.324201583862305\n",
      "[step: 542] loss: 14.322330474853516\n",
      "[step: 543] loss: 14.320743560791016\n",
      "[step: 544] loss: 14.319489479064941\n",
      "[step: 545] loss: 14.31873607635498\n",
      "[step: 546] loss: 14.318719863891602\n",
      "[step: 547] loss: 14.320037841796875\n",
      "[step: 548] loss: 14.323410987854004\n",
      "[step: 549] loss: 14.330658912658691\n",
      "[step: 550] loss: 14.344069480895996\n",
      "[step: 551] loss: 14.369634628295898\n",
      "[step: 552] loss: 14.414778709411621\n",
      "[step: 553] loss: 14.500824928283691\n",
      "[step: 554] loss: 14.650421142578125\n",
      "[step: 555] loss: 14.940728187561035\n",
      "[step: 556] loss: 15.412825584411621\n",
      "[step: 557] loss: 16.3006591796875\n",
      "[step: 558] loss: 17.37665557861328\n",
      "[step: 559] loss: 18.942062377929688\n",
      "[step: 560] loss: 19.05680274963379\n",
      "[step: 561] loss: 18.31516456604004\n",
      "[step: 562] loss: 15.866767883300781\n",
      "[step: 563] loss: 14.394686698913574\n",
      "[step: 564] loss: 14.72581958770752\n",
      "[step: 565] loss: 15.961134910583496\n",
      "[step: 566] loss: 16.536205291748047\n",
      "[step: 567] loss: 15.47464370727539\n",
      "[step: 568] loss: 14.420297622680664\n",
      "[step: 569] loss: 14.501749038696289\n",
      "[step: 570] loss: 15.251799583435059\n",
      "[step: 571] loss: 15.42149829864502\n",
      "[step: 572] loss: 14.691161155700684\n",
      "[step: 573] loss: 14.295028686523438\n",
      "[step: 574] loss: 14.665746688842773\n",
      "[step: 575] loss: 14.996853828430176\n",
      "[step: 576] loss: 14.748499870300293\n",
      "[step: 577] loss: 14.321615219116211\n",
      "[step: 578] loss: 14.358016014099121\n",
      "[step: 579] loss: 14.669646263122559\n",
      "[step: 580] loss: 14.683589935302734\n",
      "[step: 581] loss: 14.407981872558594\n",
      "[step: 582] loss: 14.260592460632324\n",
      "[step: 583] loss: 14.407876968383789\n",
      "[step: 584] loss: 14.55378246307373\n",
      "[step: 585] loss: 14.452936172485352\n",
      "[step: 586] loss: 14.280142784118652\n",
      "[step: 587] loss: 14.267831802368164\n",
      "[step: 588] loss: 14.384292602539062\n",
      "[step: 589] loss: 14.43191909790039\n",
      "[step: 590] loss: 14.338850975036621\n",
      "[step: 591] loss: 14.243245124816895\n",
      "[step: 592] loss: 14.255378723144531\n",
      "[step: 593] loss: 14.324953079223633\n",
      "[step: 594] loss: 14.339408874511719\n",
      "[step: 595] loss: 14.276910781860352\n",
      "[step: 596] loss: 14.222476959228516\n",
      "[step: 597] loss: 14.232104301452637\n",
      "[step: 598] loss: 14.27270793914795\n",
      "[step: 599] loss: 14.28201961517334\n",
      "[step: 600] loss: 14.246200561523438\n",
      "[step: 601] loss: 14.209085464477539\n",
      "[step: 602] loss: 14.206072807312012\n",
      "[step: 603] loss: 14.227717399597168\n",
      "[step: 604] loss: 14.240009307861328\n",
      "[step: 605] loss: 14.225600242614746\n",
      "[step: 606] loss: 14.200074195861816\n",
      "[step: 607] loss: 14.18630313873291\n",
      "[step: 608] loss: 14.191072463989258\n",
      "[step: 609] loss: 14.202193260192871\n",
      "[step: 610] loss: 14.204272270202637\n",
      "[step: 611] loss: 14.193896293640137\n",
      "[step: 612] loss: 14.178512573242188\n",
      "[step: 613] loss: 14.16845989227295\n",
      "[step: 614] loss: 14.167391777038574\n",
      "[step: 615] loss: 14.171504974365234\n",
      "[step: 616] loss: 14.174324035644531\n",
      "[step: 617] loss: 14.171749114990234\n",
      "[step: 618] loss: 14.164460182189941\n",
      "[step: 619] loss: 14.1555757522583\n",
      "[step: 620] loss: 14.148566246032715\n",
      "[step: 621] loss: 14.144938468933105\n",
      "[step: 622] loss: 14.144176483154297\n",
      "[step: 623] loss: 14.14456844329834\n",
      "[step: 624] loss: 14.14433479309082\n",
      "[step: 625] loss: 14.142508506774902\n",
      "[step: 626] loss: 14.138922691345215\n",
      "[step: 627] loss: 14.134218215942383\n",
      "[step: 628] loss: 14.129093170166016\n",
      "[step: 629] loss: 14.124253273010254\n",
      "[step: 630] loss: 14.120036125183105\n",
      "[step: 631] loss: 14.116558074951172\n",
      "[step: 632] loss: 14.113741874694824\n",
      "[step: 633] loss: 14.111424446105957\n",
      "[step: 634] loss: 14.109415054321289\n",
      "[step: 635] loss: 14.10758113861084\n",
      "[step: 636] loss: 14.105855941772461\n",
      "[step: 637] loss: 14.10422420501709\n",
      "[step: 638] loss: 14.102777481079102\n",
      "[step: 639] loss: 14.101604461669922\n",
      "[step: 640] loss: 14.10097599029541\n",
      "[step: 641] loss: 14.101140975952148\n",
      "[step: 642] loss: 14.102771759033203\n",
      "[step: 643] loss: 14.106507301330566\n",
      "[step: 644] loss: 14.114174842834473\n",
      "[step: 645] loss: 14.12763500213623\n",
      "[step: 646] loss: 14.152308464050293\n",
      "[step: 647] loss: 14.193683624267578\n",
      "[step: 648] loss: 14.269250869750977\n",
      "[step: 649] loss: 14.393693923950195\n",
      "[step: 650] loss: 14.624371528625488\n",
      "[step: 651] loss: 14.982028007507324\n",
      "[step: 652] loss: 15.631093978881836\n",
      "[step: 653] loss: 16.42759895324707\n",
      "[step: 654] loss: 17.637393951416016\n",
      "[step: 655] loss: 18.0828800201416\n",
      "[step: 656] loss: 18.101640701293945\n",
      "[step: 657] loss: 16.380464553833008\n",
      "[step: 658] loss: 14.747244834899902\n",
      "[step: 659] loss: 14.073418617248535\n",
      "[step: 660] loss: 14.668554306030273\n",
      "[step: 661] loss: 15.653600692749023\n",
      "[step: 662] loss: 15.698062896728516\n",
      "[step: 663] loss: 14.935327529907227\n",
      "[step: 664] loss: 14.146714210510254\n",
      "[step: 665] loss: 14.219191551208496\n",
      "[step: 666] loss: 14.80158519744873\n",
      "[step: 667] loss: 14.911388397216797\n",
      "[step: 668] loss: 14.4569673538208\n",
      "[step: 669] loss: 14.058135986328125\n",
      "[step: 670] loss: 14.203548431396484\n",
      "[step: 671] loss: 14.55178451538086\n",
      "[step: 672] loss: 14.536574363708496\n",
      "[step: 673] loss: 14.223138809204102\n",
      "[step: 674] loss: 14.02696704864502\n",
      "[step: 675] loss: 14.154488563537598\n",
      "[step: 676] loss: 14.348220825195312\n",
      "[step: 677] loss: 14.302474021911621\n",
      "[step: 678] loss: 14.106377601623535\n",
      "[step: 679] loss: 14.015127182006836\n",
      "[step: 680] loss: 14.10748291015625\n",
      "[step: 681] loss: 14.215912818908691\n",
      "[step: 682] loss: 14.180818557739258\n",
      "[step: 683] loss: 14.060288429260254\n",
      "[step: 684] loss: 13.998703002929688\n",
      "[step: 685] loss: 14.045711517333984\n",
      "[step: 686] loss: 14.110960006713867\n",
      "[step: 687] loss: 14.097797393798828\n",
      "[step: 688] loss: 14.027339935302734\n",
      "[step: 689] loss: 13.981475830078125\n",
      "[step: 690] loss: 13.999587059020996\n",
      "[step: 691] loss: 14.040048599243164\n",
      "[step: 692] loss: 14.043904304504395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 693] loss: 14.007062911987305\n",
      "[step: 694] loss: 13.969188690185547\n",
      "[step: 695] loss: 13.964619636535645\n",
      "[step: 696] loss: 13.985207557678223\n",
      "[step: 697] loss: 13.998361587524414\n",
      "[step: 698] loss: 13.98699951171875\n",
      "[step: 699] loss: 13.96116828918457\n",
      "[step: 700] loss: 13.94351863861084\n",
      "[step: 701] loss: 13.943750381469727\n",
      "[step: 702] loss: 13.953682899475098\n",
      "[step: 703] loss: 13.958967208862305\n",
      "[step: 704] loss: 13.95201301574707\n",
      "[step: 705] loss: 13.937294960021973\n",
      "[step: 706] loss: 13.923870086669922\n",
      "[step: 707] loss: 13.918088912963867\n",
      "[step: 708] loss: 13.919452667236328\n",
      "[step: 709] loss: 13.922811508178711\n",
      "[step: 710] loss: 13.923110008239746\n",
      "[step: 711] loss: 13.918196678161621\n",
      "[step: 712] loss: 13.909757614135742\n",
      "[step: 713] loss: 13.900776863098145\n",
      "[step: 714] loss: 13.893932342529297\n",
      "[step: 715] loss: 13.890129089355469\n",
      "[step: 716] loss: 13.888673782348633\n",
      "[step: 717] loss: 13.888086318969727\n",
      "[step: 718] loss: 13.88692855834961\n",
      "[step: 719] loss: 13.884434700012207\n",
      "[step: 720] loss: 13.880411148071289\n",
      "[step: 721] loss: 13.875345230102539\n",
      "[step: 722] loss: 13.869790077209473\n",
      "[step: 723] loss: 13.864367485046387\n",
      "[step: 724] loss: 13.85940170288086\n",
      "[step: 725] loss: 13.855046272277832\n",
      "[step: 726] loss: 13.851268768310547\n",
      "[step: 727] loss: 13.847966194152832\n",
      "[step: 728] loss: 13.84498119354248\n",
      "[step: 729] loss: 13.842180252075195\n",
      "[step: 730] loss: 13.839484214782715\n",
      "[step: 731] loss: 13.8368558883667\n",
      "[step: 732] loss: 13.83434772491455\n",
      "[step: 733] loss: 13.831995964050293\n",
      "[step: 734] loss: 13.82996940612793\n",
      "[step: 735] loss: 13.828401565551758\n",
      "[step: 736] loss: 13.82771110534668\n",
      "[step: 737] loss: 13.828275680541992\n",
      "[step: 738] loss: 13.831124305725098\n",
      "[step: 739] loss: 13.837287902832031\n",
      "[step: 740] loss: 13.849616050720215\n",
      "[step: 741] loss: 13.871027946472168\n",
      "[step: 742] loss: 13.910017013549805\n",
      "[step: 743] loss: 13.974489212036133\n",
      "[step: 744] loss: 14.091052055358887\n",
      "[step: 745] loss: 14.276756286621094\n",
      "[step: 746] loss: 14.611958503723145\n",
      "[step: 747] loss: 15.091081619262695\n",
      "[step: 748] loss: 15.899444580078125\n",
      "[step: 749] loss: 16.682491302490234\n",
      "[step: 750] loss: 17.626962661743164\n",
      "[step: 751] loss: 17.363990783691406\n",
      "[step: 752] loss: 16.50852394104004\n",
      "[step: 753] loss: 14.834643363952637\n",
      "[step: 754] loss: 13.849361419677734\n",
      "[step: 755] loss: 14.009550094604492\n",
      "[step: 756] loss: 14.829855918884277\n",
      "[step: 757] loss: 15.364991188049316\n",
      "[step: 758] loss: 14.87204647064209\n",
      "[step: 759] loss: 14.069436073303223\n",
      "[step: 760] loss: 13.775418281555176\n",
      "[step: 761] loss: 14.165546417236328\n",
      "[step: 762] loss: 14.562581062316895\n",
      "[step: 763] loss: 14.350933074951172\n",
      "[step: 764] loss: 13.892434120178223\n",
      "[step: 765] loss: 13.746305465698242\n",
      "[step: 766] loss: 13.994974136352539\n",
      "[step: 767] loss: 14.216422080993652\n",
      "[step: 768] loss: 14.080418586730957\n",
      "[step: 769] loss: 13.804771423339844\n",
      "[step: 770] loss: 13.725584030151367\n",
      "[step: 771] loss: 13.8784818649292\n",
      "[step: 772] loss: 14.002900123596191\n",
      "[step: 773] loss: 13.916925430297852\n",
      "[step: 774] loss: 13.751263618469238\n",
      "[step: 775] loss: 13.699886322021484\n",
      "[step: 776] loss: 13.784628868103027\n",
      "[step: 777] loss: 13.85953426361084\n",
      "[step: 778] loss: 13.81440258026123\n",
      "[step: 779] loss: 13.713255882263184\n",
      "[step: 780] loss: 13.672818183898926\n",
      "[step: 781] loss: 13.717019081115723\n",
      "[step: 782] loss: 13.764777183532715\n",
      "[step: 783] loss: 13.745576858520508\n",
      "[step: 784] loss: 13.684480667114258\n",
      "[step: 785] loss: 13.648181915283203\n",
      "[step: 786] loss: 13.663607597351074\n",
      "[step: 787] loss: 13.694060325622559\n",
      "[step: 788] loss: 13.692882537841797\n",
      "[step: 789] loss: 13.659461975097656\n",
      "[step: 790] loss: 13.627461433410645\n",
      "[step: 791] loss: 13.62289810180664\n",
      "[step: 792] loss: 13.637884140014648\n",
      "[step: 793] loss: 13.646782875061035\n",
      "[step: 794] loss: 13.636137962341309\n",
      "[step: 795] loss: 13.613219261169434\n",
      "[step: 796] loss: 13.59553337097168\n",
      "[step: 797] loss: 13.591917037963867\n",
      "[step: 798] loss: 13.597503662109375\n",
      "[step: 799] loss: 13.601208686828613\n",
      "[step: 800] loss: 13.595773696899414\n",
      "[step: 801] loss: 13.582880020141602\n",
      "[step: 802] loss: 13.56901741027832\n",
      "[step: 803] loss: 13.560152053833008\n",
      "[step: 804] loss: 13.557464599609375\n",
      "[step: 805] loss: 13.55782699584961\n",
      "[step: 806] loss: 13.55699348449707\n",
      "[step: 807] loss: 13.552427291870117\n",
      "[step: 808] loss: 13.544501304626465\n",
      "[step: 809] loss: 13.535183906555176\n",
      "[step: 810] loss: 13.52679443359375\n",
      "[step: 811] loss: 13.520617485046387\n",
      "[step: 812] loss: 13.516633033752441\n",
      "[step: 813] loss: 13.513823509216309\n",
      "[step: 814] loss: 13.510921478271484\n",
      "[step: 815] loss: 13.507072448730469\n",
      "[step: 816] loss: 13.501978874206543\n",
      "[step: 817] loss: 13.495920181274414\n",
      "[step: 818] loss: 13.489339828491211\n",
      "[step: 819] loss: 13.482770919799805\n",
      "[step: 820] loss: 13.476572036743164\n",
      "[step: 821] loss: 13.470915794372559\n",
      "[step: 822] loss: 13.465763092041016\n",
      "[step: 823] loss: 13.4610013961792\n",
      "[step: 824] loss: 13.456496238708496\n",
      "[step: 825] loss: 13.452104568481445\n",
      "[step: 826] loss: 13.447736740112305\n",
      "[step: 827] loss: 13.443328857421875\n",
      "[step: 828] loss: 13.438898086547852\n",
      "[step: 829] loss: 13.434444427490234\n",
      "[step: 830] loss: 13.430031776428223\n",
      "[step: 831] loss: 13.425707817077637\n",
      "[step: 832] loss: 13.42159366607666\n",
      "[step: 833] loss: 13.417776107788086\n",
      "[step: 834] loss: 13.414488792419434\n",
      "[step: 835] loss: 13.4119234085083\n",
      "[step: 836] loss: 13.410593032836914\n",
      "[step: 837] loss: 13.410965919494629\n",
      "[step: 838] loss: 13.414270401000977\n",
      "[step: 839] loss: 13.421724319458008\n",
      "[step: 840] loss: 13.436624526977539\n",
      "[step: 841] loss: 13.462079048156738\n",
      "[step: 842] loss: 13.507333755493164\n",
      "[step: 843] loss: 13.579708099365234\n",
      "[step: 844] loss: 13.705260276794434\n",
      "[step: 845] loss: 13.895268440246582\n",
      "[step: 846] loss: 14.217122077941895\n",
      "[step: 847] loss: 14.644268989562988\n",
      "[step: 848] loss: 15.299696922302246\n",
      "[step: 849] loss: 15.866490364074707\n",
      "[step: 850] loss: 16.43595314025879\n",
      "[step: 851] loss: 16.140472412109375\n",
      "[step: 852] loss: 15.391947746276855\n",
      "[step: 853] loss: 14.169543266296387\n",
      "[step: 854] loss: 13.409137725830078\n",
      "[step: 855] loss: 13.439040184020996\n",
      "[step: 856] loss: 13.996336936950684\n",
      "[step: 857] loss: 14.451752662658691\n",
      "[step: 858] loss: 14.254118919372559\n",
      "[step: 859] loss: 13.695290565490723\n",
      "[step: 860] loss: 13.312787055969238\n",
      "[step: 861] loss: 13.43542766571045\n",
      "[step: 862] loss: 13.788847923278809\n",
      "[step: 863] loss: 13.880833625793457\n",
      "[step: 864] loss: 13.632612228393555\n",
      "[step: 865] loss: 13.326355934143066\n",
      "[step: 866] loss: 13.286295890808105\n",
      "[step: 867] loss: 13.47077751159668\n",
      "[step: 868] loss: 13.592072486877441\n",
      "[step: 869] loss: 13.50367259979248\n",
      "[step: 870] loss: 13.314753532409668\n",
      "[step: 871] loss: 13.238153457641602\n",
      "[step: 872] loss: 13.315766334533691\n",
      "[step: 873] loss: 13.409987449645996\n",
      "[step: 874] loss: 13.39390754699707\n",
      "[step: 875] loss: 13.285902976989746\n",
      "[step: 876] loss: 13.213399887084961\n",
      "[step: 877] loss: 13.237445831298828\n",
      "[step: 878] loss: 13.298795700073242\n",
      "[step: 879] loss: 13.309868812561035\n",
      "[step: 880] loss: 13.254559516906738\n",
      "[step: 881] loss: 13.195352554321289\n",
      "[step: 882] loss: 13.185025215148926\n",
      "[step: 883] loss: 13.214675903320312\n",
      "[step: 884] loss: 13.236479759216309\n",
      "[step: 885] loss: 13.220200538635254\n",
      "[step: 886] loss: 13.181599617004395\n",
      "[step: 887] loss: 13.154861450195312\n",
      "[step: 888] loss: 13.15653133392334\n",
      "[step: 889] loss: 13.172582626342773\n",
      "[step: 890] loss: 13.178373336791992\n",
      "[step: 891] loss: 13.164722442626953\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={\n",
    "                                X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    \n",
    "    test_predict = scaler2.inverse_transform(test_predict)\n",
    "    dataY2 = scaler2.inverse_transform(dataY2) # 데이터 있을때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "888\n"
     ]
    }
   ],
   "source": [
    "print(len(test_predict))\n",
    "\n",
    "# # Plot predictions\n",
    "# plt.title(data + ' predict')\n",
    "# plt.plot(dataY2, label='row data') # 데이터 있을때\n",
    "# plt.plot(test_predict, label='predict')\n",
    "# plt.xlabel(\"Time Period\")\n",
    "# plt.ylabel(\"photometric\")\n",
    "# # plt.ylabel(\"cct\")\n",
    "# plt.xticks(np.arange(0, 892, step = 56), ['5', '6', '7', '8', '9', '10', '11', '12', '1', '2', '3', '4', '5', '6', '7', '8']) # v2 모델\n",
    "# plt.yticks(np.arange(0, 140000, step=10000))\n",
    "# # plt.yticks(np.arange(0, 60000, step=10000))\n",
    "# plt.grid(True)\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
