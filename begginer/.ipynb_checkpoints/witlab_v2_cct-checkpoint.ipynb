{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "font_name = font_manager.FontProperties(fname=\"/usr/share/fonts/truetype/nanum/NanumGothic_Coding_Bold.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "%matplotlib tk\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61.  -4.  53.  19.]\n",
      " [ 61.  -4.  53.  19.]\n",
      " [ 61.  -4.  53.  19.]\n",
      " ...\n",
      " [268. -23. 322. -14.]\n",
      " [268. -23. 322. -14.]\n",
      " [268. -23. 322. -14.]]\n",
      "[[ 9.90016  ]\n",
      " [12.2487   ]\n",
      " [15.1114   ]\n",
      " ...\n",
      " [ 0.0238816]\n",
      " [ 0.0247933]\n",
      " [ 0.0235017]]\n",
      "[[ 69. -13.  17.   7.]\n",
      " [ 69. -13.  17.   7.]\n",
      " [ 69. -13.  17.   7.]\n",
      " ...\n",
      " [289. -11.  18.   7.]\n",
      " [289. -12.  18.   7.]\n",
      " [289. -12.  18.   7.]]\n"
     ]
    }
   ],
   "source": [
    "# train Parameters\n",
    "seq_length = 4\n",
    "data_dim = 4\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 901\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler2 = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Open, High, Low, Volume, Close\n",
    "xy = np.loadtxt('./v2data/v2/train_v2_data.csv', delimiter=',')\n",
    "x = scaler.fit_transform(xy[:, 0:-1])\n",
    "y = scaler2.fit_transform(xy[:, [-1]])  # Close as label\n",
    "\n",
    "print(xy[:, 0:-1])\n",
    "print(xy[:, [-1]])\n",
    "\n",
    "# build a dataset\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, len(x) - seq_length):\n",
    "    _x = x[i:i + seq_length]\n",
    "    _y = y[i + seq_length]  # Next close price\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "\n",
    "xy2 = np.loadtxt('./sun_data/2018-04-09.csv',delimiter=',')\n",
    "print(xy2)\n",
    "\n",
    "x2 = scaler.fit_transform(xy2) # 데이터 없을때\n",
    "# x2 = scaler.fit_transform(xy2[:, 0:-1]) # 데이터 있을때\n",
    "# y2 = scaler2.fit_transform(xy2[:, [-1]]) #데이터 있을때\n",
    "\n",
    "dataX2 = []\n",
    "# dataY2 = [] #데이터 있을때\n",
    "\n",
    "for i in range(0, len(x2) - seq_length):\n",
    "    _x2 = x2[i:i+seq_length]\n",
    "#     _y2 = y2[i+seq_length] #데이터 있을때\n",
    "    dataX2.append(_x2)\n",
    "#     dataY2.append(_y2) #데이터 있을때\n",
    "# train/test split\n",
    "\n",
    "train_size = len(dataX)\n",
    "test_size = len(dataX2)\n",
    "\n",
    "trainX, testX = np.array(dataX[0:train_size]),np.array(dataX2[0:test_size])\n",
    "trainY = np.array(dataY[0:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 4217.36669921875\n",
      "[step: 1] loss: 3453.578857421875\n",
      "[step: 2] loss: 2832.28564453125\n",
      "[step: 3] loss: 2316.478515625\n",
      "[step: 4] loss: 1890.4298095703125\n",
      "[step: 5] loss: 1550.32666015625\n",
      "[step: 6] loss: 1300.350830078125\n",
      "[step: 7] loss: 1146.77392578125\n",
      "[step: 8] loss: 1087.1217041015625\n",
      "[step: 9] loss: 1097.0867919921875\n",
      "[step: 10] loss: 1132.58984375\n",
      "[step: 11] loss: 1152.9747314453125\n",
      "[step: 12] loss: 1138.0985107421875\n",
      "[step: 13] loss: 1088.2005615234375\n",
      "[step: 14] loss: 1015.3058471679688\n",
      "[step: 15] loss: 934.4859008789062\n",
      "[step: 16] loss: 858.6190795898438\n",
      "[step: 17] loss: 796.1177978515625\n",
      "[step: 18] loss: 750.4552001953125\n",
      "[step: 19] loss: 720.8535766601562\n",
      "[step: 20] loss: 703.5414428710938\n",
      "[step: 21] loss: 693.0161743164062\n",
      "[step: 22] loss: 683.1348266601562\n",
      "[step: 23] loss: 668.2427368164062\n",
      "[step: 24] loss: 644.5023803710938\n",
      "[step: 25] loss: 611.0518798828125\n",
      "[step: 26] loss: 570.3866577148438\n",
      "[step: 27] loss: 527.7991333007812\n",
      "[step: 28] loss: 490.0130920410156\n",
      "[step: 29] loss: 462.9598693847656\n",
      "[step: 30] loss: 448.6136474609375\n",
      "[step: 31] loss: 442.4314270019531\n",
      "[step: 32] loss: 434.7753601074219\n",
      "[step: 33] loss: 416.91558837890625\n",
      "[step: 34] loss: 387.10382080078125\n",
      "[step: 35] loss: 351.6371154785156\n",
      "[step: 36] loss: 320.2599182128906\n",
      "[step: 37] loss: 298.9712829589844\n",
      "[step: 38] loss: 285.02056884765625\n",
      "[step: 39] loss: 269.48388671875\n",
      "[step: 40] loss: 246.64248657226562\n",
      "[step: 41] loss: 220.1378936767578\n",
      "[step: 42] loss: 199.7480926513672\n",
      "[step: 43] loss: 192.16351318359375\n",
      "[step: 44] loss: 193.61883544921875\n",
      "[step: 45] loss: 193.89364624023438\n",
      "[step: 46] loss: 189.1119384765625\n",
      "[step: 47] loss: 185.38922119140625\n",
      "[step: 48] loss: 188.78302001953125\n",
      "[step: 49] loss: 195.89151000976562\n",
      "[step: 50] loss: 198.17774963378906\n",
      "[step: 51] loss: 193.5950164794922\n",
      "[step: 52] loss: 187.76437377929688\n",
      "[step: 53] loss: 185.12632751464844\n",
      "[step: 54] loss: 183.8114776611328\n",
      "[step: 55] loss: 180.0791778564453\n",
      "[step: 56] loss: 174.2734375\n",
      "[step: 57] loss: 169.89732360839844\n",
      "[step: 58] loss: 168.7732391357422\n",
      "[step: 59] loss: 169.2876434326172\n",
      "[step: 60] loss: 169.09043884277344\n",
      "[step: 61] loss: 167.907470703125\n",
      "[step: 62] loss: 167.14932250976562\n",
      "[step: 63] loss: 167.69371032714844\n",
      "[step: 64] loss: 168.83351135253906\n",
      "[step: 65] loss: 169.281494140625\n",
      "[step: 66] loss: 168.6390838623047\n",
      "[step: 67] loss: 167.5823974609375\n",
      "[step: 68] loss: 166.85655212402344\n",
      "[step: 69] loss: 166.44447326660156\n",
      "[step: 70] loss: 165.77101135253906\n",
      "[step: 71] loss: 164.4982147216797\n",
      "[step: 72] loss: 162.921630859375\n",
      "[step: 73] loss: 161.5924530029297\n",
      "[step: 74] loss: 160.70590209960938\n",
      "[step: 75] loss: 159.9781494140625\n",
      "[step: 76] loss: 159.09002685546875\n",
      "[step: 77] loss: 158.0980682373047\n",
      "[step: 78] loss: 157.3157196044922\n",
      "[step: 79] loss: 156.8789520263672\n",
      "[step: 80] loss: 156.57875061035156\n",
      "[step: 81] loss: 156.1414337158203\n",
      "[step: 82] loss: 155.54554748535156\n",
      "[step: 83] loss: 154.972412109375\n",
      "[step: 84] loss: 154.5102996826172\n",
      "[step: 85] loss: 154.03659057617188\n",
      "[step: 86] loss: 153.40365600585938\n",
      "[step: 87] loss: 152.63697814941406\n",
      "[step: 88] loss: 151.8797607421875\n",
      "[step: 89] loss: 151.19834899902344\n",
      "[step: 90] loss: 150.5247344970703\n",
      "[step: 91] loss: 149.7813262939453\n",
      "[step: 92] loss: 148.99154663085938\n",
      "[step: 93] loss: 148.2353057861328\n",
      "[step: 94] loss: 147.5370635986328\n",
      "[step: 95] loss: 146.84271240234375\n",
      "[step: 96] loss: 146.099853515625\n",
      "[step: 97] loss: 145.31922912597656\n",
      "[step: 98] loss: 144.54257202148438\n",
      "[step: 99] loss: 143.77525329589844\n",
      "[step: 100] loss: 142.98028564453125\n",
      "[step: 101] loss: 142.13064575195312\n",
      "[step: 102] loss: 141.24241638183594\n",
      "[step: 103] loss: 140.34637451171875\n",
      "[step: 104] loss: 139.4464569091797\n",
      "[step: 105] loss: 138.5218963623047\n",
      "[step: 106] loss: 137.56324768066406\n",
      "[step: 107] loss: 136.5872344970703\n",
      "[step: 108] loss: 135.61111450195312\n",
      "[step: 109] loss: 134.63031005859375\n",
      "[step: 110] loss: 133.62860107421875\n",
      "[step: 111] loss: 132.60284423828125\n",
      "[step: 112] loss: 131.56439208984375\n",
      "[step: 113] loss: 130.5195770263672\n",
      "[step: 114] loss: 129.46083068847656\n",
      "[step: 115] loss: 128.38082885742188\n",
      "[step: 116] loss: 127.28436279296875\n",
      "[step: 117] loss: 126.1814956665039\n",
      "[step: 118] loss: 125.07438659667969\n",
      "[step: 119] loss: 123.95870208740234\n",
      "[step: 120] loss: 122.83503723144531\n",
      "[step: 121] loss: 121.71131896972656\n",
      "[step: 122] loss: 120.59407043457031\n",
      "[step: 123] loss: 119.48282623291016\n",
      "[step: 124] loss: 118.37623596191406\n",
      "[step: 125] loss: 117.27770233154297\n",
      "[step: 126] loss: 116.19221496582031\n",
      "[step: 127] loss: 115.12084197998047\n",
      "[step: 128] loss: 114.06207275390625\n",
      "[step: 129] loss: 113.01742553710938\n",
      "[step: 130] loss: 111.99153900146484\n",
      "[step: 131] loss: 110.98743438720703\n",
      "[step: 132] loss: 110.00556945800781\n",
      "[step: 133] loss: 109.04716491699219\n",
      "[step: 134] loss: 108.11569213867188\n",
      "[step: 135] loss: 107.21422576904297\n",
      "[step: 136] loss: 106.34346771240234\n",
      "[step: 137] loss: 105.50361633300781\n",
      "[step: 138] loss: 104.6963882446289\n",
      "[step: 139] loss: 103.92340087890625\n",
      "[step: 140] loss: 103.18452453613281\n",
      "[step: 141] loss: 102.47885131835938\n",
      "[step: 142] loss: 101.80650329589844\n",
      "[step: 143] loss: 101.16773986816406\n",
      "[step: 144] loss: 100.56161499023438\n",
      "[step: 145] loss: 99.98672485351562\n",
      "[step: 146] loss: 99.44222259521484\n",
      "[step: 147] loss: 98.92755126953125\n",
      "[step: 148] loss: 98.4411849975586\n",
      "[step: 149] loss: 97.98131561279297\n",
      "[step: 150] loss: 97.5463638305664\n",
      "[step: 151] loss: 97.1349868774414\n",
      "[step: 152] loss: 96.74536895751953\n",
      "[step: 153] loss: 96.37541198730469\n",
      "[step: 154] loss: 96.02356719970703\n",
      "[step: 155] loss: 95.68859100341797\n",
      "[step: 156] loss: 95.36878967285156\n",
      "[step: 157] loss: 95.06267547607422\n",
      "[step: 158] loss: 94.76917266845703\n",
      "[step: 159] loss: 94.48716735839844\n",
      "[step: 160] loss: 94.21554565429688\n",
      "[step: 161] loss: 93.95328521728516\n",
      "[step: 162] loss: 93.69961547851562\n",
      "[step: 163] loss: 93.4538803100586\n",
      "[step: 164] loss: 93.21537017822266\n",
      "[step: 165] loss: 92.98359680175781\n",
      "[step: 166] loss: 92.7582015991211\n",
      "[step: 167] loss: 92.53894805908203\n",
      "[step: 168] loss: 92.3255615234375\n",
      "[step: 169] loss: 92.11786651611328\n",
      "[step: 170] loss: 91.91584014892578\n",
      "[step: 171] loss: 91.71939849853516\n",
      "[step: 172] loss: 91.52842712402344\n",
      "[step: 173] loss: 91.34295654296875\n",
      "[step: 174] loss: 91.16295623779297\n",
      "[step: 175] loss: 90.9884262084961\n",
      "[step: 176] loss: 90.81924438476562\n",
      "[step: 177] loss: 90.65545654296875\n",
      "[step: 178] loss: 90.49700927734375\n",
      "[step: 179] loss: 90.34386444091797\n",
      "[step: 180] loss: 90.19589233398438\n",
      "[step: 181] loss: 90.05300903320312\n",
      "[step: 182] loss: 89.91506958007812\n",
      "[step: 183] loss: 89.78193664550781\n",
      "[step: 184] loss: 89.65341186523438\n",
      "[step: 185] loss: 89.52935791015625\n",
      "[step: 186] loss: 89.40950012207031\n",
      "[step: 187] loss: 89.29368591308594\n",
      "[step: 188] loss: 89.18172454833984\n",
      "[step: 189] loss: 89.07333374023438\n",
      "[step: 190] loss: 88.96834564208984\n",
      "[step: 191] loss: 88.86653137207031\n",
      "[step: 192] loss: 88.76768493652344\n",
      "[step: 193] loss: 88.67161560058594\n",
      "[step: 194] loss: 88.57815551757812\n",
      "[step: 195] loss: 88.48702239990234\n",
      "[step: 196] loss: 88.39812469482422\n",
      "[step: 197] loss: 88.31126403808594\n",
      "[step: 198] loss: 88.22626495361328\n",
      "[step: 199] loss: 88.14302825927734\n",
      "[step: 200] loss: 88.0614242553711\n",
      "[step: 201] loss: 87.9813232421875\n",
      "[step: 202] loss: 87.90257263183594\n",
      "[step: 203] loss: 87.82513427734375\n",
      "[step: 204] loss: 87.74896240234375\n",
      "[step: 205] loss: 87.6738510131836\n",
      "[step: 206] loss: 87.59983825683594\n",
      "[step: 207] loss: 87.52684020996094\n",
      "[step: 208] loss: 87.45478057861328\n",
      "[step: 209] loss: 87.38365936279297\n",
      "[step: 210] loss: 87.3133773803711\n",
      "[step: 211] loss: 87.24395751953125\n",
      "[step: 212] loss: 87.17530822753906\n",
      "[step: 213] loss: 87.10741424560547\n",
      "[step: 214] loss: 87.04025268554688\n",
      "[step: 215] loss: 86.97379302978516\n",
      "[step: 216] loss: 86.90802001953125\n",
      "[step: 217] loss: 86.84288024902344\n",
      "[step: 218] loss: 86.77836608886719\n",
      "[step: 219] loss: 86.71440887451172\n",
      "[step: 220] loss: 86.65107727050781\n",
      "[step: 221] loss: 86.58827209472656\n",
      "[step: 222] loss: 86.52595520019531\n",
      "[step: 223] loss: 86.46414184570312\n",
      "[step: 224] loss: 86.40279388427734\n",
      "[step: 225] loss: 86.34188079833984\n",
      "[step: 226] loss: 86.28137969970703\n",
      "[step: 227] loss: 86.22132110595703\n",
      "[step: 228] loss: 86.16158294677734\n",
      "[step: 229] loss: 86.10223388671875\n",
      "[step: 230] loss: 86.04319763183594\n",
      "[step: 231] loss: 85.98446655273438\n",
      "[step: 232] loss: 85.92598724365234\n",
      "[step: 233] loss: 85.8677978515625\n",
      "[step: 234] loss: 85.80985260009766\n",
      "[step: 235] loss: 85.75211334228516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 236] loss: 85.69461822509766\n",
      "[step: 237] loss: 85.63722229003906\n",
      "[step: 238] loss: 85.58007049560547\n",
      "[step: 239] loss: 85.52303314208984\n",
      "[step: 240] loss: 85.46615600585938\n",
      "[step: 241] loss: 85.40938568115234\n",
      "[step: 242] loss: 85.35275268554688\n",
      "[step: 243] loss: 85.29618072509766\n",
      "[step: 244] loss: 85.23970794677734\n",
      "[step: 245] loss: 85.18329620361328\n",
      "[step: 246] loss: 85.126953125\n",
      "[step: 247] loss: 85.07063293457031\n",
      "[step: 248] loss: 85.0143814086914\n",
      "[step: 249] loss: 84.95811462402344\n",
      "[step: 250] loss: 84.90186309814453\n",
      "[step: 251] loss: 84.84561920166016\n",
      "[step: 252] loss: 84.78934478759766\n",
      "[step: 253] loss: 84.73310852050781\n",
      "[step: 254] loss: 84.67680358886719\n",
      "[step: 255] loss: 84.6204833984375\n",
      "[step: 256] loss: 84.56407165527344\n",
      "[step: 257] loss: 84.50767517089844\n",
      "[step: 258] loss: 84.45115661621094\n",
      "[step: 259] loss: 84.39460754394531\n",
      "[step: 260] loss: 84.3379898071289\n",
      "[step: 261] loss: 84.28126525878906\n",
      "[step: 262] loss: 84.22450256347656\n",
      "[step: 263] loss: 84.16761779785156\n",
      "[step: 264] loss: 84.11062622070312\n",
      "[step: 265] loss: 84.05354309082031\n",
      "[step: 266] loss: 83.99639129638672\n",
      "[step: 267] loss: 83.93910217285156\n",
      "[step: 268] loss: 83.8816909790039\n",
      "[step: 269] loss: 83.82418060302734\n",
      "[step: 270] loss: 83.76656341552734\n",
      "[step: 271] loss: 83.70878601074219\n",
      "[step: 272] loss: 83.6509017944336\n",
      "[step: 273] loss: 83.59288024902344\n",
      "[step: 274] loss: 83.53473663330078\n",
      "[step: 275] loss: 83.47644805908203\n",
      "[step: 276] loss: 83.41807556152344\n",
      "[step: 277] loss: 83.35954284667969\n",
      "[step: 278] loss: 83.30083465576172\n",
      "[step: 279] loss: 83.24201202392578\n",
      "[step: 280] loss: 83.18303680419922\n",
      "[step: 281] loss: 83.12395477294922\n",
      "[step: 282] loss: 83.064697265625\n",
      "[step: 283] loss: 83.00528717041016\n",
      "[step: 284] loss: 82.94577026367188\n",
      "[step: 285] loss: 82.88610076904297\n",
      "[step: 286] loss: 82.82625579833984\n",
      "[step: 287] loss: 82.76630401611328\n",
      "[step: 288] loss: 82.70618438720703\n",
      "[step: 289] loss: 82.64591217041016\n",
      "[step: 290] loss: 82.58552551269531\n",
      "[step: 291] loss: 82.52493286132812\n",
      "[step: 292] loss: 82.46428680419922\n",
      "[step: 293] loss: 82.40343475341797\n",
      "[step: 294] loss: 82.34242248535156\n",
      "[step: 295] loss: 82.28129577636719\n",
      "[step: 296] loss: 82.22000885009766\n",
      "[step: 297] loss: 82.15860748291016\n",
      "[step: 298] loss: 82.09698486328125\n",
      "[step: 299] loss: 82.03529357910156\n",
      "[step: 300] loss: 81.97343444824219\n",
      "[step: 301] loss: 81.91145324707031\n",
      "[step: 302] loss: 81.84931945800781\n",
      "[step: 303] loss: 81.78707122802734\n",
      "[step: 304] loss: 81.72461700439453\n",
      "[step: 305] loss: 81.6620864868164\n",
      "[step: 306] loss: 81.59938049316406\n",
      "[step: 307] loss: 81.53657531738281\n",
      "[step: 308] loss: 81.47362518310547\n",
      "[step: 309] loss: 81.41053009033203\n",
      "[step: 310] loss: 81.3473129272461\n",
      "[step: 311] loss: 81.28399658203125\n",
      "[step: 312] loss: 81.22051239013672\n",
      "[step: 313] loss: 81.15690612792969\n",
      "[step: 314] loss: 81.09321594238281\n",
      "[step: 315] loss: 81.02937316894531\n",
      "[step: 316] loss: 80.96542358398438\n",
      "[step: 317] loss: 80.90135192871094\n",
      "[step: 318] loss: 80.837158203125\n",
      "[step: 319] loss: 80.77290344238281\n",
      "[step: 320] loss: 80.70850372314453\n",
      "[step: 321] loss: 80.64398193359375\n",
      "[step: 322] loss: 80.57938385009766\n",
      "[step: 323] loss: 80.51470184326172\n",
      "[step: 324] loss: 80.44989776611328\n",
      "[step: 325] loss: 80.38501739501953\n",
      "[step: 326] loss: 80.32001495361328\n",
      "[step: 327] loss: 80.25492858886719\n",
      "[step: 328] loss: 80.18981170654297\n",
      "[step: 329] loss: 80.12459564208984\n",
      "[step: 330] loss: 80.0593032836914\n",
      "[step: 331] loss: 79.99392700195312\n",
      "[step: 332] loss: 79.92848205566406\n",
      "[step: 333] loss: 79.86299133300781\n",
      "[step: 334] loss: 79.79742431640625\n",
      "[step: 335] loss: 79.73182678222656\n",
      "[step: 336] loss: 79.66618347167969\n",
      "[step: 337] loss: 79.6004638671875\n",
      "[step: 338] loss: 79.53471374511719\n",
      "[step: 339] loss: 79.46893310546875\n",
      "[step: 340] loss: 79.40313720703125\n",
      "[step: 341] loss: 79.33729553222656\n",
      "[step: 342] loss: 79.27141571044922\n",
      "[step: 343] loss: 79.20556640625\n",
      "[step: 344] loss: 79.13965606689453\n",
      "[step: 345] loss: 79.07376861572266\n",
      "[step: 346] loss: 79.00788116455078\n",
      "[step: 347] loss: 78.9420166015625\n",
      "[step: 348] loss: 78.87614440917969\n",
      "[step: 349] loss: 78.81033325195312\n",
      "[step: 350] loss: 78.74446105957031\n",
      "[step: 351] loss: 78.67865753173828\n",
      "[step: 352] loss: 78.6129150390625\n",
      "[step: 353] loss: 78.54720306396484\n",
      "[step: 354] loss: 78.4815444946289\n",
      "[step: 355] loss: 78.41590881347656\n",
      "[step: 356] loss: 78.35037994384766\n",
      "[step: 357] loss: 78.284912109375\n",
      "[step: 358] loss: 78.21947479248047\n",
      "[step: 359] loss: 78.1541519165039\n",
      "[step: 360] loss: 78.08891296386719\n",
      "[step: 361] loss: 78.02375793457031\n",
      "[step: 362] loss: 77.95870208740234\n",
      "[step: 363] loss: 77.89376068115234\n",
      "[step: 364] loss: 77.82893371582031\n",
      "[step: 365] loss: 77.76419830322266\n",
      "[step: 366] loss: 77.69961547851562\n",
      "[step: 367] loss: 77.6351547241211\n",
      "[step: 368] loss: 77.5708236694336\n",
      "[step: 369] loss: 77.50663757324219\n",
      "[step: 370] loss: 77.44261169433594\n",
      "[step: 371] loss: 77.37874603271484\n",
      "[step: 372] loss: 77.3150405883789\n",
      "[step: 373] loss: 77.25151062011719\n",
      "[step: 374] loss: 77.18814086914062\n",
      "[step: 375] loss: 77.1249771118164\n",
      "[step: 376] loss: 77.0619888305664\n",
      "[step: 377] loss: 76.99919891357422\n",
      "[step: 378] loss: 76.93659210205078\n",
      "[step: 379] loss: 76.87422943115234\n",
      "[step: 380] loss: 76.81207275390625\n",
      "[step: 381] loss: 76.75010681152344\n",
      "[step: 382] loss: 76.68838500976562\n",
      "[step: 383] loss: 76.62688446044922\n",
      "[step: 384] loss: 76.56562805175781\n",
      "[step: 385] loss: 76.50462341308594\n",
      "[step: 386] loss: 76.44384002685547\n",
      "[step: 387] loss: 76.38331604003906\n",
      "[step: 388] loss: 76.32305145263672\n",
      "[step: 389] loss: 76.2630386352539\n",
      "[step: 390] loss: 76.20328521728516\n",
      "[step: 391] loss: 76.14382934570312\n",
      "[step: 392] loss: 76.08458709716797\n",
      "[step: 393] loss: 76.02568817138672\n",
      "[step: 394] loss: 75.967041015625\n",
      "[step: 395] loss: 75.90869140625\n",
      "[step: 396] loss: 75.85063934326172\n",
      "[step: 397] loss: 75.79288482666016\n",
      "[step: 398] loss: 75.73539733886719\n",
      "[step: 399] loss: 75.67823028564453\n",
      "[step: 400] loss: 75.6213607788086\n",
      "[step: 401] loss: 75.56478118896484\n",
      "[step: 402] loss: 75.50851440429688\n",
      "[step: 403] loss: 75.45257568359375\n",
      "[step: 404] loss: 75.39690399169922\n",
      "[step: 405] loss: 75.34161376953125\n",
      "[step: 406] loss: 75.28659057617188\n",
      "[step: 407] loss: 75.2319107055664\n",
      "[step: 408] loss: 75.17753601074219\n",
      "[step: 409] loss: 75.12348175048828\n",
      "[step: 410] loss: 75.06974792480469\n",
      "[step: 411] loss: 75.01634979248047\n",
      "[step: 412] loss: 74.9632339477539\n",
      "[step: 413] loss: 74.91048431396484\n",
      "[step: 414] loss: 74.85803985595703\n",
      "[step: 415] loss: 74.80592346191406\n",
      "[step: 416] loss: 74.75411224365234\n",
      "[step: 417] loss: 74.7026138305664\n",
      "[step: 418] loss: 74.65149688720703\n",
      "[step: 419] loss: 74.60064697265625\n",
      "[step: 420] loss: 74.55011749267578\n",
      "[step: 421] loss: 74.49994659423828\n",
      "[step: 422] loss: 74.45004272460938\n",
      "[step: 423] loss: 74.4004898071289\n",
      "[step: 424] loss: 74.35124969482422\n",
      "[step: 425] loss: 74.30229949951172\n",
      "[step: 426] loss: 74.25370025634766\n",
      "[step: 427] loss: 74.20537567138672\n",
      "[step: 428] loss: 74.15737915039062\n",
      "[step: 429] loss: 74.1096420288086\n",
      "[step: 430] loss: 74.062255859375\n",
      "[step: 431] loss: 74.01515197753906\n",
      "[step: 432] loss: 73.96834564208984\n",
      "[step: 433] loss: 73.92182159423828\n",
      "[step: 434] loss: 73.87557983398438\n",
      "[step: 435] loss: 73.82965087890625\n",
      "[step: 436] loss: 73.78401184082031\n",
      "[step: 437] loss: 73.73861694335938\n",
      "[step: 438] loss: 73.69352722167969\n",
      "[step: 439] loss: 73.64867401123047\n",
      "[step: 440] loss: 73.60414123535156\n",
      "[step: 441] loss: 73.55984497070312\n",
      "[step: 442] loss: 73.51580047607422\n",
      "[step: 443] loss: 73.47201538085938\n",
      "[step: 444] loss: 73.42852783203125\n",
      "[step: 445] loss: 73.38525390625\n",
      "[step: 446] loss: 73.34223175048828\n",
      "[step: 447] loss: 73.29943084716797\n",
      "[step: 448] loss: 73.25691986083984\n",
      "[step: 449] loss: 73.21459197998047\n",
      "[step: 450] loss: 73.17251586914062\n",
      "[step: 451] loss: 73.13066864013672\n",
      "[step: 452] loss: 73.0890121459961\n",
      "[step: 453] loss: 73.0475845336914\n",
      "[step: 454] loss: 73.00640106201172\n",
      "[step: 455] loss: 72.96539306640625\n",
      "[step: 456] loss: 72.92462158203125\n",
      "[step: 457] loss: 72.88402557373047\n",
      "[step: 458] loss: 72.84363555908203\n",
      "[step: 459] loss: 72.80342864990234\n",
      "[step: 460] loss: 72.76341247558594\n",
      "[step: 461] loss: 72.72359466552734\n",
      "[step: 462] loss: 72.68392181396484\n",
      "[step: 463] loss: 72.64442443847656\n",
      "[step: 464] loss: 72.60511779785156\n",
      "[step: 465] loss: 72.56598663330078\n",
      "[step: 466] loss: 72.5270004272461\n",
      "[step: 467] loss: 72.48818969726562\n",
      "[step: 468] loss: 72.44951629638672\n",
      "[step: 469] loss: 72.41101837158203\n",
      "[step: 470] loss: 72.37267303466797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 471] loss: 72.3344497680664\n",
      "[step: 472] loss: 72.29637908935547\n",
      "[step: 473] loss: 72.25845336914062\n",
      "[step: 474] loss: 72.22064208984375\n",
      "[step: 475] loss: 72.18296813964844\n",
      "[step: 476] loss: 72.14543914794922\n",
      "[step: 477] loss: 72.1080322265625\n",
      "[step: 478] loss: 72.07074737548828\n",
      "[step: 479] loss: 72.0335693359375\n",
      "[step: 480] loss: 71.99652099609375\n",
      "[step: 481] loss: 71.95960235595703\n",
      "[step: 482] loss: 71.92276000976562\n",
      "[step: 483] loss: 71.88610076904297\n",
      "[step: 484] loss: 71.84945678710938\n",
      "[step: 485] loss: 71.81295776367188\n",
      "[step: 486] loss: 71.77652740478516\n",
      "[step: 487] loss: 71.74024963378906\n",
      "[step: 488] loss: 71.70406341552734\n",
      "[step: 489] loss: 71.66793823242188\n",
      "[step: 490] loss: 71.63192749023438\n",
      "[step: 491] loss: 71.59598541259766\n",
      "[step: 492] loss: 71.5601806640625\n",
      "[step: 493] loss: 71.5244369506836\n",
      "[step: 494] loss: 71.48876190185547\n",
      "[step: 495] loss: 71.45319366455078\n",
      "[step: 496] loss: 71.4177017211914\n",
      "[step: 497] loss: 71.3823013305664\n",
      "[step: 498] loss: 71.34699249267578\n",
      "[step: 499] loss: 71.31173706054688\n",
      "[step: 500] loss: 71.27655029296875\n",
      "[step: 501] loss: 71.24146270751953\n",
      "[step: 502] loss: 71.20643615722656\n",
      "[step: 503] loss: 71.17150115966797\n",
      "[step: 504] loss: 71.13663482666016\n",
      "[step: 505] loss: 71.10183715820312\n",
      "[step: 506] loss: 71.06710052490234\n",
      "[step: 507] loss: 71.03245544433594\n",
      "[step: 508] loss: 70.99785614013672\n",
      "[step: 509] loss: 70.96336364746094\n",
      "[step: 510] loss: 70.92892456054688\n",
      "[step: 511] loss: 70.89453887939453\n",
      "[step: 512] loss: 70.86023712158203\n",
      "[step: 513] loss: 70.82598876953125\n",
      "[step: 514] loss: 70.79181671142578\n",
      "[step: 515] loss: 70.75769805908203\n",
      "[step: 516] loss: 70.72367858886719\n",
      "[step: 517] loss: 70.68970489501953\n",
      "[step: 518] loss: 70.65579223632812\n",
      "[step: 519] loss: 70.6219253540039\n",
      "[step: 520] loss: 70.58817291259766\n",
      "[step: 521] loss: 70.554443359375\n",
      "[step: 522] loss: 70.52080535888672\n",
      "[step: 523] loss: 70.48722839355469\n",
      "[step: 524] loss: 70.45370483398438\n",
      "[step: 525] loss: 70.42023468017578\n",
      "[step: 526] loss: 70.38687133789062\n",
      "[step: 527] loss: 70.35355377197266\n",
      "[step: 528] loss: 70.3203125\n",
      "[step: 529] loss: 70.287109375\n",
      "[step: 530] loss: 70.25399017333984\n",
      "[step: 531] loss: 70.22091674804688\n",
      "[step: 532] loss: 70.18792724609375\n",
      "[step: 533] loss: 70.15502166748047\n",
      "[step: 534] loss: 70.12213134765625\n",
      "[step: 535] loss: 70.0893325805664\n",
      "[step: 536] loss: 70.0566177368164\n",
      "[step: 537] loss: 70.02394104003906\n",
      "[step: 538] loss: 69.99134063720703\n",
      "[step: 539] loss: 69.9588394165039\n",
      "[step: 540] loss: 69.92633056640625\n",
      "[step: 541] loss: 69.89395904541016\n",
      "[step: 542] loss: 69.86163330078125\n",
      "[step: 543] loss: 69.82937622070312\n",
      "[step: 544] loss: 69.79717254638672\n",
      "[step: 545] loss: 69.76505279541016\n",
      "[step: 546] loss: 69.73297119140625\n",
      "[step: 547] loss: 69.70099639892578\n",
      "[step: 548] loss: 69.66905212402344\n",
      "[step: 549] loss: 69.63720703125\n",
      "[step: 550] loss: 69.60543823242188\n",
      "[step: 551] loss: 69.57369995117188\n",
      "[step: 552] loss: 69.54206085205078\n",
      "[step: 553] loss: 69.5104751586914\n",
      "[step: 554] loss: 69.47897338867188\n",
      "[step: 555] loss: 69.44754028320312\n",
      "[step: 556] loss: 69.41614532470703\n",
      "[step: 557] loss: 69.38484191894531\n",
      "[step: 558] loss: 69.35364532470703\n",
      "[step: 559] loss: 69.32247161865234\n",
      "[step: 560] loss: 69.29136657714844\n",
      "[step: 561] loss: 69.26033020019531\n",
      "[step: 562] loss: 69.22940826416016\n",
      "[step: 563] loss: 69.19850158691406\n",
      "[step: 564] loss: 69.16767120361328\n",
      "[step: 565] loss: 69.1369400024414\n",
      "[step: 566] loss: 69.10624694824219\n",
      "[step: 567] loss: 69.07563781738281\n",
      "[step: 568] loss: 69.04510498046875\n",
      "[step: 569] loss: 69.01463317871094\n",
      "[step: 570] loss: 68.98423767089844\n",
      "[step: 571] loss: 68.95392608642578\n",
      "[step: 572] loss: 68.92367553710938\n",
      "[step: 573] loss: 68.89348602294922\n",
      "[step: 574] loss: 68.86334228515625\n",
      "[step: 575] loss: 68.83329772949219\n",
      "[step: 576] loss: 68.80330657958984\n",
      "[step: 577] loss: 68.77339935302734\n",
      "[step: 578] loss: 68.74356079101562\n",
      "[step: 579] loss: 68.71378326416016\n",
      "[step: 580] loss: 68.68407440185547\n",
      "[step: 581] loss: 68.65444946289062\n",
      "[step: 582] loss: 68.6248779296875\n",
      "[step: 583] loss: 68.59538269042969\n",
      "[step: 584] loss: 68.56596374511719\n",
      "[step: 585] loss: 68.53658294677734\n",
      "[step: 586] loss: 68.50727081298828\n",
      "[step: 587] loss: 68.47802734375\n",
      "[step: 588] loss: 68.44889068603516\n",
      "[step: 589] loss: 68.41978454589844\n",
      "[step: 590] loss: 68.39073944091797\n",
      "[step: 591] loss: 68.36180114746094\n",
      "[step: 592] loss: 68.3328857421875\n",
      "[step: 593] loss: 68.3040542602539\n",
      "[step: 594] loss: 68.27527618408203\n",
      "[step: 595] loss: 68.24655151367188\n",
      "[step: 596] loss: 68.21792602539062\n",
      "[step: 597] loss: 68.18936157226562\n",
      "[step: 598] loss: 68.16084289550781\n",
      "[step: 599] loss: 68.13240051269531\n",
      "[step: 600] loss: 68.10400390625\n",
      "[step: 601] loss: 68.07565307617188\n",
      "[step: 602] loss: 68.04740142822266\n",
      "[step: 603] loss: 68.01921081542969\n",
      "[step: 604] loss: 67.99107360839844\n",
      "[step: 605] loss: 67.96296691894531\n",
      "[step: 606] loss: 67.93496704101562\n",
      "[step: 607] loss: 67.90703582763672\n",
      "[step: 608] loss: 67.87911987304688\n",
      "[step: 609] loss: 67.85125732421875\n",
      "[step: 610] loss: 67.8234634399414\n",
      "[step: 611] loss: 67.79574584960938\n",
      "[step: 612] loss: 67.76806640625\n",
      "[step: 613] loss: 67.7404556274414\n",
      "[step: 614] loss: 67.712890625\n",
      "[step: 615] loss: 67.68536376953125\n",
      "[step: 616] loss: 67.65790557861328\n",
      "[step: 617] loss: 67.63052368164062\n",
      "[step: 618] loss: 67.60315704345703\n",
      "[step: 619] loss: 67.57588195800781\n",
      "[step: 620] loss: 67.5486068725586\n",
      "[step: 621] loss: 67.52140045166016\n",
      "[step: 622] loss: 67.49427032470703\n",
      "[step: 623] loss: 67.4671630859375\n",
      "[step: 624] loss: 67.44010162353516\n",
      "[step: 625] loss: 67.41307830810547\n",
      "[step: 626] loss: 67.3861312866211\n",
      "[step: 627] loss: 67.35921478271484\n",
      "[step: 628] loss: 67.33232879638672\n",
      "[step: 629] loss: 67.30549621582031\n",
      "[step: 630] loss: 67.2787094116211\n",
      "[step: 631] loss: 67.25196075439453\n",
      "[step: 632] loss: 67.2252197265625\n",
      "[step: 633] loss: 67.19854736328125\n",
      "[step: 634] loss: 67.17192077636719\n",
      "[step: 635] loss: 67.14531707763672\n",
      "[step: 636] loss: 67.11876678466797\n",
      "[step: 637] loss: 67.09224700927734\n",
      "[step: 638] loss: 67.06572723388672\n",
      "[step: 639] loss: 67.03927612304688\n",
      "[step: 640] loss: 67.0128402709961\n",
      "[step: 641] loss: 66.9864273071289\n",
      "[step: 642] loss: 66.96006774902344\n",
      "[step: 643] loss: 66.93368530273438\n",
      "[step: 644] loss: 66.90739440917969\n",
      "[step: 645] loss: 66.8810806274414\n",
      "[step: 646] loss: 66.85480499267578\n",
      "[step: 647] loss: 66.82856750488281\n",
      "[step: 648] loss: 66.80236053466797\n",
      "[step: 649] loss: 66.77613067626953\n",
      "[step: 650] loss: 66.74996948242188\n",
      "[step: 651] loss: 66.72381591796875\n",
      "[step: 652] loss: 66.6976318359375\n",
      "[step: 653] loss: 66.67151641845703\n",
      "[step: 654] loss: 66.64539337158203\n",
      "[step: 655] loss: 66.6192855834961\n",
      "[step: 656] loss: 66.59315490722656\n",
      "[step: 657] loss: 66.56707763671875\n",
      "[step: 658] loss: 66.54100799560547\n",
      "[step: 659] loss: 66.51496124267578\n",
      "[step: 660] loss: 66.4888916015625\n",
      "[step: 661] loss: 66.46282958984375\n",
      "[step: 662] loss: 66.43677520751953\n",
      "[step: 663] loss: 66.41072082519531\n",
      "[step: 664] loss: 66.3846664428711\n",
      "[step: 665] loss: 66.35861206054688\n",
      "[step: 666] loss: 66.33256530761719\n",
      "[step: 667] loss: 66.3064956665039\n",
      "[step: 668] loss: 66.28044891357422\n",
      "[step: 669] loss: 66.25438690185547\n",
      "[step: 670] loss: 66.22830963134766\n",
      "[step: 671] loss: 66.2022476196289\n",
      "[step: 672] loss: 66.1761474609375\n",
      "[step: 673] loss: 66.15006256103516\n",
      "[step: 674] loss: 66.12394714355469\n",
      "[step: 675] loss: 66.0978012084961\n",
      "[step: 676] loss: 66.0716781616211\n",
      "[step: 677] loss: 66.0455093383789\n",
      "[step: 678] loss: 66.01935577392578\n",
      "[step: 679] loss: 65.99315643310547\n",
      "[step: 680] loss: 65.9669418334961\n",
      "[step: 681] loss: 65.94071960449219\n",
      "[step: 682] loss: 65.9144287109375\n",
      "[step: 683] loss: 65.88816833496094\n",
      "[step: 684] loss: 65.86184692382812\n",
      "[step: 685] loss: 65.83552551269531\n",
      "[step: 686] loss: 65.80913543701172\n",
      "[step: 687] loss: 65.78274536132812\n",
      "[step: 688] loss: 65.75631713867188\n",
      "[step: 689] loss: 65.72987365722656\n",
      "[step: 690] loss: 65.70337677001953\n",
      "[step: 691] loss: 65.67683410644531\n",
      "[step: 692] loss: 65.6502685546875\n",
      "[step: 693] loss: 65.6236572265625\n",
      "[step: 694] loss: 65.59701538085938\n",
      "[step: 695] loss: 65.57034301757812\n",
      "[step: 696] loss: 65.54360961914062\n",
      "[step: 697] loss: 65.51683044433594\n",
      "[step: 698] loss: 65.49002075195312\n",
      "[step: 699] loss: 65.46316528320312\n",
      "[step: 700] loss: 65.4362564086914\n",
      "[step: 701] loss: 65.40929412841797\n",
      "[step: 702] loss: 65.38227081298828\n",
      "[step: 703] loss: 65.35521697998047\n",
      "[step: 704] loss: 65.3281021118164\n",
      "[step: 705] loss: 65.30094909667969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 706] loss: 65.27371215820312\n",
      "[step: 707] loss: 65.2464370727539\n",
      "[step: 708] loss: 65.2191162109375\n",
      "[step: 709] loss: 65.19170379638672\n",
      "[step: 710] loss: 65.16425323486328\n",
      "[step: 711] loss: 65.13672637939453\n",
      "[step: 712] loss: 65.10913848876953\n",
      "[step: 713] loss: 65.08149719238281\n",
      "[step: 714] loss: 65.05377197265625\n",
      "[step: 715] loss: 65.02601623535156\n",
      "[step: 716] loss: 64.9981460571289\n",
      "[step: 717] loss: 64.9702377319336\n",
      "[step: 718] loss: 64.94225311279297\n",
      "[step: 719] loss: 64.91417694091797\n",
      "[step: 720] loss: 64.88603973388672\n",
      "[step: 721] loss: 64.85783386230469\n",
      "[step: 722] loss: 64.82954406738281\n",
      "[step: 723] loss: 64.80117797851562\n",
      "[step: 724] loss: 64.77274322509766\n",
      "[step: 725] loss: 64.74420166015625\n",
      "[step: 726] loss: 64.71561431884766\n",
      "[step: 727] loss: 64.68689727783203\n",
      "[step: 728] loss: 64.65811920166016\n",
      "[step: 729] loss: 64.62926483154297\n",
      "[step: 730] loss: 64.60031127929688\n",
      "[step: 731] loss: 64.5712890625\n",
      "[step: 732] loss: 64.54216766357422\n",
      "[step: 733] loss: 64.51292419433594\n",
      "[step: 734] loss: 64.4836196899414\n",
      "[step: 735] loss: 64.45421600341797\n",
      "[step: 736] loss: 64.42472076416016\n",
      "[step: 737] loss: 64.39513397216797\n",
      "[step: 738] loss: 64.36543273925781\n",
      "[step: 739] loss: 64.33564758300781\n",
      "[step: 740] loss: 64.3057632446289\n",
      "[step: 741] loss: 64.27574157714844\n",
      "[step: 742] loss: 64.24565124511719\n",
      "[step: 743] loss: 64.21543884277344\n",
      "[step: 744] loss: 64.18511962890625\n",
      "[step: 745] loss: 64.15471649169922\n",
      "[step: 746] loss: 64.12416076660156\n",
      "[step: 747] loss: 64.09355163574219\n",
      "[step: 748] loss: 64.06278991699219\n",
      "[step: 749] loss: 64.03192901611328\n",
      "[step: 750] loss: 64.00093841552734\n",
      "[step: 751] loss: 63.96984100341797\n",
      "[step: 752] loss: 63.93861770629883\n",
      "[step: 753] loss: 63.90728759765625\n",
      "[step: 754] loss: 63.87582778930664\n",
      "[step: 755] loss: 63.844242095947266\n",
      "[step: 756] loss: 63.812557220458984\n",
      "[step: 757] loss: 63.78075408935547\n",
      "[step: 758] loss: 63.748863220214844\n",
      "[step: 759] loss: 63.716983795166016\n",
      "[step: 760] loss: 63.68550109863281\n",
      "[step: 761] loss: 63.65563201904297\n",
      "[step: 762] loss: 63.63167953491211\n",
      "[step: 763] loss: 63.629207611083984\n",
      "[step: 764] loss: 63.699588775634766\n",
      "[step: 765] loss: 64.00162506103516\n",
      "[step: 766] loss: 64.83790588378906\n",
      "[step: 767] loss: 66.53360748291016\n",
      "[step: 768] loss: 67.99234008789062\n",
      "[step: 769] loss: 67.11266326904297\n",
      "[step: 770] loss: 64.13972473144531\n",
      "[step: 771] loss: 63.624263763427734\n",
      "[step: 772] loss: 65.53527069091797\n",
      "[step: 773] loss: 65.39192199707031\n",
      "[step: 774] loss: 63.49479675292969\n",
      "[step: 775] loss: 63.755435943603516\n",
      "[step: 776] loss: 64.86505126953125\n",
      "[step: 777] loss: 63.91724395751953\n",
      "[step: 778] loss: 63.181488037109375\n",
      "[step: 779] loss: 64.0721206665039\n",
      "[step: 780] loss: 63.954471588134766\n",
      "[step: 781] loss: 63.079254150390625\n",
      "[step: 782] loss: 63.4921989440918\n",
      "[step: 783] loss: 63.7576789855957\n",
      "[step: 784] loss: 63.07260513305664\n",
      "[step: 785] loss: 63.12053680419922\n",
      "[step: 786] loss: 63.480953216552734\n",
      "[step: 787] loss: 63.05085754394531\n",
      "[step: 788] loss: 62.88686752319336\n",
      "[step: 789] loss: 63.1997184753418\n",
      "[step: 790] loss: 62.99773025512695\n",
      "[step: 791] loss: 62.73768615722656\n",
      "[step: 792] loss: 62.936134338378906\n",
      "[step: 793] loss: 62.90727615356445\n",
      "[step: 794] loss: 62.64486312866211\n",
      "[step: 795] loss: 62.70485305786133\n",
      "[step: 796] loss: 62.77503967285156\n",
      "[step: 797] loss: 62.580509185791016\n",
      "[step: 798] loss: 62.519775390625\n",
      "[step: 799] loss: 62.607093811035156\n",
      "[step: 800] loss: 62.51431655883789\n",
      "[step: 801] loss: 62.38785171508789\n",
      "[step: 802] loss: 62.42398452758789\n",
      "[step: 803] loss: 62.41754913330078\n",
      "[step: 804] loss: 62.29729080200195\n",
      "[step: 805] loss: 62.25664138793945\n",
      "[step: 806] loss: 62.27928924560547\n",
      "[step: 807] loss: 62.216121673583984\n",
      "[step: 808] loss: 62.12925338745117\n",
      "[step: 809] loss: 62.11859130859375\n",
      "[step: 810] loss: 62.107261657714844\n",
      "[step: 811] loss: 62.03468704223633\n",
      "[step: 812] loss: 61.9749870300293\n",
      "[step: 813] loss: 61.96229553222656\n",
      "[step: 814] loss: 61.93229293823242\n",
      "[step: 815] loss: 61.86618423461914\n",
      "[step: 816] loss: 61.81689453125\n",
      "[step: 817] loss: 61.79540252685547\n",
      "[step: 818] loss: 61.759613037109375\n",
      "[step: 819] loss: 61.701087951660156\n",
      "[step: 820] loss: 61.65253448486328\n",
      "[step: 821] loss: 61.62265396118164\n",
      "[step: 822] loss: 61.58671951293945\n",
      "[step: 823] loss: 61.53453063964844\n",
      "[step: 824] loss: 61.483665466308594\n",
      "[step: 825] loss: 61.445587158203125\n",
      "[step: 826] loss: 61.409542083740234\n",
      "[step: 827] loss: 61.363651275634766\n",
      "[step: 828] loss: 61.312198638916016\n",
      "[step: 829] loss: 61.265960693359375\n",
      "[step: 830] loss: 61.225929260253906\n",
      "[step: 831] loss: 61.184364318847656\n",
      "[step: 832] loss: 61.13676834106445\n",
      "[step: 833] loss: 61.08651351928711\n",
      "[step: 834] loss: 61.03900146484375\n",
      "[step: 835] loss: 60.99489212036133\n",
      "[step: 836] loss: 60.9506950378418\n",
      "[step: 837] loss: 60.90341567993164\n",
      "[step: 838] loss: 60.853450775146484\n",
      "[step: 839] loss: 60.80317687988281\n",
      "[step: 840] loss: 60.754371643066406\n",
      "[step: 841] loss: 60.706825256347656\n",
      "[step: 842] loss: 60.65909957885742\n",
      "[step: 843] loss: 60.60996627807617\n",
      "[step: 844] loss: 60.559207916259766\n",
      "[step: 845] loss: 60.50738525390625\n",
      "[step: 846] loss: 60.45524215698242\n",
      "[step: 847] loss: 60.40324401855469\n",
      "[step: 848] loss: 60.351463317871094\n",
      "[step: 849] loss: 60.299644470214844\n",
      "[step: 850] loss: 60.24749755859375\n",
      "[step: 851] loss: 60.194847106933594\n",
      "[step: 852] loss: 60.141597747802734\n",
      "[step: 853] loss: 60.087738037109375\n",
      "[step: 854] loss: 60.033390045166016\n",
      "[step: 855] loss: 59.97865295410156\n",
      "[step: 856] loss: 59.923744201660156\n",
      "[step: 857] loss: 59.86896514892578\n",
      "[step: 858] loss: 59.81481170654297\n",
      "[step: 859] loss: 59.76237869262695\n",
      "[step: 860] loss: 59.71372985839844\n",
      "[step: 861] loss: 59.67329788208008\n",
      "[step: 862] loss: 59.65072250366211\n",
      "[step: 863] loss: 59.667205810546875\n",
      "[step: 864] loss: 59.77055358886719\n",
      "[step: 865] loss: 60.065181732177734\n",
      "[step: 866] loss: 60.76675796508789\n",
      "[step: 867] loss: 62.1981201171875\n",
      "[step: 868] loss: 64.48543548583984\n",
      "[step: 869] loss: 66.19916534423828\n",
      "[step: 870] loss: 64.7090835571289\n",
      "[step: 871] loss: 60.454566955566406\n",
      "[step: 872] loss: 59.21268844604492\n",
      "[step: 873] loss: 61.74026107788086\n",
      "[step: 874] loss: 62.35528564453125\n",
      "[step: 875] loss: 59.74781799316406\n",
      "[step: 876] loss: 59.03419494628906\n",
      "[step: 877] loss: 60.8278923034668\n",
      "[step: 878] loss: 60.5173454284668\n",
      "[step: 879] loss: 58.7371711730957\n",
      "[step: 880] loss: 59.26857376098633\n",
      "[step: 881] loss: 60.14019012451172\n",
      "[step: 882] loss: 58.99018478393555\n",
      "[step: 883] loss: 58.47669219970703\n",
      "[step: 884] loss: 59.32933044433594\n",
      "[step: 885] loss: 59.050933837890625\n",
      "[step: 886] loss: 58.21437072753906\n",
      "[step: 887] loss: 58.565185546875\n",
      "[step: 888] loss: 58.827125549316406\n",
      "[step: 889] loss: 58.17853927612305\n",
      "[step: 890] loss: 58.01445770263672\n",
      "[step: 891] loss: 58.38994598388672\n",
      "[step: 892] loss: 58.15290069580078\n",
      "[step: 893] loss: 57.724571228027344\n",
      "[step: 894] loss: 57.874271392822266\n",
      "[step: 895] loss: 57.977867126464844\n",
      "[step: 896] loss: 57.62806701660156\n",
      "[step: 897] loss: 57.45480728149414\n",
      "[step: 898] loss: 57.600318908691406\n",
      "[step: 899] loss: 57.52751541137695\n",
      "[step: 900] loss: 57.245784759521484\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={\n",
    "                                X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    \n",
    "    test_predict = scaler2.inverse_transform(test_predict)\n",
    "#     dataY2 = scaler2.inverse_transform(dataY2) # 데이터 있을때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "plt.title('2018-04-09 predict')\n",
    "# plt.plot(dataY2, label='row data') # 데이터 있을때\n",
    "plt.plot(test_predict, label='predict')\n",
    "plt.xlabel(\"Time Period\")\n",
    "plt.ylabel(\"photometric\")\n",
    "# plt.ylim(0, 140000)\n",
    "plt.xticks(np.arange(0, 901, step = 56), ['5', '6', '7', '8', '9', '10', '11', '12', '1', '2', '3', '4', '5', '6', '7', '8']) # v2 모델\n",
    "plt.yticks(np.arange(0, 140000, step=10000))\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
