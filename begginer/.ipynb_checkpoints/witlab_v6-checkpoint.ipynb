{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 그래프 외부에 출력\n",
    "%matplotlib tk\n",
    "\n",
    "tf.set_random_seed(777)  # 어느 컴퓨터에서 이 코드를 실행해도 학습 방향이 같도록, 다시 수행해도 같도록\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8019, 9, 4)\n",
      "(8019, 1)\n"
     ]
    }
   ],
   "source": [
    "# train Parameters\n",
    "# seq_length = 4\n",
    "seq_length = 9\n",
    "data_dim = 4\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 892\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1)) # 데이터 일반화\n",
    "scaler2 = MinMaxScaler(feature_range=(0, 1)) # 데이터 일반화\n",
    "\n",
    "# xy = np.loadtxt('./v3data/train_v3_data_cct.csv', delimiter=',')\n",
    "# cct, cas_swr, 446to477, uvb, ptmt (892행 5열)\n",
    "xy = np.loadtxt('./v3data/train_v3_data.csv', delimiter=',')\n",
    "\n",
    "x = scaler.fit_transform(xy[:, [1, 2, 3, 4]]) # x = 맨 마지막 ptmt 제외 모든 것\n",
    "y = scaler2.fit_transform(xy[:, [0]]) \n",
    "\n",
    "# build a dataset\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "# print(len(x))\n",
    "# print(len(y))\n",
    "# print(x[0])\n",
    "# print(y[0])\n",
    "\n",
    "for i in range(0, len(x) - seq_length): # 한 행씩 dataX, Y에 추가\n",
    "    _x = x[i:i + seq_length]\n",
    "    _y = y[i + seq_length]  # Next close price\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "#     print(np.shape(_x))\n",
    "\n",
    "print(np.shape(dataX))\n",
    "print(np.shape(dataY))\n",
    "\n",
    "data = '2018-04-21'\n",
    "# xy2 = np.loadtxt('./v3data/'+ data +'.csv',delimiter=',')\n",
    "# xy2 = np.loadtxt('./v3data/test_v3_data_cct_180331.csv', delimiter=',')\n",
    "xy2 = np.loadtxt('./v3data/new/20180421.csv', delimiter=',') # 예측할 날짜\n",
    "# xy2 = np.loadtxt('./v3data/test_v3_data_180331.csv', delimiter=',')\n",
    "\n",
    "# x2 = scaler.fit_transform(xy2) # 데이터 없을때 (데이터란 ptmt 값)\n",
    "x2 = scaler.fit_transform(xy2[:, [1, 2, 3, 4]]) # 데이터 있을때\n",
    "y2 = scaler2.fit_transform(xy2[:, [0]]) #데이터 있을때\n",
    "\n",
    "# print(np.shape(x2))\n",
    "\n",
    "dataX2 = []\n",
    "dataY2 = [] #데이터 있을때\n",
    "\n",
    "for i in range(0, len(x2) - seq_length):\n",
    "    _x2 = x2[i:i+seq_length]\n",
    "    _y2 = y2[i+seq_length] #데이터 있을때\n",
    "    dataX2.append(_x2)\n",
    "    dataY2.append(_y2) #데이터 있을때\n",
    "# train/test split\n",
    "\n",
    "train_size = len(dataX)\n",
    "test_size = len(dataX2)\n",
    "\n",
    "# print(train_size)\n",
    "# print(test_size)\n",
    "\n",
    "trainX, testX = np.array(dataX[0:train_size]),np.array(dataX2[0:test_size])\n",
    "trainY = np.array(dataY[0:train_size])\n",
    "\n",
    "# print(np.shape(trainX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gw/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 1041.018798828125\n",
      "[step: 1] loss: 414.45794677734375\n",
      "[step: 2] loss: 129.06832885742188\n",
      "[step: 3] loss: 106.75064086914062\n",
      "[step: 4] loss: 188.4075469970703\n",
      "[step: 5] loss: 231.0142059326172\n",
      "[step: 6] loss: 207.85214233398438\n",
      "[step: 7] loss: 149.35250854492188\n",
      "[step: 8] loss: 88.96247100830078\n",
      "[step: 9] loss: 47.95354461669922\n",
      "[step: 10] loss: 34.285186767578125\n",
      "[step: 11] loss: 44.24099349975586\n",
      "[step: 12] loss: 64.31188201904297\n",
      "[step: 13] loss: 78.89186096191406\n",
      "[step: 14] loss: 80.8765640258789\n",
      "[step: 15] loss: 72.11692810058594\n",
      "[step: 16] loss: 58.474510192871094\n",
      "[step: 17] loss: 45.67070770263672\n",
      "[step: 18] loss: 37.230804443359375\n",
      "[step: 19] loss: 34.1343994140625\n",
      "[step: 20] loss: 35.439693450927734\n",
      "[step: 21] loss: 39.22616958618164\n",
      "[step: 22] loss: 43.44491195678711\n",
      "[step: 23] loss: 46.50288009643555\n",
      "[step: 24] loss: 47.549068450927734\n",
      "[step: 25] loss: 46.49843978881836\n",
      "[step: 26] loss: 43.86544418334961\n",
      "[step: 27] loss: 40.49517059326172\n",
      "[step: 28] loss: 37.27686309814453\n",
      "[step: 29] loss: 34.906639099121094\n",
      "[step: 30] loss: 33.74162292480469\n",
      "[step: 31] loss: 33.76311492919922\n",
      "[step: 32] loss: 34.64201354980469\n",
      "[step: 33] loss: 35.87749481201172\n",
      "[step: 34] loss: 36.9634895324707\n",
      "[step: 35] loss: 37.53254699707031\n",
      "[step: 36] loss: 37.43785858154297\n",
      "[step: 37] loss: 36.75746536254883\n",
      "[step: 38] loss: 35.73147201538086\n",
      "[step: 39] loss: 34.662818908691406\n",
      "[step: 40] loss: 33.81723403930664\n",
      "[step: 41] loss: 33.352394104003906\n",
      "[step: 42] loss: 33.290924072265625\n",
      "[step: 43] loss: 33.537384033203125\n",
      "[step: 44] loss: 33.926177978515625\n",
      "[step: 45] loss: 34.28156280517578\n",
      "[step: 46] loss: 34.470516204833984\n",
      "[step: 47] loss: 34.43445587158203\n",
      "[step: 48] loss: 34.194156646728516\n",
      "[step: 49] loss: 33.83052444458008\n",
      "[step: 50] loss: 33.45042419433594\n",
      "[step: 51] loss: 33.14992141723633\n",
      "[step: 52] loss: 32.986122131347656\n",
      "[step: 53] loss: 32.96513748168945\n",
      "[step: 54] loss: 33.04780578613281\n",
      "[step: 55] loss: 33.16898727416992\n",
      "[step: 56] loss: 33.262691497802734\n",
      "[step: 57] loss: 33.28386306762695\n",
      "[step: 58] loss: 33.22001647949219\n",
      "[step: 59] loss: 33.090274810791016\n",
      "[step: 60] loss: 32.93378829956055\n",
      "[step: 61] loss: 32.79313278198242\n",
      "[step: 62] loss: 32.699073791503906\n",
      "[step: 63] loss: 32.6618766784668\n",
      "[step: 64] loss: 32.670963287353516\n",
      "[step: 65] loss: 32.70211410522461\n",
      "[step: 66] loss: 32.727970123291016\n",
      "[step: 67] loss: 32.72833251953125\n",
      "[step: 68] loss: 32.69609069824219\n",
      "[step: 69] loss: 32.63755798339844\n",
      "[step: 70] loss: 32.56808853149414\n",
      "[step: 71] loss: 32.504940032958984\n",
      "[step: 72] loss: 32.46057891845703\n",
      "[step: 73] loss: 32.43880844116211\n",
      "[step: 74] loss: 32.43467330932617\n",
      "[step: 75] loss: 32.437660217285156\n",
      "[step: 76] loss: 32.43662643432617\n",
      "[step: 77] loss: 32.42416763305664\n",
      "[step: 78] loss: 32.398826599121094\n",
      "[step: 79] loss: 32.36473083496094\n",
      "[step: 80] loss: 32.328983306884766\n",
      "[step: 81] loss: 32.29844665527344\n",
      "[step: 82] loss: 32.276912689208984\n",
      "[step: 83] loss: 32.26420593261719\n",
      "[step: 84] loss: 32.25685501098633\n",
      "[step: 85] loss: 32.250057220458984\n",
      "[step: 86] loss: 32.23984909057617\n",
      "[step: 87] loss: 32.224613189697266\n",
      "[step: 88] loss: 32.20530700683594\n",
      "[step: 89] loss: 32.18467330932617\n",
      "[step: 90] loss: 32.1657600402832\n",
      "[step: 91] loss: 32.15058135986328\n",
      "[step: 92] loss: 32.13943862915039\n",
      "[step: 93] loss: 32.13102722167969\n",
      "[step: 94] loss: 32.12329864501953\n",
      "[step: 95] loss: 32.114437103271484\n",
      "[step: 96] loss: 32.10360336303711\n",
      "[step: 97] loss: 32.09113311767578\n",
      "[step: 98] loss: 32.07818603515625\n",
      "[step: 99] loss: 32.06608200073242\n",
      "[step: 100] loss: 32.05569076538086\n",
      "[step: 101] loss: 32.04711151123047\n",
      "[step: 102] loss: 32.039730072021484\n",
      "[step: 103] loss: 32.032630920410156\n",
      "[step: 104] loss: 32.025081634521484\n",
      "[step: 105] loss: 32.01678466796875\n",
      "[step: 106] loss: 32.007999420166016\n",
      "[step: 107] loss: 31.999252319335938\n",
      "[step: 108] loss: 31.9910945892334\n",
      "[step: 109] loss: 31.98381996154785\n",
      "[step: 110] loss: 31.97734260559082\n",
      "[step: 111] loss: 31.97132682800293\n",
      "[step: 112] loss: 31.96533966064453\n",
      "[step: 113] loss: 31.959171295166016\n",
      "[step: 114] loss: 31.95271873474121\n",
      "[step: 115] loss: 31.946208953857422\n",
      "[step: 116] loss: 31.939868927001953\n",
      "[step: 117] loss: 31.933923721313477\n",
      "[step: 118] loss: 31.92839813232422\n",
      "[step: 119] loss: 31.923192977905273\n",
      "[step: 120] loss: 31.918100357055664\n",
      "[step: 121] loss: 31.912992477416992\n",
      "[step: 122] loss: 31.907779693603516\n",
      "[step: 123] loss: 31.902528762817383\n",
      "[step: 124] loss: 31.89735221862793\n",
      "[step: 125] loss: 31.892335891723633\n",
      "[step: 126] loss: 31.887540817260742\n",
      "[step: 127] loss: 31.88289451599121\n",
      "[step: 128] loss: 31.878332138061523\n",
      "[step: 129] loss: 31.8737850189209\n",
      "[step: 130] loss: 31.869192123413086\n",
      "[step: 131] loss: 31.86458396911621\n",
      "[step: 132] loss: 31.85999298095703\n",
      "[step: 133] loss: 31.855487823486328\n",
      "[step: 134] loss: 31.851045608520508\n",
      "[step: 135] loss: 31.84669303894043\n",
      "[step: 136] loss: 31.84237289428711\n",
      "[step: 137] loss: 31.838056564331055\n",
      "[step: 138] loss: 31.8337345123291\n",
      "[step: 139] loss: 31.829370498657227\n",
      "[step: 140] loss: 31.82501983642578\n",
      "[step: 141] loss: 31.820680618286133\n",
      "[step: 142] loss: 31.816383361816406\n",
      "[step: 143] loss: 31.812101364135742\n",
      "[step: 144] loss: 31.80783462524414\n",
      "[step: 145] loss: 31.80354118347168\n",
      "[step: 146] loss: 31.799226760864258\n",
      "[step: 147] loss: 31.794889450073242\n",
      "[step: 148] loss: 31.790538787841797\n",
      "[step: 149] loss: 31.78618621826172\n",
      "[step: 150] loss: 31.781829833984375\n",
      "[step: 151] loss: 31.7774715423584\n",
      "[step: 152] loss: 31.773115158081055\n",
      "[step: 153] loss: 31.76871109008789\n",
      "[step: 154] loss: 31.764299392700195\n",
      "[step: 155] loss: 31.75983238220215\n",
      "[step: 156] loss: 31.755374908447266\n",
      "[step: 157] loss: 31.75088119506836\n",
      "[step: 158] loss: 31.746383666992188\n",
      "[step: 159] loss: 31.741865158081055\n",
      "[step: 160] loss: 31.73731231689453\n",
      "[step: 161] loss: 31.73273468017578\n",
      "[step: 162] loss: 31.728139877319336\n",
      "[step: 163] loss: 31.72349739074707\n",
      "[step: 164] loss: 31.71883201599121\n",
      "[step: 165] loss: 31.714143753051758\n",
      "[step: 166] loss: 31.709430694580078\n",
      "[step: 167] loss: 31.704687118530273\n",
      "[step: 168] loss: 31.69991111755371\n",
      "[step: 169] loss: 31.695104598999023\n",
      "[step: 170] loss: 31.69026756286621\n",
      "[step: 171] loss: 31.68540382385254\n",
      "[step: 172] loss: 31.68050193786621\n",
      "[step: 173] loss: 31.675567626953125\n",
      "[step: 174] loss: 31.670608520507812\n",
      "[step: 175] loss: 31.66561508178711\n",
      "[step: 176] loss: 31.66057777404785\n",
      "[step: 177] loss: 31.655521392822266\n",
      "[step: 178] loss: 31.650415420532227\n",
      "[step: 179] loss: 31.645282745361328\n",
      "[step: 180] loss: 31.640111923217773\n",
      "[step: 181] loss: 31.63490867614746\n",
      "[step: 182] loss: 31.629667282104492\n",
      "[step: 183] loss: 31.624385833740234\n",
      "[step: 184] loss: 31.61908531188965\n",
      "[step: 185] loss: 31.613719940185547\n",
      "[step: 186] loss: 31.608335494995117\n",
      "[step: 187] loss: 31.602895736694336\n",
      "[step: 188] loss: 31.59743309020996\n",
      "[step: 189] loss: 31.59192657470703\n",
      "[step: 190] loss: 31.586389541625977\n",
      "[step: 191] loss: 31.580795288085938\n",
      "[step: 192] loss: 31.575176239013672\n",
      "[step: 193] loss: 31.569509506225586\n",
      "[step: 194] loss: 31.563785552978516\n",
      "[step: 195] loss: 31.558032989501953\n",
      "[step: 196] loss: 31.5522518157959\n",
      "[step: 197] loss: 31.546405792236328\n",
      "[step: 198] loss: 31.540512084960938\n",
      "[step: 199] loss: 31.534584045410156\n",
      "[step: 200] loss: 31.528614044189453\n",
      "[step: 201] loss: 31.522594451904297\n",
      "[step: 202] loss: 31.516525268554688\n",
      "[step: 203] loss: 31.510414123535156\n",
      "[step: 204] loss: 31.50425148010254\n",
      "[step: 205] loss: 31.498044967651367\n",
      "[step: 206] loss: 31.49178123474121\n",
      "[step: 207] loss: 31.4854736328125\n",
      "[step: 208] loss: 31.47911834716797\n",
      "[step: 209] loss: 31.472700119018555\n",
      "[step: 210] loss: 31.46624755859375\n",
      "[step: 211] loss: 31.4597225189209\n",
      "[step: 212] loss: 31.45316505432129\n",
      "[step: 213] loss: 31.446542739868164\n",
      "[step: 214] loss: 31.439861297607422\n",
      "[step: 215] loss: 31.433134078979492\n",
      "[step: 216] loss: 31.42635154724121\n",
      "[step: 217] loss: 31.41950225830078\n",
      "[step: 218] loss: 31.4126033782959\n",
      "[step: 219] loss: 31.4056453704834\n",
      "[step: 220] loss: 31.398622512817383\n",
      "[step: 221] loss: 31.391538619995117\n",
      "[step: 222] loss: 31.384395599365234\n",
      "[step: 223] loss: 31.377199172973633\n",
      "[step: 224] loss: 31.36993408203125\n",
      "[step: 225] loss: 31.362600326538086\n",
      "[step: 226] loss: 31.355195999145508\n",
      "[step: 227] loss: 31.347742080688477\n",
      "[step: 228] loss: 31.340211868286133\n",
      "[step: 229] loss: 31.332622528076172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 230] loss: 31.324962615966797\n",
      "[step: 231] loss: 31.317211151123047\n",
      "[step: 232] loss: 31.309415817260742\n",
      "[step: 233] loss: 31.30154037475586\n",
      "[step: 234] loss: 31.293598175048828\n",
      "[step: 235] loss: 31.285558700561523\n",
      "[step: 236] loss: 31.277467727661133\n",
      "[step: 237] loss: 31.269296646118164\n",
      "[step: 238] loss: 31.26105308532715\n",
      "[step: 239] loss: 31.252714157104492\n",
      "[step: 240] loss: 31.244306564331055\n",
      "[step: 241] loss: 31.235816955566406\n",
      "[step: 242] loss: 31.227249145507812\n",
      "[step: 243] loss: 31.21858787536621\n",
      "[step: 244] loss: 31.209848403930664\n",
      "[step: 245] loss: 31.201030731201172\n",
      "[step: 246] loss: 31.192113876342773\n",
      "[step: 247] loss: 31.183109283447266\n",
      "[step: 248] loss: 31.174030303955078\n",
      "[step: 249] loss: 31.164840698242188\n",
      "[step: 250] loss: 31.155563354492188\n",
      "[step: 251] loss: 31.146202087402344\n",
      "[step: 252] loss: 31.136730194091797\n",
      "[step: 253] loss: 31.12717628479004\n",
      "[step: 254] loss: 31.11752700805664\n",
      "[step: 255] loss: 31.107772827148438\n",
      "[step: 256] loss: 31.097909927368164\n",
      "[step: 257] loss: 31.08795166015625\n",
      "[step: 258] loss: 31.077890396118164\n",
      "[step: 259] loss: 31.067718505859375\n",
      "[step: 260] loss: 31.057451248168945\n",
      "[step: 261] loss: 31.047069549560547\n",
      "[step: 262] loss: 31.03656578063965\n",
      "[step: 263] loss: 31.025949478149414\n",
      "[step: 264] loss: 31.01523780822754\n",
      "[step: 265] loss: 31.00440788269043\n",
      "[step: 266] loss: 30.993440628051758\n",
      "[step: 267] loss: 30.982372283935547\n",
      "[step: 268] loss: 30.971181869506836\n",
      "[step: 269] loss: 30.95985984802246\n",
      "[step: 270] loss: 30.94841766357422\n",
      "[step: 271] loss: 30.936859130859375\n",
      "[step: 272] loss: 30.9251651763916\n",
      "[step: 273] loss: 30.91333770751953\n",
      "[step: 274] loss: 30.901378631591797\n",
      "[step: 275] loss: 30.889291763305664\n",
      "[step: 276] loss: 30.877071380615234\n",
      "[step: 277] loss: 30.864700317382812\n",
      "[step: 278] loss: 30.852203369140625\n",
      "[step: 279] loss: 30.83957290649414\n",
      "[step: 280] loss: 30.826780319213867\n",
      "[step: 281] loss: 30.813852310180664\n",
      "[step: 282] loss: 30.8007755279541\n",
      "[step: 283] loss: 30.787561416625977\n",
      "[step: 284] loss: 30.77418327331543\n",
      "[step: 285] loss: 30.760648727416992\n",
      "[step: 286] loss: 30.74696159362793\n",
      "[step: 287] loss: 30.73311424255371\n",
      "[step: 288] loss: 30.719112396240234\n",
      "[step: 289] loss: 30.7049560546875\n",
      "[step: 290] loss: 30.690635681152344\n",
      "[step: 291] loss: 30.676149368286133\n",
      "[step: 292] loss: 30.6614990234375\n",
      "[step: 293] loss: 30.646669387817383\n",
      "[step: 294] loss: 30.631683349609375\n",
      "[step: 295] loss: 30.61651039123535\n",
      "[step: 296] loss: 30.601167678833008\n",
      "[step: 297] loss: 30.585649490356445\n",
      "[step: 298] loss: 30.569948196411133\n",
      "[step: 299] loss: 30.554075241088867\n",
      "[step: 300] loss: 30.538026809692383\n",
      "[step: 301] loss: 30.521770477294922\n",
      "[step: 302] loss: 30.505340576171875\n",
      "[step: 303] loss: 30.48873519897461\n",
      "[step: 304] loss: 30.471925735473633\n",
      "[step: 305] loss: 30.454927444458008\n",
      "[step: 306] loss: 30.437755584716797\n",
      "[step: 307] loss: 30.420385360717773\n",
      "[step: 308] loss: 30.402801513671875\n",
      "[step: 309] loss: 30.385038375854492\n",
      "[step: 310] loss: 30.36707305908203\n",
      "[step: 311] loss: 30.348913192749023\n",
      "[step: 312] loss: 30.330556869506836\n",
      "[step: 313] loss: 30.31199073791504\n",
      "[step: 314] loss: 30.293237686157227\n",
      "[step: 315] loss: 30.27427864074707\n",
      "[step: 316] loss: 30.255115509033203\n",
      "[step: 317] loss: 30.235761642456055\n",
      "[step: 318] loss: 30.216182708740234\n",
      "[step: 319] loss: 30.19643211364746\n",
      "[step: 320] loss: 30.176456451416016\n",
      "[step: 321] loss: 30.156301498413086\n",
      "[step: 322] loss: 30.13593864440918\n",
      "[step: 323] loss: 30.11536979675293\n",
      "[step: 324] loss: 30.0946102142334\n",
      "[step: 325] loss: 30.07364845275879\n",
      "[step: 326] loss: 30.052494049072266\n",
      "[step: 327] loss: 30.031139373779297\n",
      "[step: 328] loss: 30.009592056274414\n",
      "[step: 329] loss: 29.987855911254883\n",
      "[step: 330] loss: 29.965944290161133\n",
      "[step: 331] loss: 29.94383430480957\n",
      "[step: 332] loss: 29.92154884338379\n",
      "[step: 333] loss: 29.899078369140625\n",
      "[step: 334] loss: 29.876447677612305\n",
      "[step: 335] loss: 29.853628158569336\n",
      "[step: 336] loss: 29.830646514892578\n",
      "[step: 337] loss: 29.8075008392334\n",
      "[step: 338] loss: 29.78420066833496\n",
      "[step: 339] loss: 29.760746002197266\n",
      "[step: 340] loss: 29.737136840820312\n",
      "[step: 341] loss: 29.713409423828125\n",
      "[step: 342] loss: 29.689533233642578\n",
      "[step: 343] loss: 29.6655216217041\n",
      "[step: 344] loss: 29.641401290893555\n",
      "[step: 345] loss: 29.61716079711914\n",
      "[step: 346] loss: 29.59280014038086\n",
      "[step: 347] loss: 29.56835174560547\n",
      "[step: 348] loss: 29.54381561279297\n",
      "[step: 349] loss: 29.519193649291992\n",
      "[step: 350] loss: 29.494503021240234\n",
      "[step: 351] loss: 29.4697322845459\n",
      "[step: 352] loss: 29.444934844970703\n",
      "[step: 353] loss: 29.420068740844727\n",
      "[step: 354] loss: 29.39516830444336\n",
      "[step: 355] loss: 29.370243072509766\n",
      "[step: 356] loss: 29.345321655273438\n",
      "[step: 357] loss: 29.32037925720215\n",
      "[step: 358] loss: 29.295448303222656\n",
      "[step: 359] loss: 29.270540237426758\n",
      "[step: 360] loss: 29.245656967163086\n",
      "[step: 361] loss: 29.220813751220703\n",
      "[step: 362] loss: 29.196025848388672\n",
      "[step: 363] loss: 29.171295166015625\n",
      "[step: 364] loss: 29.146642684936523\n",
      "[step: 365] loss: 29.1220703125\n",
      "[step: 366] loss: 29.097593307495117\n",
      "[step: 367] loss: 29.07325553894043\n",
      "[step: 368] loss: 29.048992156982422\n",
      "[step: 369] loss: 29.024890899658203\n",
      "[step: 370] loss: 29.00090789794922\n",
      "[step: 371] loss: 28.977083206176758\n",
      "[step: 372] loss: 28.953432083129883\n",
      "[step: 373] loss: 28.92993927001953\n",
      "[step: 374] loss: 28.906618118286133\n",
      "[step: 375] loss: 28.883487701416016\n",
      "[step: 376] loss: 28.860565185546875\n",
      "[step: 377] loss: 28.83783721923828\n",
      "[step: 378] loss: 28.815332412719727\n",
      "[step: 379] loss: 28.79304313659668\n",
      "[step: 380] loss: 28.77096939086914\n",
      "[step: 381] loss: 28.749135971069336\n",
      "[step: 382] loss: 28.72754669189453\n",
      "[step: 383] loss: 28.706188201904297\n",
      "[step: 384] loss: 28.685070037841797\n",
      "[step: 385] loss: 28.66420555114746\n",
      "[step: 386] loss: 28.643596649169922\n",
      "[step: 387] loss: 28.623231887817383\n",
      "[step: 388] loss: 28.60311508178711\n",
      "[step: 389] loss: 28.583271026611328\n",
      "[step: 390] loss: 28.563657760620117\n",
      "[step: 391] loss: 28.544321060180664\n",
      "[step: 392] loss: 28.52523422241211\n",
      "[step: 393] loss: 28.506385803222656\n",
      "[step: 394] loss: 28.4877986907959\n",
      "[step: 395] loss: 28.469453811645508\n",
      "[step: 396] loss: 28.451337814331055\n",
      "[step: 397] loss: 28.43347930908203\n",
      "[step: 398] loss: 28.41583824157715\n",
      "[step: 399] loss: 28.398439407348633\n",
      "[step: 400] loss: 28.381248474121094\n",
      "[step: 401] loss: 28.364282608032227\n",
      "[step: 402] loss: 28.347532272338867\n",
      "[step: 403] loss: 28.330991744995117\n",
      "[step: 404] loss: 28.314647674560547\n",
      "[step: 405] loss: 28.298503875732422\n",
      "[step: 406] loss: 28.282541275024414\n",
      "[step: 407] loss: 28.266754150390625\n",
      "[step: 408] loss: 28.251155853271484\n",
      "[step: 409] loss: 28.23572540283203\n",
      "[step: 410] loss: 28.220455169677734\n",
      "[step: 411] loss: 28.205341339111328\n",
      "[step: 412] loss: 28.190380096435547\n",
      "[step: 413] loss: 28.17557716369629\n",
      "[step: 414] loss: 28.160905838012695\n",
      "[step: 415] loss: 28.14635467529297\n",
      "[step: 416] loss: 28.131956100463867\n",
      "[step: 417] loss: 28.117677688598633\n",
      "[step: 418] loss: 28.10352325439453\n",
      "[step: 419] loss: 28.08949089050293\n",
      "[step: 420] loss: 28.075576782226562\n",
      "[step: 421] loss: 28.0617733001709\n",
      "[step: 422] loss: 28.048086166381836\n",
      "[step: 423] loss: 28.034502029418945\n",
      "[step: 424] loss: 28.02104949951172\n",
      "[step: 425] loss: 28.0076904296875\n",
      "[step: 426] loss: 27.994447708129883\n",
      "[step: 427] loss: 27.981325149536133\n",
      "[step: 428] loss: 27.96831512451172\n",
      "[step: 429] loss: 27.955421447753906\n",
      "[step: 430] loss: 27.942644119262695\n",
      "[step: 431] loss: 27.930002212524414\n",
      "[step: 432] loss: 27.917476654052734\n",
      "[step: 433] loss: 27.905086517333984\n",
      "[step: 434] loss: 27.892841339111328\n",
      "[step: 435] loss: 27.880725860595703\n",
      "[step: 436] loss: 27.8687744140625\n",
      "[step: 437] loss: 27.856979370117188\n",
      "[step: 438] loss: 27.84532928466797\n",
      "[step: 439] loss: 27.833860397338867\n",
      "[step: 440] loss: 27.82255744934082\n",
      "[step: 441] loss: 27.81143569946289\n",
      "[step: 442] loss: 27.800487518310547\n",
      "[step: 443] loss: 27.789724349975586\n",
      "[step: 444] loss: 27.77919578552246\n",
      "[step: 445] loss: 27.769180297851562\n",
      "[step: 446] loss: 27.76274871826172\n",
      "[step: 447] loss: 27.795236587524414\n",
      "[step: 448] loss: 28.280370712280273\n",
      "[step: 449] loss: 30.884328842163086\n",
      "[step: 450] loss: 32.95155715942383\n",
      "[step: 451] loss: 34.309356689453125\n",
      "[step: 452] loss: 31.39235496520996\n",
      "[step: 453] loss: 29.83371925354004\n",
      "[step: 454] loss: 33.44355010986328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 455] loss: 28.50031089782715\n",
      "[step: 456] loss: 29.827543258666992\n",
      "[step: 457] loss: 31.36321258544922\n",
      "[step: 458] loss: 29.673809051513672\n",
      "[step: 459] loss: 28.66693115234375\n",
      "[step: 460] loss: 30.541711807250977\n",
      "[step: 461] loss: 29.171913146972656\n",
      "[step: 462] loss: 28.536596298217773\n",
      "[step: 463] loss: 29.26346778869629\n",
      "[step: 464] loss: 29.675735473632812\n",
      "[step: 465] loss: 29.3864803314209\n",
      "[step: 466] loss: 28.876964569091797\n",
      "[step: 467] loss: 28.682241439819336\n",
      "[step: 468] loss: 28.7962703704834\n",
      "[step: 469] loss: 28.707801818847656\n",
      "[step: 470] loss: 28.441619873046875\n",
      "[step: 471] loss: 28.559431076049805\n",
      "[step: 472] loss: 28.760452270507812\n",
      "[step: 473] loss: 28.514358520507812\n",
      "[step: 474] loss: 28.19312286376953\n",
      "[step: 475] loss: 28.2686710357666\n",
      "[step: 476] loss: 28.440122604370117\n",
      "[step: 477] loss: 28.365270614624023\n",
      "[step: 478] loss: 28.25907325744629\n",
      "[step: 479] loss: 28.29360580444336\n",
      "[step: 480] loss: 28.281002044677734\n",
      "[step: 481] loss: 28.164749145507812\n",
      "[step: 482] loss: 28.20532989501953\n",
      "[step: 483] loss: 28.308143615722656\n",
      "[step: 484] loss: 28.182193756103516\n",
      "[step: 485] loss: 28.103330612182617\n",
      "[step: 486] loss: 28.157819747924805\n",
      "[step: 487] loss: 28.133872985839844\n",
      "[step: 488] loss: 28.072322845458984\n",
      "[step: 489] loss: 28.087915420532227\n",
      "[step: 490] loss: 28.069015502929688\n",
      "[step: 491] loss: 27.98098373413086\n",
      "[step: 492] loss: 27.976518630981445\n",
      "[step: 493] loss: 28.000102996826172\n",
      "[step: 494] loss: 27.95227813720703\n",
      "[step: 495] loss: 27.928874969482422\n",
      "[step: 496] loss: 27.92471694946289\n",
      "[step: 497] loss: 27.87871551513672\n",
      "[step: 498] loss: 27.87456703186035\n",
      "[step: 499] loss: 27.884109497070312\n",
      "[step: 500] loss: 27.848567962646484\n",
      "[step: 501] loss: 27.82909393310547\n",
      "[step: 502] loss: 27.824684143066406\n",
      "[step: 503] loss: 27.794240951538086\n",
      "[step: 504] loss: 27.793691635131836\n",
      "[step: 505] loss: 27.784832000732422\n",
      "[step: 506] loss: 27.7554874420166\n",
      "[step: 507] loss: 27.750829696655273\n",
      "[step: 508] loss: 27.72915267944336\n",
      "[step: 509] loss: 27.72076988220215\n",
      "[step: 510] loss: 27.714584350585938\n",
      "[step: 511] loss: 27.692691802978516\n",
      "[step: 512] loss: 27.687856674194336\n",
      "[step: 513] loss: 27.668560028076172\n",
      "[step: 514] loss: 27.664697647094727\n",
      "[step: 515] loss: 27.65322494506836\n",
      "[step: 516] loss: 27.643999099731445\n",
      "[step: 517] loss: 27.633342742919922\n",
      "[step: 518] loss: 27.623069763183594\n",
      "[step: 519] loss: 27.616710662841797\n",
      "[step: 520] loss: 27.607725143432617\n",
      "[step: 521] loss: 27.600967407226562\n",
      "[step: 522] loss: 27.59119987487793\n",
      "[step: 523] loss: 27.583723068237305\n",
      "[step: 524] loss: 27.577117919921875\n",
      "[step: 525] loss: 27.56941032409668\n",
      "[step: 526] loss: 27.564403533935547\n",
      "[step: 527] loss: 27.554698944091797\n",
      "[step: 528] loss: 27.550662994384766\n",
      "[step: 529] loss: 27.5424861907959\n",
      "[step: 530] loss: 27.538047790527344\n",
      "[step: 531] loss: 27.5322322845459\n",
      "[step: 532] loss: 27.525144577026367\n",
      "[step: 533] loss: 27.521299362182617\n",
      "[step: 534] loss: 27.51503562927246\n",
      "[step: 535] loss: 27.510372161865234\n",
      "[step: 536] loss: 27.506534576416016\n",
      "[step: 537] loss: 27.500503540039062\n",
      "[step: 538] loss: 27.496170043945312\n",
      "[step: 539] loss: 27.49274444580078\n",
      "[step: 540] loss: 27.487810134887695\n",
      "[step: 541] loss: 27.483509063720703\n",
      "[step: 542] loss: 27.48021125793457\n",
      "[step: 543] loss: 27.47620964050293\n",
      "[step: 544] loss: 27.47191047668457\n",
      "[step: 545] loss: 27.46843147277832\n",
      "[step: 546] loss: 27.46544075012207\n",
      "[step: 547] loss: 27.46216583251953\n",
      "[step: 548] loss: 27.458515167236328\n",
      "[step: 549] loss: 27.45499610900879\n",
      "[step: 550] loss: 27.451904296875\n",
      "[step: 551] loss: 27.449148178100586\n",
      "[step: 552] loss: 27.446590423583984\n",
      "[step: 553] loss: 27.444183349609375\n",
      "[step: 554] loss: 27.442026138305664\n",
      "[step: 555] loss: 27.440187454223633\n",
      "[step: 556] loss: 27.439376831054688\n",
      "[step: 557] loss: 27.440446853637695\n",
      "[step: 558] loss: 27.448284149169922\n",
      "[step: 559] loss: 27.468671798706055\n",
      "[step: 560] loss: 27.5467529296875\n",
      "[step: 561] loss: 27.681150436401367\n",
      "[step: 562] loss: 28.23785972595215\n",
      "[step: 563] loss: 27.857336044311523\n",
      "[step: 564] loss: 27.85054588317871\n",
      "[step: 565] loss: 27.478954315185547\n",
      "[step: 566] loss: 27.477888107299805\n",
      "[step: 567] loss: 27.74517250061035\n",
      "[step: 568] loss: 27.647314071655273\n",
      "[step: 569] loss: 27.561189651489258\n",
      "[step: 570] loss: 27.42831802368164\n",
      "[step: 571] loss: 27.48964500427246\n",
      "[step: 572] loss: 27.626686096191406\n",
      "[step: 573] loss: 27.486557006835938\n",
      "[step: 574] loss: 27.42780113220215\n",
      "[step: 575] loss: 27.49579620361328\n",
      "[step: 576] loss: 27.493257522583008\n",
      "[step: 577] loss: 27.444995880126953\n",
      "[step: 578] loss: 27.420024871826172\n",
      "[step: 579] loss: 27.458932876586914\n",
      "[step: 580] loss: 27.491622924804688\n",
      "[step: 581] loss: 27.437936782836914\n",
      "[step: 582] loss: 27.41312599182129\n",
      "[step: 583] loss: 27.43783950805664\n",
      "[step: 584] loss: 27.451126098632812\n",
      "[step: 585] loss: 27.4390811920166\n",
      "[step: 586] loss: 27.411447525024414\n",
      "[step: 587] loss: 27.41215705871582\n",
      "[step: 588] loss: 27.433094024658203\n",
      "[step: 589] loss: 27.439258575439453\n",
      "[step: 590] loss: 27.43462562561035\n",
      "[step: 591] loss: 27.413604736328125\n",
      "[step: 592] loss: 27.402244567871094\n",
      "[step: 593] loss: 27.4036922454834\n",
      "[step: 594] loss: 27.412370681762695\n",
      "[step: 595] loss: 27.42304801940918\n",
      "[step: 596] loss: 27.423465728759766\n",
      "[step: 597] loss: 27.42501449584961\n",
      "[step: 598] loss: 27.41691017150879\n",
      "[step: 599] loss: 27.412986755371094\n",
      "[step: 600] loss: 27.406028747558594\n",
      "[step: 601] loss: 27.40290069580078\n",
      "[step: 602] loss: 27.399320602416992\n",
      "[step: 603] loss: 27.398479461669922\n",
      "[step: 604] loss: 27.39820098876953\n",
      "[step: 605] loss: 27.402652740478516\n",
      "[step: 606] loss: 27.410921096801758\n",
      "[step: 607] loss: 27.439550399780273\n",
      "[step: 608] loss: 27.48405647277832\n",
      "[step: 609] loss: 27.64523696899414\n",
      "[step: 610] loss: 27.721128463745117\n",
      "[step: 611] loss: 28.121732711791992\n",
      "[step: 612] loss: 27.589448928833008\n",
      "[step: 613] loss: 27.406091690063477\n",
      "[step: 614] loss: 27.42823028564453\n",
      "[step: 615] loss: 27.567153930664062\n",
      "[step: 616] loss: 27.800390243530273\n",
      "[step: 617] loss: 27.525625228881836\n",
      "[step: 618] loss: 27.396366119384766\n",
      "[step: 619] loss: 27.421846389770508\n",
      "[step: 620] loss: 27.50528907775879\n",
      "[step: 621] loss: 27.549049377441406\n",
      "[step: 622] loss: 27.413774490356445\n",
      "[step: 623] loss: 27.409622192382812\n",
      "[step: 624] loss: 27.50419044494629\n",
      "[step: 625] loss: 27.465221405029297\n",
      "[step: 626] loss: 27.405107498168945\n",
      "[step: 627] loss: 27.39129638671875\n",
      "[step: 628] loss: 27.433256149291992\n",
      "[step: 629] loss: 27.462217330932617\n",
      "[step: 630] loss: 27.40788459777832\n",
      "[step: 631] loss: 27.38570785522461\n",
      "[step: 632] loss: 27.412809371948242\n",
      "[step: 633] loss: 27.42709732055664\n",
      "[step: 634] loss: 27.4167537689209\n",
      "[step: 635] loss: 27.38737678527832\n",
      "[step: 636] loss: 27.385156631469727\n",
      "[step: 637] loss: 27.404735565185547\n",
      "[step: 638] loss: 27.41168785095215\n",
      "[step: 639] loss: 27.40538787841797\n",
      "[step: 640] loss: 27.385787963867188\n",
      "[step: 641] loss: 27.378585815429688\n",
      "[step: 642] loss: 27.3857364654541\n",
      "[step: 643] loss: 27.395647048950195\n",
      "[step: 644] loss: 27.402549743652344\n",
      "[step: 645] loss: 27.395097732543945\n",
      "[step: 646] loss: 27.38656997680664\n",
      "[step: 647] loss: 27.37741470336914\n",
      "[step: 648] loss: 27.373401641845703\n",
      "[step: 649] loss: 27.374191284179688\n",
      "[step: 650] loss: 27.378002166748047\n",
      "[step: 651] loss: 27.383947372436523\n",
      "[step: 652] loss: 27.388811111450195\n",
      "[step: 653] loss: 27.397693634033203\n",
      "[step: 654] loss: 27.403196334838867\n",
      "[step: 655] loss: 27.420522689819336\n",
      "[step: 656] loss: 27.43045997619629\n",
      "[step: 657] loss: 27.46961212158203\n",
      "[step: 658] loss: 27.481849670410156\n",
      "[step: 659] loss: 27.554845809936523\n",
      "[step: 660] loss: 27.53443717956543\n",
      "[step: 661] loss: 27.59258270263672\n",
      "[step: 662] loss: 27.50263786315918\n",
      "[step: 663] loss: 27.4688720703125\n",
      "[step: 664] loss: 27.401765823364258\n",
      "[step: 665] loss: 27.371978759765625\n",
      "[step: 666] loss: 27.371383666992188\n",
      "[step: 667] loss: 27.3920955657959\n",
      "[step: 668] loss: 27.425479888916016\n",
      "[step: 669] loss: 27.43291473388672\n",
      "[step: 670] loss: 27.441205978393555\n",
      "[step: 671] loss: 27.40799903869629\n",
      "[step: 672] loss: 27.383621215820312\n",
      "[step: 673] loss: 27.368148803710938\n",
      "[step: 674] loss: 27.370336532592773\n",
      "[step: 675] loss: 27.385244369506836\n",
      "[step: 676] loss: 27.399303436279297\n",
      "[step: 677] loss: 27.415264129638672\n",
      "[step: 678] loss: 27.409456253051758\n",
      "[step: 679] loss: 27.406160354614258\n",
      "[step: 680] loss: 27.38831901550293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 681] loss: 27.375703811645508\n",
      "[step: 682] loss: 27.366626739501953\n",
      "[step: 683] loss: 27.36449432373047\n",
      "[step: 684] loss: 27.368019104003906\n",
      "[step: 685] loss: 27.374692916870117\n",
      "[step: 686] loss: 27.384838104248047\n",
      "[step: 687] loss: 27.39311981201172\n",
      "[step: 688] loss: 27.40869140625\n",
      "[step: 689] loss: 27.413116455078125\n",
      "[step: 690] loss: 27.429161071777344\n",
      "[step: 691] loss: 27.42380714416504\n",
      "[step: 692] loss: 27.43474006652832\n",
      "[step: 693] loss: 27.4227237701416\n",
      "[step: 694] loss: 27.42777442932129\n",
      "[step: 695] loss: 27.413042068481445\n",
      "[step: 696] loss: 27.41114044189453\n",
      "[step: 697] loss: 27.395910263061523\n",
      "[step: 698] loss: 27.38822364807129\n",
      "[step: 699] loss: 27.376474380493164\n",
      "[step: 700] loss: 27.3692626953125\n",
      "[step: 701] loss: 27.363895416259766\n",
      "[step: 702] loss: 27.3613338470459\n",
      "[step: 703] loss: 27.36070442199707\n",
      "[step: 704] loss: 27.361495971679688\n",
      "[step: 705] loss: 27.363527297973633\n",
      "[step: 706] loss: 27.366474151611328\n",
      "[step: 707] loss: 27.37143898010254\n",
      "[step: 708] loss: 27.377071380615234\n",
      "[step: 709] loss: 27.38908576965332\n",
      "[step: 710] loss: 27.4018611907959\n",
      "[step: 711] loss: 27.435901641845703\n",
      "[step: 712] loss: 27.46299934387207\n",
      "[step: 713] loss: 27.549156188964844\n",
      "[step: 714] loss: 27.554872512817383\n",
      "[step: 715] loss: 27.644521713256836\n",
      "[step: 716] loss: 27.522878646850586\n",
      "[step: 717] loss: 27.45953369140625\n",
      "[step: 718] loss: 27.378101348876953\n",
      "[step: 719] loss: 27.361438751220703\n",
      "[step: 720] loss: 27.39751434326172\n",
      "[step: 721] loss: 27.43563461303711\n",
      "[step: 722] loss: 27.464462280273438\n",
      "[step: 723] loss: 27.41258430480957\n",
      "[step: 724] loss: 27.37196159362793\n",
      "[step: 725] loss: 27.36355972290039\n",
      "[step: 726] loss: 27.38748550415039\n",
      "[step: 727] loss: 27.412891387939453\n",
      "[step: 728] loss: 27.395557403564453\n",
      "[step: 729] loss: 27.372114181518555\n",
      "[step: 730] loss: 27.360002517700195\n",
      "[step: 731] loss: 27.36952781677246\n",
      "[step: 732] loss: 27.38600730895996\n",
      "[step: 733] loss: 27.384002685546875\n",
      "[step: 734] loss: 27.37250328063965\n",
      "[step: 735] loss: 27.360239028930664\n",
      "[step: 736] loss: 27.360427856445312\n",
      "[step: 737] loss: 27.369672775268555\n",
      "[step: 738] loss: 27.375728607177734\n",
      "[step: 739] loss: 27.375951766967773\n",
      "[step: 740] loss: 27.367326736450195\n",
      "[step: 741] loss: 27.360000610351562\n",
      "[step: 742] loss: 27.357315063476562\n",
      "[step: 743] loss: 27.359752655029297\n",
      "[step: 744] loss: 27.364503860473633\n",
      "[step: 745] loss: 27.367210388183594\n",
      "[step: 746] loss: 27.36805534362793\n",
      "[step: 747] loss: 27.364892959594727\n",
      "[step: 748] loss: 27.36142349243164\n",
      "[step: 749] loss: 27.357852935791016\n",
      "[step: 750] loss: 27.3557186126709\n",
      "[step: 751] loss: 27.355024337768555\n",
      "[step: 752] loss: 27.35552215576172\n",
      "[step: 753] loss: 27.356891632080078\n",
      "[step: 754] loss: 27.3586368560791\n",
      "[step: 755] loss: 27.36099624633789\n",
      "[step: 756] loss: 27.363235473632812\n",
      "[step: 757] loss: 27.367151260375977\n",
      "[step: 758] loss: 27.370866775512695\n",
      "[step: 759] loss: 27.378934860229492\n",
      "[step: 760] loss: 27.38640785217285\n",
      "[step: 761] loss: 27.405364990234375\n",
      "[step: 762] loss: 27.420917510986328\n",
      "[step: 763] loss: 27.465574264526367\n",
      "[step: 764] loss: 27.484312057495117\n",
      "[step: 765] loss: 27.555158615112305\n",
      "[step: 766] loss: 27.522178649902344\n",
      "[step: 767] loss: 27.530336380004883\n",
      "[step: 768] loss: 27.43827247619629\n",
      "[step: 769] loss: 27.382335662841797\n",
      "[step: 770] loss: 27.354339599609375\n",
      "[step: 771] loss: 27.367568969726562\n",
      "[step: 772] loss: 27.403837203979492\n",
      "[step: 773] loss: 27.416536331176758\n",
      "[step: 774] loss: 27.41326141357422\n",
      "[step: 775] loss: 27.37590217590332\n",
      "[step: 776] loss: 27.355501174926758\n",
      "[step: 777] loss: 27.362125396728516\n",
      "[step: 778] loss: 27.38033103942871\n",
      "[step: 779] loss: 27.39150619506836\n",
      "[step: 780] loss: 27.376346588134766\n",
      "[step: 781] loss: 27.35967254638672\n",
      "[step: 782] loss: 27.35369110107422\n",
      "[step: 783] loss: 27.36145782470703\n",
      "[step: 784] loss: 27.372041702270508\n",
      "[step: 785] loss: 27.370635986328125\n",
      "[step: 786] loss: 27.36273956298828\n",
      "[step: 787] loss: 27.35443878173828\n",
      "[step: 788] loss: 27.353591918945312\n",
      "[step: 789] loss: 27.358718872070312\n",
      "[step: 790] loss: 27.363239288330078\n",
      "[step: 791] loss: 27.364168167114258\n",
      "[step: 792] loss: 27.35940170288086\n",
      "[step: 793] loss: 27.354393005371094\n",
      "[step: 794] loss: 27.35187339782715\n",
      "[step: 795] loss: 27.352937698364258\n",
      "[step: 796] loss: 27.355941772460938\n",
      "[step: 797] loss: 27.358083724975586\n",
      "[step: 798] loss: 27.358694076538086\n",
      "[step: 799] loss: 27.356769561767578\n",
      "[step: 800] loss: 27.354339599609375\n",
      "[step: 801] loss: 27.35196876525879\n",
      "[step: 802] loss: 27.350648880004883\n",
      "[step: 803] loss: 27.350522994995117\n",
      "[step: 804] loss: 27.35131072998047\n",
      "[step: 805] loss: 27.35256576538086\n",
      "[step: 806] loss: 27.353748321533203\n",
      "[step: 807] loss: 27.35499382019043\n",
      "[step: 808] loss: 27.355745315551758\n",
      "[step: 809] loss: 27.356801986694336\n",
      "[step: 810] loss: 27.357267379760742\n",
      "[step: 811] loss: 27.3585262298584\n",
      "[step: 812] loss: 27.359500885009766\n",
      "[step: 813] loss: 27.362247467041016\n",
      "[step: 814] loss: 27.365087509155273\n",
      "[step: 815] loss: 27.371885299682617\n",
      "[step: 816] loss: 27.378963470458984\n",
      "[step: 817] loss: 27.395429611206055\n",
      "[step: 818] loss: 27.409534454345703\n",
      "[step: 819] loss: 27.44462013244629\n",
      "[step: 820] loss: 27.46004867553711\n",
      "[step: 821] loss: 27.508934020996094\n",
      "[step: 822] loss: 27.489648818969727\n",
      "[step: 823] loss: 27.49501609802246\n",
      "[step: 824] loss: 27.428695678710938\n",
      "[step: 825] loss: 27.38331413269043\n",
      "[step: 826] loss: 27.352336883544922\n",
      "[step: 827] loss: 27.354320526123047\n",
      "[step: 828] loss: 27.378549575805664\n",
      "[step: 829] loss: 27.394968032836914\n",
      "[step: 830] loss: 27.397850036621094\n",
      "[step: 831] loss: 27.372079849243164\n",
      "[step: 832] loss: 27.352724075317383\n",
      "[step: 833] loss: 27.351760864257812\n",
      "[step: 834] loss: 27.364845275878906\n",
      "[step: 835] loss: 27.376548767089844\n",
      "[step: 836] loss: 27.370046615600586\n",
      "[step: 837] loss: 27.357742309570312\n",
      "[step: 838] loss: 27.34931755065918\n",
      "[step: 839] loss: 27.352035522460938\n",
      "[step: 840] loss: 27.360218048095703\n",
      "[step: 841] loss: 27.3627986907959\n",
      "[step: 842] loss: 27.359149932861328\n",
      "[step: 843] loss: 27.351850509643555\n",
      "[step: 844] loss: 27.348506927490234\n",
      "[step: 845] loss: 27.350479125976562\n",
      "[step: 846] loss: 27.35445785522461\n",
      "[step: 847] loss: 27.356769561767578\n",
      "[step: 848] loss: 27.354679107666016\n",
      "[step: 849] loss: 27.351015090942383\n",
      "[step: 850] loss: 27.348081588745117\n",
      "[step: 851] loss: 27.34770965576172\n",
      "[step: 852] loss: 27.34937858581543\n",
      "[step: 853] loss: 27.351289749145508\n",
      "[step: 854] loss: 27.352277755737305\n",
      "[step: 855] loss: 27.351459503173828\n",
      "[step: 856] loss: 27.3498477935791\n",
      "[step: 857] loss: 27.34798812866211\n",
      "[step: 858] loss: 27.346805572509766\n",
      "[step: 859] loss: 27.346527099609375\n",
      "[step: 860] loss: 27.34699058532715\n",
      "[step: 861] loss: 27.347829818725586\n",
      "[step: 862] loss: 27.348623275756836\n",
      "[step: 863] loss: 27.349327087402344\n",
      "[step: 864] loss: 27.349599838256836\n",
      "[step: 865] loss: 27.349777221679688\n",
      "[step: 866] loss: 27.34959602355957\n",
      "[step: 867] loss: 27.34953498840332\n",
      "[step: 868] loss: 27.349342346191406\n",
      "[step: 869] loss: 27.34945297241211\n",
      "[step: 870] loss: 27.349573135375977\n",
      "[step: 871] loss: 27.350221633911133\n",
      "[step: 872] loss: 27.351043701171875\n",
      "[step: 873] loss: 27.352853775024414\n",
      "[step: 874] loss: 27.355161666870117\n",
      "[step: 875] loss: 27.359867095947266\n",
      "[step: 876] loss: 27.36580467224121\n",
      "[step: 877] loss: 27.378244400024414\n",
      "[step: 878] loss: 27.39217758178711\n",
      "[step: 879] loss: 27.42275047302246\n",
      "[step: 880] loss: 27.445587158203125\n",
      "[step: 881] loss: 27.499237060546875\n",
      "[step: 882] loss: 27.498592376708984\n",
      "[step: 883] loss: 27.522293090820312\n",
      "[step: 884] loss: 27.455049514770508\n",
      "[step: 885] loss: 27.40415382385254\n",
      "[step: 886] loss: 27.35567283630371\n",
      "[step: 887] loss: 27.34734344482422\n",
      "[step: 888] loss: 27.371606826782227\n",
      "[step: 889] loss: 27.39447593688965\n",
      "[step: 890] loss: 27.4000301361084\n",
      "[step: 891] loss: 27.370336532592773\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={\n",
    "                                X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    \n",
    "    test_predict = scaler2.inverse_transform(test_predict)\n",
    "    dataY2 = scaler2.inverse_transform(dataY2) # 데이터 있을때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "plt.title(data + ' predict')\n",
    "plt.plot(dataY2, label='raw data') # 데이터 있을때\n",
    "plt.plot(test_predict, label='predict')\n",
    "plt.xlabel(\"Time Period\")\n",
    "# plt.ylabel(\"photometric\")\n",
    "plt.ylabel(\"cct\")\n",
    "# plt.ylabel(\"CAS_SWR\")\n",
    "# plt.ylabel(\"446to477\")\n",
    "# plt.ylabel(\"UV-B\")\n",
    "plt.xticks(np.arange(0, 892, step = 56), ['5', '6', '7', '8', '9', '10', '11', '12', '1', '2', '3', '4', '5', '6', '7', '8']) # v2 모델\n",
    "# plt.yticks(np.arange(0, 140000, step=10000)) # ptmt\n",
    "plt.yticks(np.arange(0, 60000, step=10000)) # cct\n",
    "# plt.yticks(np.arange(0, 2.0, step=0.2)) # uvb\n",
    "# plt.ylim(0, 100) #swr, 446to477\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
