{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "font_name = font_manager.FontProperties(fname=\"/usr/share/fonts/truetype/nanum/NanumGothic_Coding_Bold.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "%matplotlib tk\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 57.  -2.  83.  23.]\n",
      " [ 58.  -2.  83.  23.]\n",
      " [ 58.  -2.  83.  23.]\n",
      " ...\n",
      " [268. -23. 322. -14.]\n",
      " [268. -23. 322. -14.]\n",
      " [268. -23. 322. -14.]]\n",
      "[[6.56707e+01]\n",
      " [7.64514e+01]\n",
      " [8.85869e+01]\n",
      " ...\n",
      " [2.38816e-02]\n",
      " [2.47933e-02]\n",
      " [2.35017e-02]]\n",
      "[[ 68. -12.  20.   8.]\n",
      " [ 69. -12.  20.   8.]\n",
      " [ 69. -12.  20.   8.]\n",
      " ...\n",
      " [290. -11.  20.   8.]\n",
      " [290. -11.  20.   8.]\n",
      " [290. -11.  20.   8.]]\n"
     ]
    }
   ],
   "source": [
    "# train Parameters\n",
    "seq_length = 9\n",
    "data_dim = 4\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 901\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler2 = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Open, High, Low, Volume, Close\n",
    "xy = np.loadtxt('./v2data/ptmt/train1_v2_data.csv', delimiter=',')\n",
    "x = scaler.fit_transform(xy[:, 0:-1])\n",
    "y = scaler2.fit_transform(xy[:, [-1]])  # Close as label\n",
    "\n",
    "print(xy[:, 0:-1])\n",
    "print(xy[:, [-1]])\n",
    "\n",
    "# build a dataset\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, len(x) - seq_length):\n",
    "    _x = x[i:i + seq_length]\n",
    "    _y = y[i + seq_length]  # Next close price\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "\n",
    "xy2 = np.loadtxt('./sun_data/2018-04-12.csv',delimiter=',')\n",
    "print(xy2)\n",
    "\n",
    "x2 = scaler.fit_transform(xy2) # 데이터 없을때\n",
    "# x2 = scaler.fit_transform(xy2[:, 0:-1]) # 데이터 있을때\n",
    "# y2 = scaler2.fit_transform(xy2[:, [-1]]) #데이터 있을때\n",
    "\n",
    "dataX2 = []\n",
    "# dataY2 = [] #데이터 있을때\n",
    "\n",
    "for i in range(0, len(x2) - seq_length):\n",
    "    _x2 = x2[i:i+seq_length]\n",
    "#     _y2 = y2[i+seq_length] #데이터 있을때\n",
    "    dataX2.append(_x2)\n",
    "#     dataY2.append(_y2) #데이터 있을때\n",
    "# train/test split\n",
    "\n",
    "train_size = len(dataX)\n",
    "test_size = len(dataX2)\n",
    "\n",
    "trainX, testX = np.array(dataX[0:train_size]),np.array(dataX2[0:test_size])\n",
    "trainY = np.array(dataY[0:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 2258.882080078125\n",
      "[step: 1] loss: 1660.694580078125\n",
      "[step: 2] loss: 1197.8079833984375\n",
      "[step: 3] loss: 852.0170288085938\n",
      "[step: 4] loss: 629.59912109375\n",
      "[step: 5] loss: 541.6940307617188\n",
      "[step: 6] loss: 565.2955322265625\n",
      "[step: 7] loss: 620.8690795898438\n",
      "[step: 8] loss: 635.406005859375\n",
      "[step: 9] loss: 596.9840698242188\n",
      "[step: 10] loss: 529.6331176757812\n",
      "[step: 11] loss: 460.34783935546875\n",
      "[step: 12] loss: 406.0227966308594\n",
      "[step: 13] loss: 372.6711120605469\n",
      "[step: 14] loss: 358.565673828125\n",
      "[step: 15] loss: 357.4196472167969\n",
      "[step: 16] loss: 360.7814025878906\n",
      "[step: 17] loss: 360.1778259277344\n",
      "[step: 18] loss: 349.28369140625\n",
      "[step: 19] loss: 325.50494384765625\n",
      "[step: 20] loss: 290.63018798828125\n",
      "[step: 21] loss: 250.84361267089844\n",
      "[step: 22] loss: 216.0191192626953\n",
      "[step: 23] loss: 196.56802368164062\n",
      "[step: 24] loss: 195.6203155517578\n",
      "[step: 25] loss: 201.45733642578125\n",
      "[step: 26] loss: 194.50184631347656\n",
      "[step: 27] loss: 167.23782348632812\n",
      "[step: 28] loss: 131.5294647216797\n",
      "[step: 29] loss: 106.10425567626953\n",
      "[step: 30] loss: 98.92489624023438\n",
      "[step: 31] loss: 99.7901382446289\n",
      "[step: 32] loss: 92.44146728515625\n",
      "[step: 33] loss: 73.89022064208984\n",
      "[step: 34] loss: 57.44390106201172\n",
      "[step: 35] loss: 57.08167266845703\n",
      "[step: 36] loss: 69.11746978759766\n",
      "[step: 37] loss: 76.49504852294922\n",
      "[step: 38] loss: 72.58135986328125\n",
      "[step: 39] loss: 65.74779510498047\n",
      "[step: 40] loss: 64.4931869506836\n",
      "[step: 41] loss: 67.68607330322266\n",
      "[step: 42] loss: 68.6226806640625\n",
      "[step: 43] loss: 64.0875473022461\n",
      "[step: 44] loss: 56.860557556152344\n",
      "[step: 45] loss: 51.54350280761719\n",
      "[step: 46] loss: 50.046051025390625\n",
      "[step: 47] loss: 50.677215576171875\n",
      "[step: 48] loss: 50.635799407958984\n",
      "[step: 49] loss: 48.87725830078125\n",
      "[step: 50] loss: 46.64456558227539\n",
      "[step: 51] loss: 45.72667694091797\n",
      "[step: 52] loss: 46.52799987792969\n",
      "[step: 53] loss: 47.76848602294922\n",
      "[step: 54] loss: 47.87162780761719\n",
      "[step: 55] loss: 46.470497131347656\n",
      "[step: 56] loss: 44.5312614440918\n",
      "[step: 57] loss: 43.176963806152344\n",
      "[step: 58] loss: 42.56502914428711\n",
      "[step: 59] loss: 41.94478988647461\n",
      "[step: 60] loss: 40.67351150512695\n",
      "[step: 61] loss: 38.95951843261719\n",
      "[step: 62] loss: 37.564056396484375\n",
      "[step: 63] loss: 36.94843292236328\n",
      "[step: 64] loss: 36.856178283691406\n",
      "[step: 65] loss: 36.685096740722656\n",
      "[step: 66] loss: 36.129173278808594\n",
      "[step: 67] loss: 35.39493942260742\n",
      "[step: 68] loss: 34.85249328613281\n",
      "[step: 69] loss: 34.57881164550781\n",
      "[step: 70] loss: 34.296661376953125\n",
      "[step: 71] loss: 33.725059509277344\n",
      "[step: 72] loss: 32.89903259277344\n",
      "[step: 73] loss: 32.10108184814453\n",
      "[step: 74] loss: 31.532739639282227\n",
      "[step: 75] loss: 31.128192901611328\n",
      "[step: 76] loss: 30.691707611083984\n",
      "[step: 77] loss: 30.150123596191406\n",
      "[step: 78] loss: 29.616790771484375\n",
      "[step: 79] loss: 29.225893020629883\n",
      "[step: 80] loss: 28.963712692260742\n",
      "[step: 81] loss: 28.694517135620117\n",
      "[step: 82] loss: 28.324419021606445\n",
      "[step: 83] loss: 27.892545700073242\n",
      "[step: 84] loss: 27.495872497558594\n",
      "[step: 85] loss: 27.164587020874023\n",
      "[step: 86] loss: 26.842618942260742\n",
      "[step: 87] loss: 26.4763240814209\n",
      "[step: 88] loss: 26.086042404174805\n",
      "[step: 89] loss: 25.736051559448242\n",
      "[step: 90] loss: 25.453624725341797\n",
      "[step: 91] loss: 25.20231056213379\n",
      "[step: 92] loss: 24.933921813964844\n",
      "[step: 93] loss: 24.644243240356445\n",
      "[step: 94] loss: 24.36425018310547\n",
      "[step: 95] loss: 24.10963249206543\n",
      "[step: 96] loss: 23.859891891479492\n",
      "[step: 97] loss: 23.589008331298828\n",
      "[step: 98] loss: 23.300395965576172\n",
      "[step: 99] loss: 23.02035140991211\n",
      "[step: 100] loss: 22.76413345336914\n",
      "[step: 101] loss: 22.521238327026367\n",
      "[step: 102] loss: 22.274988174438477\n",
      "[step: 103] loss: 22.024965286254883\n",
      "[step: 104] loss: 21.78264808654785\n",
      "[step: 105] loss: 21.55120277404785\n",
      "[step: 106] loss: 21.319387435913086\n",
      "[step: 107] loss: 21.0765438079834\n",
      "[step: 108] loss: 20.825634002685547\n",
      "[step: 109] loss: 20.577190399169922\n",
      "[step: 110] loss: 20.335023880004883\n",
      "[step: 111] loss: 20.093822479248047\n",
      "[step: 112] loss: 19.849477767944336\n",
      "[step: 113] loss: 19.60548210144043\n",
      "[step: 114] loss: 19.366846084594727\n",
      "[step: 115] loss: 19.132160186767578\n",
      "[step: 116] loss: 18.895614624023438\n",
      "[step: 117] loss: 18.65497398376465\n",
      "[step: 118] loss: 18.413686752319336\n",
      "[step: 119] loss: 18.175230026245117\n",
      "[step: 120] loss: 17.938966751098633\n",
      "[step: 121] loss: 17.70307159423828\n",
      "[step: 122] loss: 17.468595504760742\n",
      "[step: 123] loss: 17.238250732421875\n",
      "[step: 124] loss: 17.01227569580078\n",
      "[step: 125] loss: 16.78822898864746\n",
      "[step: 126] loss: 16.56473159790039\n",
      "[step: 127] loss: 16.343198776245117\n",
      "[step: 128] loss: 16.125417709350586\n",
      "[step: 129] loss: 15.91134262084961\n",
      "[step: 130] loss: 15.700331687927246\n",
      "[step: 131] loss: 15.49315357208252\n",
      "[step: 132] loss: 15.291210174560547\n",
      "[step: 133] loss: 15.09450912475586\n",
      "[step: 134] loss: 14.901910781860352\n",
      "[step: 135] loss: 14.712987899780273\n",
      "[step: 136] loss: 14.528481483459473\n",
      "[step: 137] loss: 14.348922729492188\n",
      "[step: 138] loss: 14.173991203308105\n",
      "[step: 139] loss: 14.003432273864746\n",
      "[step: 140] loss: 13.837632179260254\n",
      "[step: 141] loss: 13.676755905151367\n",
      "[step: 142] loss: 13.520201683044434\n",
      "[step: 143] loss: 13.367318153381348\n",
      "[step: 144] loss: 13.218138694763184\n",
      "[step: 145] loss: 13.072908401489258\n",
      "[step: 146] loss: 12.931464195251465\n",
      "[step: 147] loss: 12.793500900268555\n",
      "[step: 148] loss: 12.659036636352539\n",
      "[step: 149] loss: 12.52816104888916\n",
      "[step: 150] loss: 12.400606155395508\n",
      "[step: 151] loss: 12.276025772094727\n",
      "[step: 152] loss: 12.154387474060059\n",
      "[step: 153] loss: 12.035847663879395\n",
      "[step: 154] loss: 11.92037582397461\n",
      "[step: 155] loss: 11.807861328125\n",
      "[step: 156] loss: 11.698335647583008\n",
      "[step: 157] loss: 11.591870307922363\n",
      "[step: 158] loss: 11.488373756408691\n",
      "[step: 159] loss: 11.387737274169922\n",
      "[step: 160] loss: 11.289999008178711\n",
      "[step: 161] loss: 11.195253372192383\n",
      "[step: 162] loss: 11.103476524353027\n",
      "[step: 163] loss: 11.014593124389648\n",
      "[step: 164] loss: 10.92860221862793\n",
      "[step: 165] loss: 10.845484733581543\n",
      "[step: 166] loss: 10.765141487121582\n",
      "[step: 167] loss: 10.687475204467773\n",
      "[step: 168] loss: 10.612467765808105\n",
      "[step: 169] loss: 10.540072441101074\n",
      "[step: 170] loss: 10.470198631286621\n",
      "[step: 171] loss: 10.402751922607422\n",
      "[step: 172] loss: 10.337666511535645\n",
      "[step: 173] loss: 10.274864196777344\n",
      "[step: 174] loss: 10.214247703552246\n",
      "[step: 175] loss: 10.155740737915039\n",
      "[step: 176] loss: 10.099295616149902\n",
      "[step: 177] loss: 10.044853210449219\n",
      "[step: 178] loss: 9.992329597473145\n",
      "[step: 179] loss: 9.941665649414062\n",
      "[step: 180] loss: 9.892806053161621\n",
      "[step: 181] loss: 9.84569263458252\n",
      "[step: 182] loss: 9.800257682800293\n",
      "[step: 183] loss: 9.756443977355957\n",
      "[step: 184] loss: 9.714207649230957\n",
      "[step: 185] loss: 9.673481941223145\n",
      "[step: 186] loss: 9.634197235107422\n",
      "[step: 187] loss: 9.596294403076172\n",
      "[step: 188] loss: 9.559720039367676\n",
      "[step: 189] loss: 9.52440071105957\n",
      "[step: 190] loss: 9.490272521972656\n",
      "[step: 191] loss: 9.457280158996582\n",
      "[step: 192] loss: 9.425363540649414\n",
      "[step: 193] loss: 9.394452095031738\n",
      "[step: 194] loss: 9.364489555358887\n",
      "[step: 195] loss: 9.335416793823242\n",
      "[step: 196] loss: 9.307184219360352\n",
      "[step: 197] loss: 9.279730796813965\n",
      "[step: 198] loss: 9.253005981445312\n",
      "[step: 199] loss: 9.226967811584473\n",
      "[step: 200] loss: 9.201567649841309\n",
      "[step: 201] loss: 9.176756858825684\n",
      "[step: 202] loss: 9.152502059936523\n",
      "[step: 203] loss: 9.128764152526855\n",
      "[step: 204] loss: 9.105502128601074\n",
      "[step: 205] loss: 9.08268928527832\n",
      "[step: 206] loss: 9.060297012329102\n",
      "[step: 207] loss: 9.038289070129395\n",
      "[step: 208] loss: 9.016637802124023\n",
      "[step: 209] loss: 8.995326042175293\n",
      "[step: 210] loss: 8.974325180053711\n",
      "[step: 211] loss: 8.953612327575684\n",
      "[step: 212] loss: 8.933172225952148\n",
      "[step: 213] loss: 8.912981033325195\n",
      "[step: 214] loss: 8.893026351928711\n",
      "[step: 215] loss: 8.873291969299316\n",
      "[step: 216] loss: 8.853759765625\n",
      "[step: 217] loss: 8.83442211151123\n",
      "[step: 218] loss: 8.815266609191895\n",
      "[step: 219] loss: 8.796280860900879\n",
      "[step: 220] loss: 8.777458190917969\n",
      "[step: 221] loss: 8.7587890625\n",
      "[step: 222] loss: 8.740266799926758\n",
      "[step: 223] loss: 8.721880912780762\n",
      "[step: 224] loss: 8.703632354736328\n",
      "[step: 225] loss: 8.685510635375977\n",
      "[step: 226] loss: 8.667512893676758\n",
      "[step: 227] loss: 8.64963436126709\n",
      "[step: 228] loss: 8.631871223449707\n",
      "[step: 229] loss: 8.614218711853027\n",
      "[step: 230] loss: 8.59667682647705\n",
      "[step: 231] loss: 8.579238891601562\n",
      "[step: 232] loss: 8.561901092529297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 233] loss: 8.544663429260254\n",
      "[step: 234] loss: 8.527522087097168\n",
      "[step: 235] loss: 8.510476112365723\n",
      "[step: 236] loss: 8.493517875671387\n",
      "[step: 237] loss: 8.476648330688477\n",
      "[step: 238] loss: 8.459870338439941\n",
      "[step: 239] loss: 8.443172454833984\n",
      "[step: 240] loss: 8.426563262939453\n",
      "[step: 241] loss: 8.4100341796875\n",
      "[step: 242] loss: 8.393579483032227\n",
      "[step: 243] loss: 8.37721061706543\n",
      "[step: 244] loss: 8.36091423034668\n",
      "[step: 245] loss: 8.34469223022461\n",
      "[step: 246] loss: 8.32854175567627\n",
      "[step: 247] loss: 8.31246566772461\n",
      "[step: 248] loss: 8.296462059020996\n",
      "[step: 249] loss: 8.280526161193848\n",
      "[step: 250] loss: 8.264657020568848\n",
      "[step: 251] loss: 8.248848915100098\n",
      "[step: 252] loss: 8.233113288879395\n",
      "[step: 253] loss: 8.217436790466309\n",
      "[step: 254] loss: 8.201824188232422\n",
      "[step: 255] loss: 8.186272621154785\n",
      "[step: 256] loss: 8.170782089233398\n",
      "[step: 257] loss: 8.15534782409668\n",
      "[step: 258] loss: 8.139974594116211\n",
      "[step: 259] loss: 8.124652862548828\n",
      "[step: 260] loss: 8.109389305114746\n",
      "[step: 261] loss: 8.094181060791016\n",
      "[step: 262] loss: 8.079026222229004\n",
      "[step: 263] loss: 8.063920974731445\n",
      "[step: 264] loss: 8.048871040344238\n",
      "[step: 265] loss: 8.033869743347168\n",
      "[step: 266] loss: 8.018918991088867\n",
      "[step: 267] loss: 8.00401782989502\n",
      "[step: 268] loss: 7.989164352416992\n",
      "[step: 269] loss: 7.974359035491943\n",
      "[step: 270] loss: 7.959598541259766\n",
      "[step: 271] loss: 7.944887161254883\n",
      "[step: 272] loss: 7.930217742919922\n",
      "[step: 273] loss: 7.915596961975098\n",
      "[step: 274] loss: 7.901015281677246\n",
      "[step: 275] loss: 7.886480331420898\n",
      "[step: 276] loss: 7.871984481811523\n",
      "[step: 277] loss: 7.857534408569336\n",
      "[step: 278] loss: 7.843123435974121\n",
      "[step: 279] loss: 7.828752040863037\n",
      "[step: 280] loss: 7.814422130584717\n",
      "[step: 281] loss: 7.800132751464844\n",
      "[step: 282] loss: 7.785881519317627\n",
      "[step: 283] loss: 7.771667957305908\n",
      "[step: 284] loss: 7.757492542266846\n",
      "[step: 285] loss: 7.743354320526123\n",
      "[step: 286] loss: 7.7292561531066895\n",
      "[step: 287] loss: 7.715190410614014\n",
      "[step: 288] loss: 7.7011613845825195\n",
      "[step: 289] loss: 7.687167644500732\n",
      "[step: 290] loss: 7.673207759857178\n",
      "[step: 291] loss: 7.6592817306518555\n",
      "[step: 292] loss: 7.645392417907715\n",
      "[step: 293] loss: 7.631536483764648\n",
      "[step: 294] loss: 7.617712497711182\n",
      "[step: 295] loss: 7.603920936584473\n",
      "[step: 296] loss: 7.590159893035889\n",
      "[step: 297] loss: 7.5764336585998535\n",
      "[step: 298] loss: 7.562735080718994\n",
      "[step: 299] loss: 7.549069881439209\n",
      "[step: 300] loss: 7.535433769226074\n",
      "[step: 301] loss: 7.521827220916748\n",
      "[step: 302] loss: 7.508252143859863\n",
      "[step: 303] loss: 7.494703769683838\n",
      "[step: 304] loss: 7.481187343597412\n",
      "[step: 305] loss: 7.467698097229004\n",
      "[step: 306] loss: 7.454235076904297\n",
      "[step: 307] loss: 7.440800666809082\n",
      "[step: 308] loss: 7.427395820617676\n",
      "[step: 309] loss: 7.414015769958496\n",
      "[step: 310] loss: 7.400661468505859\n",
      "[step: 311] loss: 7.387331962585449\n",
      "[step: 312] loss: 7.374032497406006\n",
      "[step: 313] loss: 7.360754013061523\n",
      "[step: 314] loss: 7.347504138946533\n",
      "[step: 315] loss: 7.3342766761779785\n",
      "[step: 316] loss: 7.321072578430176\n",
      "[step: 317] loss: 7.307896137237549\n",
      "[step: 318] loss: 7.294742584228516\n",
      "[step: 319] loss: 7.281608581542969\n",
      "[step: 320] loss: 7.268502235412598\n",
      "[step: 321] loss: 7.255417346954346\n",
      "[step: 322] loss: 7.2423529624938965\n",
      "[step: 323] loss: 7.229312419891357\n",
      "[step: 324] loss: 7.2162909507751465\n",
      "[step: 325] loss: 7.203293323516846\n",
      "[step: 326] loss: 7.190315246582031\n",
      "[step: 327] loss: 7.1773600578308105\n",
      "[step: 328] loss: 7.164422512054443\n",
      "[step: 329] loss: 7.151505947113037\n",
      "[step: 330] loss: 7.138609886169434\n",
      "[step: 331] loss: 7.125733852386475\n",
      "[step: 332] loss: 7.112875938415527\n",
      "[step: 333] loss: 7.100035667419434\n",
      "[step: 334] loss: 7.087216377258301\n",
      "[step: 335] loss: 7.0744147300720215\n",
      "[step: 336] loss: 7.061631202697754\n",
      "[step: 337] loss: 7.048865795135498\n",
      "[step: 338] loss: 7.036114692687988\n",
      "[step: 339] loss: 7.023383617401123\n",
      "[step: 340] loss: 7.010668754577637\n",
      "[step: 341] loss: 6.997970104217529\n",
      "[step: 342] loss: 6.985286235809326\n",
      "[step: 343] loss: 6.972620010375977\n",
      "[step: 344] loss: 6.959968090057373\n",
      "[step: 345] loss: 6.947335243225098\n",
      "[step: 346] loss: 6.9347147941589355\n",
      "[step: 347] loss: 6.9221086502075195\n",
      "[step: 348] loss: 6.909519195556641\n",
      "[step: 349] loss: 6.896940231323242\n",
      "[step: 350] loss: 6.884379863739014\n",
      "[step: 351] loss: 6.871828556060791\n",
      "[step: 352] loss: 6.8592939376831055\n",
      "[step: 353] loss: 6.846772193908691\n",
      "[step: 354] loss: 6.834263324737549\n",
      "[step: 355] loss: 6.8217668533325195\n",
      "[step: 356] loss: 6.809284687042236\n",
      "[step: 357] loss: 6.796812534332275\n",
      "[step: 358] loss: 6.784353256225586\n",
      "[step: 359] loss: 6.771903991699219\n",
      "[step: 360] loss: 6.759467601776123\n",
      "[step: 361] loss: 6.74704122543335\n",
      "[step: 362] loss: 6.734627723693848\n",
      "[step: 363] loss: 6.722221851348877\n",
      "[step: 364] loss: 6.709826469421387\n",
      "[step: 365] loss: 6.697444438934326\n",
      "[step: 366] loss: 6.6850690841674805\n",
      "[step: 367] loss: 6.6727070808410645\n",
      "[step: 368] loss: 6.66035270690918\n",
      "[step: 369] loss: 6.648006916046143\n",
      "[step: 370] loss: 6.635669231414795\n",
      "[step: 371] loss: 6.623341083526611\n",
      "[step: 372] loss: 6.611019611358643\n",
      "[step: 373] loss: 6.598708629608154\n",
      "[step: 374] loss: 6.586404800415039\n",
      "[step: 375] loss: 6.57411003112793\n",
      "[step: 376] loss: 6.561821460723877\n",
      "[step: 377] loss: 6.5495381355285645\n",
      "[step: 378] loss: 6.5372633934021\n",
      "[step: 379] loss: 6.524994850158691\n",
      "[step: 380] loss: 6.512734413146973\n",
      "[step: 381] loss: 6.500478744506836\n",
      "[step: 382] loss: 6.488229751586914\n",
      "[step: 383] loss: 6.475986480712891\n",
      "[step: 384] loss: 6.463749408721924\n",
      "[step: 385] loss: 6.451515197753906\n",
      "[step: 386] loss: 6.439288139343262\n",
      "[step: 387] loss: 6.427066326141357\n",
      "[step: 388] loss: 6.414848327636719\n",
      "[step: 389] loss: 6.402635097503662\n",
      "[step: 390] loss: 6.390424728393555\n",
      "[step: 391] loss: 6.3782172203063965\n",
      "[step: 392] loss: 6.366015911102295\n",
      "[step: 393] loss: 6.353817939758301\n",
      "[step: 394] loss: 6.341621398925781\n",
      "[step: 395] loss: 6.32943058013916\n",
      "[step: 396] loss: 6.317242622375488\n",
      "[step: 397] loss: 6.305054664611816\n",
      "[step: 398] loss: 6.292868614196777\n",
      "[step: 399] loss: 6.280686378479004\n",
      "[step: 400] loss: 6.268506050109863\n",
      "[step: 401] loss: 6.256326675415039\n",
      "[step: 402] loss: 6.244149684906006\n",
      "[step: 403] loss: 6.231974124908447\n",
      "[step: 404] loss: 6.219794750213623\n",
      "[step: 405] loss: 6.207623481750488\n",
      "[step: 406] loss: 6.195448398590088\n",
      "[step: 407] loss: 6.183273792266846\n",
      "[step: 408] loss: 6.1711015701293945\n",
      "[step: 409] loss: 6.158928871154785\n",
      "[step: 410] loss: 6.146753787994385\n",
      "[step: 411] loss: 6.134579181671143\n",
      "[step: 412] loss: 6.122405052185059\n",
      "[step: 413] loss: 6.110228061676025\n",
      "[step: 414] loss: 6.098051071166992\n",
      "[step: 415] loss: 6.085870265960693\n",
      "[step: 416] loss: 6.073690414428711\n",
      "[step: 417] loss: 6.061509132385254\n",
      "[step: 418] loss: 6.049325942993164\n",
      "[step: 419] loss: 6.037137985229492\n",
      "[step: 420] loss: 6.0249481201171875\n",
      "[step: 421] loss: 6.012758255004883\n",
      "[step: 422] loss: 6.000564098358154\n",
      "[step: 423] loss: 5.988366603851318\n",
      "[step: 424] loss: 5.976163864135742\n",
      "[step: 425] loss: 5.963962078094482\n",
      "[step: 426] loss: 5.951754093170166\n",
      "[step: 427] loss: 5.939541339874268\n",
      "[step: 428] loss: 5.927326202392578\n",
      "[step: 429] loss: 5.915104389190674\n",
      "[step: 430] loss: 5.902880668640137\n",
      "[step: 431] loss: 5.890651702880859\n",
      "[step: 432] loss: 5.878415584564209\n",
      "[step: 433] loss: 5.866176128387451\n",
      "[step: 434] loss: 5.853931903839111\n",
      "[step: 435] loss: 5.841679573059082\n",
      "[step: 436] loss: 5.8294219970703125\n",
      "[step: 437] loss: 5.8171610832214355\n",
      "[step: 438] loss: 5.804892539978027\n",
      "[step: 439] loss: 5.792618274688721\n",
      "[step: 440] loss: 5.780337810516357\n",
      "[step: 441] loss: 5.76804780960083\n",
      "[step: 442] loss: 5.755751609802246\n",
      "[step: 443] loss: 5.743448734283447\n",
      "[step: 444] loss: 5.731138229370117\n",
      "[step: 445] loss: 5.7188191413879395\n",
      "[step: 446] loss: 5.706495761871338\n",
      "[step: 447] loss: 5.694162368774414\n",
      "[step: 448] loss: 5.681818962097168\n",
      "[step: 449] loss: 5.669470310211182\n",
      "[step: 450] loss: 5.657110691070557\n",
      "[step: 451] loss: 5.644741058349609\n",
      "[step: 452] loss: 5.632369041442871\n",
      "[step: 453] loss: 5.619978904724121\n",
      "[step: 454] loss: 5.607586860656738\n",
      "[step: 455] loss: 5.595180034637451\n",
      "[step: 456] loss: 5.582765579223633\n",
      "[step: 457] loss: 5.570340633392334\n",
      "[step: 458] loss: 5.557905197143555\n",
      "[step: 459] loss: 5.545459747314453\n",
      "[step: 460] loss: 5.533003807067871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 461] loss: 5.520538330078125\n",
      "[step: 462] loss: 5.508061408996582\n",
      "[step: 463] loss: 5.495572566986084\n",
      "[step: 464] loss: 5.48307466506958\n",
      "[step: 465] loss: 5.47056245803833\n",
      "[step: 466] loss: 5.458037853240967\n",
      "[step: 467] loss: 5.445502281188965\n",
      "[step: 468] loss: 5.432955265045166\n",
      "[step: 469] loss: 5.420393943786621\n",
      "[step: 470] loss: 5.4078240394592285\n",
      "[step: 471] loss: 5.39523983001709\n",
      "[step: 472] loss: 5.382641792297363\n",
      "[step: 473] loss: 5.370030879974365\n",
      "[step: 474] loss: 5.357409477233887\n",
      "[step: 475] loss: 5.344771862030029\n",
      "[step: 476] loss: 5.332121849060059\n",
      "[step: 477] loss: 5.319457530975342\n",
      "[step: 478] loss: 5.306780815124512\n",
      "[step: 479] loss: 5.294088363647461\n",
      "[step: 480] loss: 5.281383037567139\n",
      "[step: 481] loss: 5.26866340637207\n",
      "[step: 482] loss: 5.255929470062256\n",
      "[step: 483] loss: 5.243181228637695\n",
      "[step: 484] loss: 5.230417251586914\n",
      "[step: 485] loss: 5.217638969421387\n",
      "[step: 486] loss: 5.204845428466797\n",
      "[step: 487] loss: 5.192036151885986\n",
      "[step: 488] loss: 5.179213047027588\n",
      "[step: 489] loss: 5.166374683380127\n",
      "[step: 490] loss: 5.153519153594971\n",
      "[step: 491] loss: 5.140649318695068\n",
      "[step: 492] loss: 5.127765655517578\n",
      "[step: 493] loss: 5.114863395690918\n",
      "[step: 494] loss: 5.1019439697265625\n",
      "[step: 495] loss: 5.089010715484619\n",
      "[step: 496] loss: 5.076061725616455\n",
      "[step: 497] loss: 5.063096046447754\n",
      "[step: 498] loss: 5.050113201141357\n",
      "[step: 499] loss: 5.037116050720215\n",
      "[step: 500] loss: 5.024099826812744\n",
      "[step: 501] loss: 5.011067867279053\n",
      "[step: 502] loss: 4.998019695281982\n",
      "[step: 503] loss: 4.984955787658691\n",
      "[step: 504] loss: 4.971874713897705\n",
      "[step: 505] loss: 4.958775997161865\n",
      "[step: 506] loss: 4.9456610679626465\n",
      "[step: 507] loss: 4.932533264160156\n",
      "[step: 508] loss: 4.919384002685547\n",
      "[step: 509] loss: 4.906219482421875\n",
      "[step: 510] loss: 4.893038272857666\n",
      "[step: 511] loss: 4.8798418045043945\n",
      "[step: 512] loss: 4.86662483215332\n",
      "[step: 513] loss: 4.853395938873291\n",
      "[step: 514] loss: 4.840149402618408\n",
      "[step: 515] loss: 4.826883792877197\n",
      "[step: 516] loss: 4.813604354858398\n",
      "[step: 517] loss: 4.800307273864746\n",
      "[step: 518] loss: 4.786994457244873\n",
      "[step: 519] loss: 4.773664474487305\n",
      "[step: 520] loss: 4.760320663452148\n",
      "[step: 521] loss: 4.746960163116455\n",
      "[step: 522] loss: 4.733582973480225\n",
      "[step: 523] loss: 4.720190048217773\n",
      "[step: 524] loss: 4.706782341003418\n",
      "[step: 525] loss: 4.693359851837158\n",
      "[step: 526] loss: 4.679920673370361\n",
      "[step: 527] loss: 4.666468620300293\n",
      "[step: 528] loss: 4.653000831604004\n",
      "[step: 529] loss: 4.6395182609558105\n",
      "[step: 530] loss: 4.626022815704346\n",
      "[step: 531] loss: 4.612511157989502\n",
      "[step: 532] loss: 4.598989009857178\n",
      "[step: 533] loss: 4.58544921875\n",
      "[step: 534] loss: 4.571901798248291\n",
      "[step: 535] loss: 4.558337211608887\n",
      "[step: 536] loss: 4.544762134552002\n",
      "[step: 537] loss: 4.531174182891846\n",
      "[step: 538] loss: 4.517574787139893\n",
      "[step: 539] loss: 4.503964900970459\n",
      "[step: 540] loss: 4.4903411865234375\n",
      "[step: 541] loss: 4.476708889007568\n",
      "[step: 542] loss: 4.463066577911377\n",
      "[step: 543] loss: 4.44941520690918\n",
      "[step: 544] loss: 4.435754299163818\n",
      "[step: 545] loss: 4.422082901000977\n",
      "[step: 546] loss: 4.4084062576293945\n",
      "[step: 547] loss: 4.394718647003174\n",
      "[step: 548] loss: 4.3810248374938965\n",
      "[step: 549] loss: 4.3673272132873535\n",
      "[step: 550] loss: 4.353618621826172\n",
      "[step: 551] loss: 4.339905738830566\n",
      "[step: 552] loss: 4.326189994812012\n",
      "[step: 553] loss: 4.3124680519104\n",
      "[step: 554] loss: 4.29874324798584\n",
      "[step: 555] loss: 4.285015106201172\n",
      "[step: 556] loss: 4.2712836265563965\n",
      "[step: 557] loss: 4.2575531005859375\n",
      "[step: 558] loss: 4.243819236755371\n",
      "[step: 559] loss: 4.2300848960876465\n",
      "[step: 560] loss: 4.216353416442871\n",
      "[step: 561] loss: 4.202621936798096\n",
      "[step: 562] loss: 4.188891887664795\n",
      "[step: 563] loss: 4.175163269042969\n",
      "[step: 564] loss: 4.161438941955566\n",
      "[step: 565] loss: 4.147720813751221\n",
      "[step: 566] loss: 4.134006023406982\n",
      "[step: 567] loss: 4.120299816131592\n",
      "[step: 568] loss: 4.1065993309021\n",
      "[step: 569] loss: 4.092906475067139\n",
      "[step: 570] loss: 4.079223155975342\n",
      "[step: 571] loss: 4.065549373626709\n",
      "[step: 572] loss: 4.051885604858398\n",
      "[step: 573] loss: 4.038233280181885\n",
      "[step: 574] loss: 4.024596214294434\n",
      "[step: 575] loss: 4.010971546173096\n",
      "[step: 576] loss: 3.9973599910736084\n",
      "[step: 577] loss: 3.9837653636932373\n",
      "[step: 578] loss: 3.970186948776245\n",
      "[step: 579] loss: 3.956629514694214\n",
      "[step: 580] loss: 3.9430882930755615\n",
      "[step: 581] loss: 3.929567813873291\n",
      "[step: 582] loss: 3.9160683155059814\n",
      "[step: 583] loss: 3.9025919437408447\n",
      "[step: 584] loss: 3.8891377449035645\n",
      "[step: 585] loss: 3.8757107257843018\n",
      "[step: 586] loss: 3.862304925918579\n",
      "[step: 587] loss: 3.848931074142456\n",
      "[step: 588] loss: 3.8355836868286133\n",
      "[step: 589] loss: 3.822263717651367\n",
      "[step: 590] loss: 3.8089780807495117\n",
      "[step: 591] loss: 3.7957210540771484\n",
      "[step: 592] loss: 3.7824976444244385\n",
      "[step: 593] loss: 3.7693092823028564\n",
      "[step: 594] loss: 3.7561545372009277\n",
      "[step: 595] loss: 3.7430403232574463\n",
      "[step: 596] loss: 3.7299609184265137\n",
      "[step: 597] loss: 3.7169225215911865\n",
      "[step: 598] loss: 3.703923225402832\n",
      "[step: 599] loss: 3.6909661293029785\n",
      "[step: 600] loss: 3.6780524253845215\n",
      "[step: 601] loss: 3.6651837825775146\n",
      "[step: 602] loss: 3.652357578277588\n",
      "[step: 603] loss: 3.639580488204956\n",
      "[step: 604] loss: 3.6268508434295654\n",
      "[step: 605] loss: 3.614170551300049\n",
      "[step: 606] loss: 3.6015408039093018\n",
      "[step: 607] loss: 3.588963031768799\n",
      "[step: 608] loss: 3.57643723487854\n",
      "[step: 609] loss: 3.563966989517212\n",
      "[step: 610] loss: 3.5515501499176025\n",
      "[step: 611] loss: 3.5391921997070312\n",
      "[step: 612] loss: 3.5268893241882324\n",
      "[step: 613] loss: 3.5146484375\n",
      "[step: 614] loss: 3.5024662017822266\n",
      "[step: 615] loss: 3.490344524383545\n",
      "[step: 616] loss: 3.478285789489746\n",
      "[step: 617] loss: 3.466292142868042\n",
      "[step: 618] loss: 3.4543604850769043\n",
      "[step: 619] loss: 3.4424960613250732\n",
      "[step: 620] loss: 3.430696964263916\n",
      "[step: 621] loss: 3.4189672470092773\n",
      "[step: 622] loss: 3.407304286956787\n",
      "[step: 623] loss: 3.395714521408081\n",
      "[step: 624] loss: 3.3841912746429443\n",
      "[step: 625] loss: 3.372743606567383\n",
      "[step: 626] loss: 3.3613662719726562\n",
      "[step: 627] loss: 3.3500635623931885\n",
      "[step: 628] loss: 3.3388326168060303\n",
      "[step: 629] loss: 3.3276782035827637\n",
      "[step: 630] loss: 3.316601276397705\n",
      "[step: 631] loss: 3.3055992126464844\n",
      "[step: 632] loss: 3.2946746349334717\n",
      "[step: 633] loss: 3.2838289737701416\n",
      "[step: 634] loss: 3.2730610370635986\n",
      "[step: 635] loss: 3.2623744010925293\n",
      "[step: 636] loss: 3.2517683506011963\n",
      "[step: 637] loss: 3.2412402629852295\n",
      "[step: 638] loss: 3.2307939529418945\n",
      "[step: 639] loss: 3.2204291820526123\n",
      "[step: 640] loss: 3.2101500034332275\n",
      "[step: 641] loss: 3.1999504566192627\n",
      "[step: 642] loss: 3.1898341178894043\n",
      "[step: 643] loss: 3.179802656173706\n",
      "[step: 644] loss: 3.169856071472168\n",
      "[step: 645] loss: 3.159991502761841\n",
      "[step: 646] loss: 3.1502132415771484\n",
      "[step: 647] loss: 3.140516996383667\n",
      "[step: 648] loss: 3.1309094429016113\n",
      "[step: 649] loss: 3.1213855743408203\n",
      "[step: 650] loss: 3.1119461059570312\n",
      "[step: 651] loss: 3.102595567703247\n",
      "[step: 652] loss: 3.093327760696411\n",
      "[step: 653] loss: 3.084148645401001\n",
      "[step: 654] loss: 3.075054168701172\n",
      "[step: 655] loss: 3.066044569015503\n",
      "[step: 656] loss: 3.0571227073669434\n",
      "[step: 657] loss: 3.048285961151123\n",
      "[step: 658] loss: 3.0395359992980957\n",
      "[step: 659] loss: 3.0308728218078613\n",
      "[step: 660] loss: 3.022294044494629\n",
      "[step: 661] loss: 3.0138018131256104\n",
      "[step: 662] loss: 3.005394697189331\n",
      "[step: 663] loss: 2.997075080871582\n",
      "[step: 664] loss: 2.9888391494750977\n",
      "[step: 665] loss: 2.9806907176971436\n",
      "[step: 666] loss: 2.9726247787475586\n",
      "[step: 667] loss: 2.9646451473236084\n",
      "[step: 668] loss: 2.956749200820923\n",
      "[step: 669] loss: 2.9489383697509766\n",
      "[step: 670] loss: 2.9412124156951904\n",
      "[step: 671] loss: 2.93356990814209\n",
      "[step: 672] loss: 2.926009178161621\n",
      "[step: 673] loss: 2.918532609939575\n",
      "[step: 674] loss: 2.911139488220215\n",
      "[step: 675] loss: 2.9038281440734863\n",
      "[step: 676] loss: 2.896599531173706\n",
      "[step: 677] loss: 2.889450788497925\n",
      "[step: 678] loss: 2.8823845386505127\n",
      "[step: 679] loss: 2.875399351119995\n",
      "[step: 680] loss: 2.8684942722320557\n",
      "[step: 681] loss: 2.861668109893799\n",
      "[step: 682] loss: 2.8549227714538574\n",
      "[step: 683] loss: 2.848254919052124\n",
      "[step: 684] loss: 2.841665029525757\n",
      "[step: 685] loss: 2.8351547718048096\n",
      "[step: 686] loss: 2.8287200927734375\n",
      "[step: 687] loss: 2.8223633766174316\n",
      "[step: 688] loss: 2.8160831928253174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 689] loss: 2.8098766803741455\n",
      "[step: 690] loss: 2.803746223449707\n",
      "[step: 691] loss: 2.797689437866211\n",
      "[step: 692] loss: 2.791706085205078\n",
      "[step: 693] loss: 2.785796880722046\n",
      "[step: 694] loss: 2.779960870742798\n",
      "[step: 695] loss: 2.7741942405700684\n",
      "[step: 696] loss: 2.768501043319702\n",
      "[step: 697] loss: 2.7628772258758545\n",
      "[step: 698] loss: 2.757323741912842\n",
      "[step: 699] loss: 2.751838445663452\n",
      "[step: 700] loss: 2.746422052383423\n",
      "[step: 701] loss: 2.7410736083984375\n",
      "[step: 702] loss: 2.735793352127075\n",
      "[step: 703] loss: 2.730576515197754\n",
      "[step: 704] loss: 2.7254269123077393\n",
      "[step: 705] loss: 2.7203421592712402\n",
      "[step: 706] loss: 2.7153210639953613\n",
      "[step: 707] loss: 2.7103629112243652\n",
      "[step: 708] loss: 2.7054684162139893\n",
      "[step: 709] loss: 2.7006373405456543\n",
      "[step: 710] loss: 2.6958649158477783\n",
      "[step: 711] loss: 2.691153049468994\n",
      "[step: 712] loss: 2.686501979827881\n",
      "[step: 713] loss: 2.681910753250122\n",
      "[step: 714] loss: 2.6773767471313477\n",
      "[step: 715] loss: 2.672900676727295\n",
      "[step: 716] loss: 2.668480396270752\n",
      "[step: 717] loss: 2.664116382598877\n",
      "[step: 718] loss: 2.65980863571167\n",
      "[step: 719] loss: 2.65555477142334\n",
      "[step: 720] loss: 2.6513540744781494\n",
      "[step: 721] loss: 2.647207260131836\n",
      "[step: 722] loss: 2.6431126594543457\n",
      "[step: 723] loss: 2.639070987701416\n",
      "[step: 724] loss: 2.635077953338623\n",
      "[step: 725] loss: 2.631138324737549\n",
      "[step: 726] loss: 2.6272459030151367\n",
      "[step: 727] loss: 2.6234023571014404\n",
      "[step: 728] loss: 2.619607448577881\n",
      "[step: 729] loss: 2.615858554840088\n",
      "[step: 730] loss: 2.6121585369110107\n",
      "[step: 731] loss: 2.6085031032562256\n",
      "[step: 732] loss: 2.6048941612243652\n",
      "[step: 733] loss: 2.601330041885376\n",
      "[step: 734] loss: 2.597808837890625\n",
      "[step: 735] loss: 2.5943312644958496\n",
      "[step: 736] loss: 2.5908966064453125\n",
      "[step: 737] loss: 2.5875046253204346\n",
      "[step: 738] loss: 2.584153413772583\n",
      "[step: 739] loss: 2.5808420181274414\n",
      "[step: 740] loss: 2.57757306098938\n",
      "[step: 741] loss: 2.574341297149658\n",
      "[step: 742] loss: 2.5711498260498047\n",
      "[step: 743] loss: 2.5679984092712402\n",
      "[step: 744] loss: 2.5648818016052246\n",
      "[step: 745] loss: 2.5618040561676025\n",
      "[step: 746] loss: 2.558762311935425\n",
      "[step: 747] loss: 2.5557568073272705\n",
      "[step: 748] loss: 2.5527877807617188\n",
      "[step: 749] loss: 2.549851179122925\n",
      "[step: 750] loss: 2.5469512939453125\n",
      "[step: 751] loss: 2.5440831184387207\n",
      "[step: 752] loss: 2.5412492752075195\n",
      "[step: 753] loss: 2.5384480953216553\n",
      "[step: 754] loss: 2.535679340362549\n",
      "[step: 755] loss: 2.5329418182373047\n",
      "[step: 756] loss: 2.530236005783081\n",
      "[step: 757] loss: 2.5275614261627197\n",
      "[step: 758] loss: 2.5249156951904297\n",
      "[step: 759] loss: 2.5223002433776855\n",
      "[step: 760] loss: 2.51971435546875\n",
      "[step: 761] loss: 2.5171566009521484\n",
      "[step: 762] loss: 2.514627456665039\n",
      "[step: 763] loss: 2.5121262073516846\n",
      "[step: 764] loss: 2.5096523761749268\n",
      "[step: 765] loss: 2.507205009460449\n",
      "[step: 766] loss: 2.504784107208252\n",
      "[step: 767] loss: 2.5023903846740723\n",
      "[step: 768] loss: 2.5000224113464355\n",
      "[step: 769] loss: 2.497678279876709\n",
      "[step: 770] loss: 2.4953603744506836\n",
      "[step: 771] loss: 2.4930665493011475\n",
      "[step: 772] loss: 2.4907968044281006\n",
      "[step: 773] loss: 2.488551139831543\n",
      "[step: 774] loss: 2.486328601837158\n",
      "[step: 775] loss: 2.484128713607788\n",
      "[step: 776] loss: 2.481950283050537\n",
      "[step: 777] loss: 2.4797961711883545\n",
      "[step: 778] loss: 2.4776618480682373\n",
      "[step: 779] loss: 2.475550651550293\n",
      "[step: 780] loss: 2.4734597206115723\n",
      "[step: 781] loss: 2.4713900089263916\n",
      "[step: 782] loss: 2.4693410396575928\n",
      "[step: 783] loss: 2.4673125743865967\n",
      "[step: 784] loss: 2.4653031826019287\n",
      "[step: 785] loss: 2.463313102722168\n",
      "[step: 786] loss: 2.461343288421631\n",
      "[step: 787] loss: 2.4593913555145264\n",
      "[step: 788] loss: 2.4574592113494873\n",
      "[step: 789] loss: 2.455544948577881\n",
      "[step: 790] loss: 2.4536492824554443\n",
      "[step: 791] loss: 2.451770782470703\n",
      "[step: 792] loss: 2.4499082565307617\n",
      "[step: 793] loss: 2.448065757751465\n",
      "[step: 794] loss: 2.4462389945983887\n",
      "[step: 795] loss: 2.444429397583008\n",
      "[step: 796] loss: 2.442636251449585\n",
      "[step: 797] loss: 2.4408583641052246\n",
      "[step: 798] loss: 2.4390978813171387\n",
      "[step: 799] loss: 2.437352418899536\n",
      "[step: 800] loss: 2.4356212615966797\n",
      "[step: 801] loss: 2.4339077472686768\n",
      "[step: 802] loss: 2.4322071075439453\n",
      "[step: 803] loss: 2.430522918701172\n",
      "[step: 804] loss: 2.4288523197174072\n",
      "[step: 805] loss: 2.4271955490112305\n",
      "[step: 806] loss: 2.425554037094116\n",
      "[step: 807] loss: 2.4239261150360107\n",
      "[step: 808] loss: 2.4223124980926514\n",
      "[step: 809] loss: 2.420710802078247\n",
      "[step: 810] loss: 2.4191246032714844\n",
      "[step: 811] loss: 2.4175493717193604\n",
      "[step: 812] loss: 2.4159882068634033\n",
      "[step: 813] loss: 2.414440631866455\n",
      "[step: 814] loss: 2.4129042625427246\n",
      "[step: 815] loss: 2.4113802909851074\n",
      "[step: 816] loss: 2.409869432449341\n",
      "[step: 817] loss: 2.408369779586792\n",
      "[step: 818] loss: 2.4068830013275146\n",
      "[step: 819] loss: 2.4054067134857178\n",
      "[step: 820] loss: 2.4039435386657715\n",
      "[step: 821] loss: 2.4024908542633057\n",
      "[step: 822] loss: 2.4010496139526367\n",
      "[step: 823] loss: 2.3996188640594482\n",
      "[step: 824] loss: 2.3981988430023193\n",
      "[step: 825] loss: 2.3967907428741455\n",
      "[step: 826] loss: 2.3953933715820312\n",
      "[step: 827] loss: 2.39400577545166\n",
      "[step: 828] loss: 2.3926286697387695\n",
      "[step: 829] loss: 2.391261339187622\n",
      "[step: 830] loss: 2.3899052143096924\n",
      "[step: 831] loss: 2.3885586261749268\n",
      "[step: 832] loss: 2.387221097946167\n",
      "[step: 833] loss: 2.3858938217163086\n",
      "[step: 834] loss: 2.384575366973877\n",
      "[step: 835] loss: 2.3832671642303467\n",
      "[step: 836] loss: 2.381967544555664\n",
      "[step: 837] loss: 2.3806779384613037\n",
      "[step: 838] loss: 2.379396438598633\n",
      "[step: 839] loss: 2.378124952316284\n",
      "[step: 840] loss: 2.376861810684204\n",
      "[step: 841] loss: 2.3756065368652344\n",
      "[step: 842] loss: 2.374361991882324\n",
      "[step: 843] loss: 2.373124361038208\n",
      "[step: 844] loss: 2.3718950748443604\n",
      "[step: 845] loss: 2.3706750869750977\n",
      "[step: 846] loss: 2.369462490081787\n",
      "[step: 847] loss: 2.3682587146759033\n",
      "[step: 848] loss: 2.3670620918273926\n",
      "[step: 849] loss: 2.3658735752105713\n",
      "[step: 850] loss: 2.3646929264068604\n",
      "[step: 851] loss: 2.363521099090576\n",
      "[step: 852] loss: 2.3623571395874023\n",
      "[step: 853] loss: 2.361198663711548\n",
      "[step: 854] loss: 2.360048294067383\n",
      "[step: 855] loss: 2.3589060306549072\n",
      "[step: 856] loss: 2.3577709197998047\n",
      "[step: 857] loss: 2.356642007827759\n",
      "[step: 858] loss: 2.3555209636688232\n",
      "[step: 859] loss: 2.354407548904419\n",
      "[step: 860] loss: 2.353299856185913\n",
      "[step: 861] loss: 2.3521993160247803\n",
      "[step: 862] loss: 2.3511064052581787\n",
      "[step: 863] loss: 2.3500192165374756\n",
      "[step: 864] loss: 2.348939895629883\n",
      "[step: 865] loss: 2.347865581512451\n",
      "[step: 866] loss: 2.346799612045288\n",
      "[step: 867] loss: 2.3457398414611816\n",
      "[step: 868] loss: 2.3446860313415527\n",
      "[step: 869] loss: 2.3436384201049805\n",
      "[step: 870] loss: 2.3425962924957275\n",
      "[step: 871] loss: 2.3415613174438477\n",
      "[step: 872] loss: 2.3405327796936035\n",
      "[step: 873] loss: 2.3395090103149414\n",
      "[step: 874] loss: 2.3384928703308105\n",
      "[step: 875] loss: 2.3374810218811035\n",
      "[step: 876] loss: 2.3364760875701904\n",
      "[step: 877] loss: 2.3354766368865967\n",
      "[step: 878] loss: 2.3344833850860596\n",
      "[step: 879] loss: 2.3334953784942627\n",
      "[step: 880] loss: 2.3325130939483643\n",
      "[step: 881] loss: 2.331535816192627\n",
      "[step: 882] loss: 2.3305647373199463\n",
      "[step: 883] loss: 2.3295998573303223\n",
      "[step: 884] loss: 2.328639507293701\n",
      "[step: 885] loss: 2.3276853561401367\n",
      "[step: 886] loss: 2.326735019683838\n",
      "[step: 887] loss: 2.325791835784912\n",
      "[step: 888] loss: 2.3248531818389893\n",
      "[step: 889] loss: 2.3239195346832275\n",
      "[step: 890] loss: 2.322990894317627\n",
      "[step: 891] loss: 2.3220672607421875\n",
      "[step: 892] loss: 2.3211488723754883\n",
      "[step: 893] loss: 2.3202357292175293\n",
      "[step: 894] loss: 2.3193278312683105\n",
      "[step: 895] loss: 2.3184237480163574\n",
      "[step: 896] loss: 2.31752610206604\n",
      "[step: 897] loss: 2.316631555557251\n",
      "[step: 898] loss: 2.315743923187256\n",
      "[step: 899] loss: 2.314858913421631\n",
      "[step: 900] loss: 2.3139801025390625\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={\n",
    "                                X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    \n",
    "    test_predict = scaler2.inverse_transform(test_predict)\n",
    "#     dataY2 = scaler2.inverse_transform(dataY2) # 데이터 있을때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "plt.title('2018-04-09 predict')\n",
    "# plt.plot(dataY2, label='row data') # 데이터 있을때\n",
    "plt.plot(test_predict, label='predict')\n",
    "plt.xlabel(\"Time Period\")\n",
    "plt.ylabel(\"photometric\")\n",
    "# plt.ylim(0, 140000)\n",
    "plt.xticks(np.arange(0, 901, step = 56), ['5', '6', '7', '8', '9', '10', '11', '12', '1', '2', '3', '4', '5', '6', '7', '8']) # v2 모델\n",
    "plt.yticks(np.arange(0, 140000, step=10000))\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
