{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 그래프 외부에 출력\n",
    "%matplotlib tk\n",
    "\n",
    "tf.set_random_seed(777)  # 어느 컴퓨터에서 이 코드를 실행해도 학습 방향이 같도록, 다시 수행해도 같도록\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Parameters\n",
    "seq_length = 88\n",
    "data_dim = 4\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 772\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1)) # 데이터 일반화\n",
    "scaler2 = MinMaxScaler(feature_range=(0, 1)) # 데이터 일반화\n",
    "\n",
    "# xy = np.loadtxt('./v3data/train_v3_data_cct.csv', delimiter=',')\n",
    "# cct, cas_swr, 446to477, uvb, temp, ptmt (892행 5열)\n",
    "xy = np.loadtxt('./v5data/train_v5_data.csv', delimiter=',')\n",
    "x = scaler.fit_transform(xy[:, [1, 2, 3,4]]) # x = 맨 마지막 ptmt 제외 모든 것\n",
    "y = scaler2.fit_transform(xy[:, [0]])  # y = ptmt\n",
    "\n",
    "\n",
    "# build a dataset\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "# print(len(x))\n",
    "# print(len(y))\n",
    "# print(x[0])\n",
    "# print(y[0])\n",
    "# print(xy[:, [0,2,3,4]])\n",
    "\n",
    "for i in range(0, len(x) - seq_length): # 한 행씩 dataX, Y에 추가\n",
    "    _x = x[i:i + seq_length]\n",
    "    _y = y[i + seq_length]  # Next close price\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "#     print(np.shape(_x))\n",
    "\n",
    "\n",
    "data = '2018-04-21'\n",
    "# xy2 = np.loadtxt('./v3data/'+ data +'.csv',delimiter=',')\n",
    "# xy2 = np.loadtxt('./v3data/test_v3_data_cct_180331.csv', delimiter=',')\n",
    "xy2 = np.loadtxt('./v5data/test_v5_data_20180421.csv', delimiter=',') # 예측할 날짜\n",
    "\n",
    "# x2 = scaler.fit_transform(xy2) # 데이터 없을때 (데이터란 ptmt 값)\n",
    "x2 = scaler.fit_transform(xy2[:, [1, 2, 3,4]]) # 데이터 있을때\n",
    "y2 = scaler2.fit_transform(xy2[:, [0]]) #데이터 있을때\n",
    "\n",
    "# print(np.shape(x2))\n",
    "\n",
    "dataX2 = []\n",
    "dataY2 = [] #데이터 있을때\n",
    "\n",
    "for i in range(0, len(x2) - seq_length):\n",
    "    _x2 = x2[i:i+seq_length]\n",
    "    _y2 = y2[i+seq_length] #데이터 있을때\n",
    "    dataX2.append(_x2)\n",
    "    dataY2.append(_y2) #데이터 있을때\n",
    "# train/test split\n",
    "\n",
    "train_size = len(dataX)\n",
    "test_size = len(dataX2)\n",
    "\n",
    "# print(train_size)\n",
    "# print(test_size)\n",
    "\n",
    "trainX, testX = np.array(dataX[0:train_size]),np.array(dataX2[0:test_size])\n",
    "trainY = np.array(dataY[0:train_size])\n",
    "\n",
    "# print(np.shape(trainX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gw/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 4582.3193359375\n",
      "[step: 1] loss: 614.9935302734375\n",
      "[step: 2] loss: 431.81109619140625\n",
      "[step: 3] loss: 1180.817138671875\n",
      "[step: 4] loss: 1155.3004150390625\n",
      "[step: 5] loss: 677.7159423828125\n",
      "[step: 6] loss: 271.30908203125\n",
      "[step: 7] loss: 147.4420166015625\n",
      "[step: 8] loss: 257.4156494140625\n",
      "[step: 9] loss: 417.9058837890625\n",
      "[step: 10] loss: 483.04815673828125\n",
      "[step: 11] loss: 429.60986328125\n",
      "[step: 12] loss: 310.45574951171875\n",
      "[step: 13] loss: 192.8248291015625\n",
      "[step: 14] loss: 122.57412719726562\n",
      "[step: 15] loss: 112.38102722167969\n",
      "[step: 16] loss: 146.35794067382812\n",
      "[step: 17] loss: 194.17166137695312\n",
      "[step: 18] loss: 227.44107055664062\n",
      "[step: 19] loss: 231.3511199951172\n",
      "[step: 20] loss: 207.54429626464844\n",
      "[step: 21] loss: 169.3095703125\n",
      "[step: 22] loss: 133.29443359375\n",
      "[step: 23] loss: 111.99490356445312\n",
      "[step: 24] loss: 109.5997085571289\n",
      "[step: 25] loss: 121.92073822021484\n",
      "[step: 26] loss: 139.6439208984375\n",
      "[step: 27] loss: 153.08035278320312\n",
      "[step: 28] loss: 156.23690795898438\n",
      "[step: 29] loss: 148.61575317382812\n",
      "[step: 30] loss: 134.39552307128906\n",
      "[step: 31] loss: 119.83374786376953\n",
      "[step: 32] loss: 110.28871154785156\n",
      "[step: 33] loss: 108.13070678710938\n",
      "[step: 34] loss: 112.23702239990234\n",
      "[step: 35] loss: 119.03978729248047\n",
      "[step: 36] loss: 124.4595718383789\n",
      "[step: 37] loss: 125.76277923583984\n",
      "[step: 38] loss: 122.53462982177734\n",
      "[step: 39] loss: 116.47477722167969\n",
      "[step: 40] loss: 110.2904281616211\n",
      "[step: 41] loss: 106.32305908203125\n",
      "[step: 42] loss: 105.5563735961914\n",
      "[step: 43] loss: 107.3890609741211\n",
      "[step: 44] loss: 110.16146850585938\n",
      "[step: 45] loss: 112.08842468261719\n",
      "[step: 46] loss: 112.10782623291016\n",
      "[step: 47] loss: 110.26007080078125\n",
      "[step: 48] loss: 107.48870086669922\n",
      "[step: 49] loss: 105.04353332519531\n",
      "[step: 50] loss: 103.83038330078125\n",
      "[step: 51] loss: 104.02664184570312\n",
      "[step: 52] loss: 105.10981750488281\n",
      "[step: 53] loss: 106.2247085571289\n",
      "[step: 54] loss: 106.65863037109375\n",
      "[step: 55] loss: 106.17269897460938\n",
      "[step: 56] loss: 105.04447174072266\n",
      "[step: 57] loss: 103.842041015625\n",
      "[step: 58] loss: 103.08576965332031\n",
      "[step: 59] loss: 102.98641204833984\n",
      "[step: 60] loss: 103.38507080078125\n",
      "[step: 61] loss: 103.89450073242188\n",
      "[step: 62] loss: 104.13954162597656\n",
      "[step: 63] loss: 103.95365142822266\n",
      "[step: 64] loss: 103.43378448486328\n",
      "[step: 65] loss: 102.84443664550781\n",
      "[step: 66] loss: 102.44636535644531\n",
      "[step: 67] loss: 102.354248046875\n",
      "[step: 68] loss: 102.49728393554688\n",
      "[step: 69] loss: 102.68828582763672\n",
      "[step: 70] loss: 102.74754333496094\n",
      "[step: 71] loss: 102.60218811035156\n",
      "[step: 72] loss: 102.31011962890625\n",
      "[step: 73] loss: 102.00540924072266\n",
      "[step: 74] loss: 101.8082504272461\n",
      "[step: 75] loss: 101.75825500488281\n",
      "[step: 76] loss: 101.80451202392578\n",
      "[step: 77] loss: 101.85034942626953\n",
      "[step: 78] loss: 101.81776428222656\n",
      "[step: 79] loss: 101.69070434570312\n",
      "[step: 80] loss: 101.51443481445312\n",
      "[step: 81] loss: 101.35763549804688\n",
      "[step: 82] loss: 101.2672119140625\n",
      "[step: 83] loss: 101.24331665039062\n",
      "[step: 84] loss: 101.24678039550781\n",
      "[step: 85] loss: 101.23002624511719\n",
      "[step: 86] loss: 101.16748046875\n",
      "[step: 87] loss: 101.06820678710938\n",
      "[step: 88] loss: 100.9637451171875\n",
      "[step: 89] loss: 100.88490295410156\n",
      "[step: 90] loss: 100.84176635742188\n",
      "[step: 91] loss: 100.82125091552734\n",
      "[step: 92] loss: 100.79865264892578\n",
      "[step: 93] loss: 100.75621795654297\n",
      "[step: 94] loss: 100.69276428222656\n",
      "[step: 95] loss: 100.62197875976562\n",
      "[step: 96] loss: 100.56085968017578\n",
      "[step: 97] loss: 100.51757049560547\n",
      "[step: 98] loss: 100.48809051513672\n",
      "[step: 99] loss: 100.46033477783203\n",
      "[step: 100] loss: 100.42389678955078\n",
      "[step: 101] loss: 100.37693786621094\n",
      "[step: 102] loss: 100.32551574707031\n",
      "[step: 103] loss: 100.27832794189453\n",
      "[step: 104] loss: 100.24044036865234\n",
      "[step: 105] loss: 100.21006774902344\n",
      "[step: 106] loss: 100.18135070800781\n",
      "[step: 107] loss: 100.14886474609375\n",
      "[step: 108] loss: 100.1114501953125\n",
      "[step: 109] loss: 100.072021484375\n",
      "[step: 110] loss: 100.03520202636719\n",
      "[step: 111] loss: 100.00341796875\n",
      "[step: 112] loss: 99.97572326660156\n",
      "[step: 113] loss: 99.94882202148438\n",
      "[step: 114] loss: 99.92025756835938\n",
      "[step: 115] loss: 99.88948059082031\n",
      "[step: 116] loss: 99.8582992553711\n",
      "[step: 117] loss: 99.82893371582031\n",
      "[step: 118] loss: 99.80244445800781\n",
      "[step: 119] loss: 99.77796936035156\n",
      "[step: 120] loss: 99.75386047363281\n",
      "[step: 121] loss: 99.72897338867188\n",
      "[step: 122] loss: 99.70323181152344\n",
      "[step: 123] loss: 99.67779541015625\n",
      "[step: 124] loss: 99.6536865234375\n",
      "[step: 125] loss: 99.63114929199219\n",
      "[step: 126] loss: 99.60974884033203\n",
      "[step: 127] loss: 99.58838653564453\n",
      "[step: 128] loss: 99.56670379638672\n",
      "[step: 129] loss: 99.54491424560547\n",
      "[step: 130] loss: 99.52363586425781\n",
      "[step: 131] loss: 99.5034408569336\n",
      "[step: 132] loss: 99.48405456542969\n",
      "[step: 133] loss: 99.46516418457031\n",
      "[step: 134] loss: 99.44630432128906\n",
      "[step: 135] loss: 99.42748260498047\n",
      "[step: 136] loss: 99.40878295898438\n",
      "[step: 137] loss: 99.39076232910156\n",
      "[step: 138] loss: 99.37327575683594\n",
      "[step: 139] loss: 99.3563461303711\n",
      "[step: 140] loss: 99.33969116210938\n",
      "[step: 141] loss: 99.32302856445312\n",
      "[step: 142] loss: 99.30661010742188\n",
      "[step: 143] loss: 99.2904052734375\n",
      "[step: 144] loss: 99.27467346191406\n",
      "[step: 145] loss: 99.25936889648438\n",
      "[step: 146] loss: 99.24421691894531\n",
      "[step: 147] loss: 99.2293701171875\n",
      "[step: 148] loss: 99.21452331542969\n",
      "[step: 149] loss: 99.1999282836914\n",
      "[step: 150] loss: 99.18568420410156\n",
      "[step: 151] loss: 99.171630859375\n",
      "[step: 152] loss: 99.15785217285156\n",
      "[step: 153] loss: 99.14422607421875\n",
      "[step: 154] loss: 99.13070678710938\n",
      "[step: 155] loss: 99.11742401123047\n",
      "[step: 156] loss: 99.10417175292969\n",
      "[step: 157] loss: 99.09127807617188\n",
      "[step: 158] loss: 99.07846069335938\n",
      "[step: 159] loss: 99.06581115722656\n",
      "[step: 160] loss: 99.05325317382812\n",
      "[step: 161] loss: 99.04086303710938\n",
      "[step: 162] loss: 99.02860260009766\n",
      "[step: 163] loss: 99.01641845703125\n",
      "[step: 164] loss: 99.00448608398438\n",
      "[step: 165] loss: 98.99259948730469\n",
      "[step: 166] loss: 98.980712890625\n",
      "[step: 167] loss: 98.9691162109375\n",
      "[step: 168] loss: 98.95743560791016\n",
      "[step: 169] loss: 98.94595336914062\n",
      "[step: 170] loss: 98.93455505371094\n",
      "[step: 171] loss: 98.92328643798828\n",
      "[step: 172] loss: 98.91200256347656\n",
      "[step: 173] loss: 98.9007797241211\n",
      "[step: 174] loss: 98.88970184326172\n",
      "[step: 175] loss: 98.87867736816406\n",
      "[step: 176] loss: 98.86770629882812\n",
      "[step: 177] loss: 98.85678100585938\n",
      "[step: 178] loss: 98.8459243774414\n",
      "[step: 179] loss: 98.8351058959961\n",
      "[step: 180] loss: 98.82443237304688\n",
      "[step: 181] loss: 98.81371307373047\n",
      "[step: 182] loss: 98.80307006835938\n",
      "[step: 183] loss: 98.79240417480469\n",
      "[step: 184] loss: 98.7818603515625\n",
      "[step: 185] loss: 98.77131652832031\n",
      "[step: 186] loss: 98.76078796386719\n",
      "[step: 187] loss: 98.75033569335938\n",
      "[step: 188] loss: 98.73993682861328\n",
      "[step: 189] loss: 98.72955322265625\n",
      "[step: 190] loss: 98.71916198730469\n",
      "[step: 191] loss: 98.70878601074219\n",
      "[step: 192] loss: 98.69844055175781\n",
      "[step: 193] loss: 98.68807220458984\n",
      "[step: 194] loss: 98.67779541015625\n",
      "[step: 195] loss: 98.66752624511719\n",
      "[step: 196] loss: 98.65724182128906\n",
      "[step: 197] loss: 98.64700317382812\n",
      "[step: 198] loss: 98.63673400878906\n",
      "[step: 199] loss: 98.62657165527344\n",
      "[step: 200] loss: 98.61628723144531\n",
      "[step: 201] loss: 98.6060562133789\n",
      "[step: 202] loss: 98.59584045410156\n",
      "[step: 203] loss: 98.58561706542969\n",
      "[step: 204] loss: 98.57546997070312\n",
      "[step: 205] loss: 98.5652847290039\n",
      "[step: 206] loss: 98.55511474609375\n",
      "[step: 207] loss: 98.54496002197266\n",
      "[step: 208] loss: 98.53477478027344\n",
      "[step: 209] loss: 98.52452087402344\n",
      "[step: 210] loss: 98.51435852050781\n",
      "[step: 211] loss: 98.50421142578125\n",
      "[step: 212] loss: 98.49406433105469\n",
      "[step: 213] loss: 98.48384094238281\n",
      "[step: 214] loss: 98.47366333007812\n",
      "[step: 215] loss: 98.46342468261719\n",
      "[step: 216] loss: 98.45320129394531\n",
      "[step: 217] loss: 98.4428939819336\n",
      "[step: 218] loss: 98.43276977539062\n",
      "[step: 219] loss: 98.42257690429688\n",
      "[step: 220] loss: 98.41232299804688\n",
      "[step: 221] loss: 98.40214538574219\n",
      "[step: 222] loss: 98.39189147949219\n",
      "[step: 223] loss: 98.3816909790039\n",
      "[step: 224] loss: 98.37144470214844\n",
      "[step: 225] loss: 98.36117553710938\n",
      "[step: 226] loss: 98.3509521484375\n",
      "[step: 227] loss: 98.34073638916016\n",
      "[step: 228] loss: 98.33038330078125\n",
      "[step: 229] loss: 98.32012176513672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 230] loss: 98.30979919433594\n",
      "[step: 231] loss: 98.29946899414062\n",
      "[step: 232] loss: 98.28910827636719\n",
      "[step: 233] loss: 98.27880096435547\n",
      "[step: 234] loss: 98.26840209960938\n",
      "[step: 235] loss: 98.2580795288086\n",
      "[step: 236] loss: 98.24769592285156\n",
      "[step: 237] loss: 98.23733520507812\n",
      "[step: 238] loss: 98.22693634033203\n",
      "[step: 239] loss: 98.21649169921875\n",
      "[step: 240] loss: 98.20613098144531\n",
      "[step: 241] loss: 98.19560241699219\n",
      "[step: 242] loss: 98.18521118164062\n",
      "[step: 243] loss: 98.17471313476562\n",
      "[step: 244] loss: 98.16419982910156\n",
      "[step: 245] loss: 98.15371704101562\n",
      "[step: 246] loss: 98.14324951171875\n",
      "[step: 247] loss: 98.13276672363281\n",
      "[step: 248] loss: 98.1221923828125\n",
      "[step: 249] loss: 98.111572265625\n",
      "[step: 250] loss: 98.10105895996094\n",
      "[step: 251] loss: 98.09046936035156\n",
      "[step: 252] loss: 98.07982635498047\n",
      "[step: 253] loss: 98.0692138671875\n",
      "[step: 254] loss: 98.05860900878906\n",
      "[step: 255] loss: 98.04791259765625\n",
      "[step: 256] loss: 98.03724670410156\n",
      "[step: 257] loss: 98.02655029296875\n",
      "[step: 258] loss: 98.01591491699219\n",
      "[step: 259] loss: 98.00516510009766\n",
      "[step: 260] loss: 97.99439239501953\n",
      "[step: 261] loss: 97.98360443115234\n",
      "[step: 262] loss: 97.97286987304688\n",
      "[step: 263] loss: 97.9621353149414\n",
      "[step: 264] loss: 97.9512939453125\n",
      "[step: 265] loss: 97.94044494628906\n",
      "[step: 266] loss: 97.92958068847656\n",
      "[step: 267] loss: 97.9187240600586\n",
      "[step: 268] loss: 97.90784454345703\n",
      "[step: 269] loss: 97.89689636230469\n",
      "[step: 270] loss: 97.88601684570312\n",
      "[step: 271] loss: 97.87513732910156\n",
      "[step: 272] loss: 97.8641128540039\n",
      "[step: 273] loss: 97.85307312011719\n",
      "[step: 274] loss: 97.84219360351562\n",
      "[step: 275] loss: 97.83110046386719\n",
      "[step: 276] loss: 97.81999206542969\n",
      "[step: 277] loss: 97.80892944335938\n",
      "[step: 278] loss: 97.79788208007812\n",
      "[step: 279] loss: 97.78680419921875\n",
      "[step: 280] loss: 97.77560424804688\n",
      "[step: 281] loss: 97.76448059082031\n",
      "[step: 282] loss: 97.75331115722656\n",
      "[step: 283] loss: 97.74208068847656\n",
      "[step: 284] loss: 97.73085021972656\n",
      "[step: 285] loss: 97.71963500976562\n",
      "[step: 286] loss: 97.70835876464844\n",
      "[step: 287] loss: 97.6970443725586\n",
      "[step: 288] loss: 97.68572998046875\n",
      "[step: 289] loss: 97.67440795898438\n",
      "[step: 290] loss: 97.66300201416016\n",
      "[step: 291] loss: 97.65167236328125\n",
      "[step: 292] loss: 97.64027404785156\n",
      "[step: 293] loss: 97.62884521484375\n",
      "[step: 294] loss: 97.6173095703125\n",
      "[step: 295] loss: 97.60581970214844\n",
      "[step: 296] loss: 97.59432983398438\n",
      "[step: 297] loss: 97.58273315429688\n",
      "[step: 298] loss: 97.57124328613281\n",
      "[step: 299] loss: 97.55962371826172\n",
      "[step: 300] loss: 97.54801940917969\n",
      "[step: 301] loss: 97.53630065917969\n",
      "[step: 302] loss: 97.5246353149414\n",
      "[step: 303] loss: 97.51294708251953\n",
      "[step: 304] loss: 97.501220703125\n",
      "[step: 305] loss: 97.48947143554688\n",
      "[step: 306] loss: 97.47760009765625\n",
      "[step: 307] loss: 97.46580505371094\n",
      "[step: 308] loss: 97.45397186279297\n",
      "[step: 309] loss: 97.44212341308594\n",
      "[step: 310] loss: 97.4301986694336\n",
      "[step: 311] loss: 97.4183120727539\n",
      "[step: 312] loss: 97.40635681152344\n",
      "[step: 313] loss: 97.39431762695312\n",
      "[step: 314] loss: 97.38226318359375\n",
      "[step: 315] loss: 97.37020874023438\n",
      "[step: 316] loss: 97.35806274414062\n",
      "[step: 317] loss: 97.34599304199219\n",
      "[step: 318] loss: 97.33381652832031\n",
      "[step: 319] loss: 97.32159423828125\n",
      "[step: 320] loss: 97.30937194824219\n",
      "[step: 321] loss: 97.297119140625\n",
      "[step: 322] loss: 97.28489685058594\n",
      "[step: 323] loss: 97.27250671386719\n",
      "[step: 324] loss: 97.26019287109375\n",
      "[step: 325] loss: 97.24781036376953\n",
      "[step: 326] loss: 97.2353744506836\n",
      "[step: 327] loss: 97.22288513183594\n",
      "[step: 328] loss: 97.21046447753906\n",
      "[step: 329] loss: 97.19795227050781\n",
      "[step: 330] loss: 97.18538665771484\n",
      "[step: 331] loss: 97.17284393310547\n",
      "[step: 332] loss: 97.16021728515625\n",
      "[step: 333] loss: 97.14751434326172\n",
      "[step: 334] loss: 97.13481140136719\n",
      "[step: 335] loss: 97.12203216552734\n",
      "[step: 336] loss: 97.10924530029297\n",
      "[step: 337] loss: 97.0964126586914\n",
      "[step: 338] loss: 97.08354187011719\n",
      "[step: 339] loss: 97.07062530517578\n",
      "[step: 340] loss: 97.0576400756836\n",
      "[step: 341] loss: 97.04469299316406\n",
      "[step: 342] loss: 97.03169250488281\n",
      "[step: 343] loss: 97.0185775756836\n",
      "[step: 344] loss: 97.00550079345703\n",
      "[step: 345] loss: 96.99235534667969\n",
      "[step: 346] loss: 96.97917175292969\n",
      "[step: 347] loss: 96.96598815917969\n",
      "[step: 348] loss: 96.95269012451172\n",
      "[step: 349] loss: 96.93936157226562\n",
      "[step: 350] loss: 96.926025390625\n",
      "[step: 351] loss: 96.9126205444336\n",
      "[step: 352] loss: 96.89916229248047\n",
      "[step: 353] loss: 96.88571166992188\n",
      "[step: 354] loss: 96.87214660644531\n",
      "[step: 355] loss: 96.85858154296875\n",
      "[step: 356] loss: 96.8449935913086\n",
      "[step: 357] loss: 96.83125305175781\n",
      "[step: 358] loss: 96.81753540039062\n",
      "[step: 359] loss: 96.80378723144531\n",
      "[step: 360] loss: 96.78997039794922\n",
      "[step: 361] loss: 96.77610778808594\n",
      "[step: 362] loss: 96.76211547851562\n",
      "[step: 363] loss: 96.7481689453125\n",
      "[step: 364] loss: 96.73419952392578\n",
      "[step: 365] loss: 96.72015380859375\n",
      "[step: 366] loss: 96.7060317993164\n",
      "[step: 367] loss: 96.69188690185547\n",
      "[step: 368] loss: 96.6776123046875\n",
      "[step: 369] loss: 96.66336059570312\n",
      "[step: 370] loss: 96.64909362792969\n",
      "[step: 371] loss: 96.63471221923828\n",
      "[step: 372] loss: 96.62031555175781\n",
      "[step: 373] loss: 96.60577392578125\n",
      "[step: 374] loss: 96.59130859375\n",
      "[step: 375] loss: 96.57671356201172\n",
      "[step: 376] loss: 96.5621337890625\n",
      "[step: 377] loss: 96.54739379882812\n",
      "[step: 378] loss: 96.53263854980469\n",
      "[step: 379] loss: 96.51786804199219\n",
      "[step: 380] loss: 96.50298309326172\n",
      "[step: 381] loss: 96.48809051513672\n",
      "[step: 382] loss: 96.47308349609375\n",
      "[step: 383] loss: 96.45803833007812\n",
      "[step: 384] loss: 96.44292449951172\n",
      "[step: 385] loss: 96.42778778076172\n",
      "[step: 386] loss: 96.4125747680664\n",
      "[step: 387] loss: 96.39732360839844\n",
      "[step: 388] loss: 96.38192749023438\n",
      "[step: 389] loss: 96.3665771484375\n",
      "[step: 390] loss: 96.3511734008789\n",
      "[step: 391] loss: 96.33555603027344\n",
      "[step: 392] loss: 96.31997680664062\n",
      "[step: 393] loss: 96.30430603027344\n",
      "[step: 394] loss: 96.28863525390625\n",
      "[step: 395] loss: 96.27281188964844\n",
      "[step: 396] loss: 96.25698852539062\n",
      "[step: 397] loss: 96.24112701416016\n",
      "[step: 398] loss: 96.2251205444336\n",
      "[step: 399] loss: 96.20906066894531\n",
      "[step: 400] loss: 96.19288635253906\n",
      "[step: 401] loss: 96.17674255371094\n",
      "[step: 402] loss: 96.16051483154297\n",
      "[step: 403] loss: 96.1441421508789\n",
      "[step: 404] loss: 96.12776184082031\n",
      "[step: 405] loss: 96.11126708984375\n",
      "[step: 406] loss: 96.09475708007812\n",
      "[step: 407] loss: 96.07814025878906\n",
      "[step: 408] loss: 96.06140899658203\n",
      "[step: 409] loss: 96.04473876953125\n",
      "[step: 410] loss: 96.02791595458984\n",
      "[step: 411] loss: 96.01094055175781\n",
      "[step: 412] loss: 95.99395751953125\n",
      "[step: 413] loss: 95.9769287109375\n",
      "[step: 414] loss: 95.95974731445312\n",
      "[step: 415] loss: 95.94259643554688\n",
      "[step: 416] loss: 95.92523193359375\n",
      "[step: 417] loss: 95.90786743164062\n",
      "[step: 418] loss: 95.89041137695312\n",
      "[step: 419] loss: 95.87286376953125\n",
      "[step: 420] loss: 95.85526275634766\n",
      "[step: 421] loss: 95.83756256103516\n",
      "[step: 422] loss: 95.81978607177734\n",
      "[step: 423] loss: 95.80196380615234\n",
      "[step: 424] loss: 95.78392791748047\n",
      "[step: 425] loss: 95.76596069335938\n",
      "[step: 426] loss: 95.747802734375\n",
      "[step: 427] loss: 95.7296142578125\n",
      "[step: 428] loss: 95.7113037109375\n",
      "[step: 429] loss: 95.69296264648438\n",
      "[step: 430] loss: 95.67451477050781\n",
      "[step: 431] loss: 95.65587615966797\n",
      "[step: 432] loss: 95.63723754882812\n",
      "[step: 433] loss: 95.61849212646484\n",
      "[step: 434] loss: 95.59963989257812\n",
      "[step: 435] loss: 95.58071899414062\n",
      "[step: 436] loss: 95.56172180175781\n",
      "[step: 437] loss: 95.5425796508789\n",
      "[step: 438] loss: 95.5233154296875\n",
      "[step: 439] loss: 95.50399780273438\n",
      "[step: 440] loss: 95.48460388183594\n",
      "[step: 441] loss: 95.4651107788086\n",
      "[step: 442] loss: 95.44551086425781\n",
      "[step: 443] loss: 95.42578125\n",
      "[step: 444] loss: 95.40596008300781\n",
      "[step: 445] loss: 95.3859634399414\n",
      "[step: 446] loss: 95.36597442626953\n",
      "[step: 447] loss: 95.34590148925781\n",
      "[step: 448] loss: 95.32559204101562\n",
      "[step: 449] loss: 95.30532836914062\n",
      "[step: 450] loss: 95.28488159179688\n",
      "[step: 451] loss: 95.26429748535156\n",
      "[step: 452] loss: 95.24365234375\n",
      "[step: 453] loss: 95.22288513183594\n",
      "[step: 454] loss: 95.20194244384766\n",
      "[step: 455] loss: 95.18098449707031\n",
      "[step: 456] loss: 95.15983581542969\n",
      "[step: 457] loss: 95.13876342773438\n",
      "[step: 458] loss: 95.11739349365234\n",
      "[step: 459] loss: 95.09596252441406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 460] loss: 95.07431030273438\n",
      "[step: 461] loss: 95.05268859863281\n",
      "[step: 462] loss: 95.03080749511719\n",
      "[step: 463] loss: 95.00889587402344\n",
      "[step: 464] loss: 94.98686218261719\n",
      "[step: 465] loss: 94.96467590332031\n",
      "[step: 466] loss: 94.94242858886719\n",
      "[step: 467] loss: 94.91999816894531\n",
      "[step: 468] loss: 94.89743041992188\n",
      "[step: 469] loss: 94.87480163574219\n",
      "[step: 470] loss: 94.85193634033203\n",
      "[step: 471] loss: 94.82904815673828\n",
      "[step: 472] loss: 94.8060302734375\n",
      "[step: 473] loss: 94.78279113769531\n",
      "[step: 474] loss: 94.75946044921875\n",
      "[step: 475] loss: 94.73599243164062\n",
      "[step: 476] loss: 94.71240234375\n",
      "[step: 477] loss: 94.68865966796875\n",
      "[step: 478] loss: 94.66475677490234\n",
      "[step: 479] loss: 94.64070129394531\n",
      "[step: 480] loss: 94.61654663085938\n",
      "[step: 481] loss: 94.59220886230469\n",
      "[step: 482] loss: 94.56787109375\n",
      "[step: 483] loss: 94.54322814941406\n",
      "[step: 484] loss: 94.51840209960938\n",
      "[step: 485] loss: 94.4935531616211\n",
      "[step: 486] loss: 94.46855163574219\n",
      "[step: 487] loss: 94.44329833984375\n",
      "[step: 488] loss: 94.41797637939453\n",
      "[step: 489] loss: 94.39250183105469\n",
      "[step: 490] loss: 94.3668212890625\n",
      "[step: 491] loss: 94.34101104736328\n",
      "[step: 492] loss: 94.31498718261719\n",
      "[step: 493] loss: 94.28887939453125\n",
      "[step: 494] loss: 94.26260375976562\n",
      "[step: 495] loss: 94.23609924316406\n",
      "[step: 496] loss: 94.20945739746094\n",
      "[step: 497] loss: 94.18270874023438\n",
      "[step: 498] loss: 94.15567779541016\n",
      "[step: 499] loss: 94.1285629272461\n",
      "[step: 500] loss: 94.10127258300781\n",
      "[step: 501] loss: 94.07374572753906\n",
      "[step: 502] loss: 94.04607391357422\n",
      "[step: 503] loss: 94.0181884765625\n",
      "[step: 504] loss: 93.99024963378906\n",
      "[step: 505] loss: 93.96197509765625\n",
      "[step: 506] loss: 93.93360900878906\n",
      "[step: 507] loss: 93.90492248535156\n",
      "[step: 508] loss: 93.87625122070312\n",
      "[step: 509] loss: 93.8472671508789\n",
      "[step: 510] loss: 93.81808471679688\n",
      "[step: 511] loss: 93.7887954711914\n",
      "[step: 512] loss: 93.75920104980469\n",
      "[step: 513] loss: 93.72947692871094\n",
      "[step: 514] loss: 93.69955444335938\n",
      "[step: 515] loss: 93.66940307617188\n",
      "[step: 516] loss: 93.63909912109375\n",
      "[step: 517] loss: 93.60842895507812\n",
      "[step: 518] loss: 93.57765197753906\n",
      "[step: 519] loss: 93.54666137695312\n",
      "[step: 520] loss: 93.51542663574219\n",
      "[step: 521] loss: 93.48400115966797\n",
      "[step: 522] loss: 93.45238494873047\n",
      "[step: 523] loss: 93.42050170898438\n",
      "[step: 524] loss: 93.38839721679688\n",
      "[step: 525] loss: 93.35603332519531\n",
      "[step: 526] loss: 93.32351684570312\n",
      "[step: 527] loss: 93.29069519042969\n",
      "[step: 528] loss: 93.25763702392578\n",
      "[step: 529] loss: 93.22431945800781\n",
      "[step: 530] loss: 93.19085693359375\n",
      "[step: 531] loss: 93.15708923339844\n",
      "[step: 532] loss: 93.12307739257812\n",
      "[step: 533] loss: 93.08879852294922\n",
      "[step: 534] loss: 93.05421447753906\n",
      "[step: 535] loss: 93.01945495605469\n",
      "[step: 536] loss: 92.9843978881836\n",
      "[step: 537] loss: 92.9490737915039\n",
      "[step: 538] loss: 92.91342163085938\n",
      "[step: 539] loss: 92.87761688232422\n",
      "[step: 540] loss: 92.84147644042969\n",
      "[step: 541] loss: 92.80506134033203\n",
      "[step: 542] loss: 92.76828002929688\n",
      "[step: 543] loss: 92.73126220703125\n",
      "[step: 544] loss: 92.69398498535156\n",
      "[step: 545] loss: 92.65631103515625\n",
      "[step: 546] loss: 92.61843872070312\n",
      "[step: 547] loss: 92.58023071289062\n",
      "[step: 548] loss: 92.54170227050781\n",
      "[step: 549] loss: 92.50282287597656\n",
      "[step: 550] loss: 92.46366882324219\n",
      "[step: 551] loss: 92.42415618896484\n",
      "[step: 552] loss: 92.38433074951172\n",
      "[step: 553] loss: 92.34416198730469\n",
      "[step: 554] loss: 92.30364227294922\n",
      "[step: 555] loss: 92.26277923583984\n",
      "[step: 556] loss: 92.22164154052734\n",
      "[step: 557] loss: 92.1799545288086\n",
      "[step: 558] loss: 92.13804626464844\n",
      "[step: 559] loss: 92.09573364257812\n",
      "[step: 560] loss: 92.05294036865234\n",
      "[step: 561] loss: 92.00988006591797\n",
      "[step: 562] loss: 91.96640014648438\n",
      "[step: 563] loss: 91.92256927490234\n",
      "[step: 564] loss: 91.87815856933594\n",
      "[step: 565] loss: 91.83348083496094\n",
      "[step: 566] loss: 91.78839111328125\n",
      "[step: 567] loss: 91.74285888671875\n",
      "[step: 568] loss: 91.69683074951172\n",
      "[step: 569] loss: 91.65040588378906\n",
      "[step: 570] loss: 91.60346984863281\n",
      "[step: 571] loss: 91.5560531616211\n",
      "[step: 572] loss: 91.50820922851562\n",
      "[step: 573] loss: 91.4599609375\n",
      "[step: 574] loss: 91.41110229492188\n",
      "[step: 575] loss: 91.36177825927734\n",
      "[step: 576] loss: 91.31193542480469\n",
      "[step: 577] loss: 91.26155853271484\n",
      "[step: 578] loss: 91.21063232421875\n",
      "[step: 579] loss: 91.15919494628906\n",
      "[step: 580] loss: 91.1072006225586\n",
      "[step: 581] loss: 91.0545883178711\n",
      "[step: 582] loss: 91.00138854980469\n",
      "[step: 583] loss: 90.94762420654297\n",
      "[step: 584] loss: 90.89320373535156\n",
      "[step: 585] loss: 90.83824920654297\n",
      "[step: 586] loss: 90.78246307373047\n",
      "[step: 587] loss: 90.72616577148438\n",
      "[step: 588] loss: 90.66926574707031\n",
      "[step: 589] loss: 90.61151123046875\n",
      "[step: 590] loss: 90.55303192138672\n",
      "[step: 591] loss: 90.49395751953125\n",
      "[step: 592] loss: 90.43406677246094\n",
      "[step: 593] loss: 90.3734130859375\n",
      "[step: 594] loss: 90.31204223632812\n",
      "[step: 595] loss: 90.24971771240234\n",
      "[step: 596] loss: 90.18666076660156\n",
      "[step: 597] loss: 90.122802734375\n",
      "[step: 598] loss: 90.05792236328125\n",
      "[step: 599] loss: 89.99229431152344\n",
      "[step: 600] loss: 89.9256362915039\n",
      "[step: 601] loss: 89.85812377929688\n",
      "[step: 602] loss: 89.78956604003906\n",
      "[step: 603] loss: 89.72001647949219\n",
      "[step: 604] loss: 89.6495590209961\n",
      "[step: 605] loss: 89.57789611816406\n",
      "[step: 606] loss: 89.50509643554688\n",
      "[step: 607] loss: 89.43133544921875\n",
      "[step: 608] loss: 89.35636901855469\n",
      "[step: 609] loss: 89.28019714355469\n",
      "[step: 610] loss: 89.20286560058594\n",
      "[step: 611] loss: 89.12425231933594\n",
      "[step: 612] loss: 89.04441833496094\n",
      "[step: 613] loss: 88.96321105957031\n",
      "[step: 614] loss: 88.88062286376953\n",
      "[step: 615] loss: 88.79669189453125\n",
      "[step: 616] loss: 88.71138000488281\n",
      "[step: 617] loss: 88.62464904785156\n",
      "[step: 618] loss: 88.53636932373047\n",
      "[step: 619] loss: 88.44664764404297\n",
      "[step: 620] loss: 88.35539245605469\n",
      "[step: 621] loss: 88.26252746582031\n",
      "[step: 622] loss: 88.16807556152344\n",
      "[step: 623] loss: 88.07203674316406\n",
      "[step: 624] loss: 87.97441101074219\n",
      "[step: 625] loss: 87.87516784667969\n",
      "[step: 626] loss: 87.77438354492188\n",
      "[step: 627] loss: 87.67195129394531\n",
      "[step: 628] loss: 87.5680923461914\n",
      "[step: 629] loss: 87.46263122558594\n",
      "[step: 630] loss: 87.35582733154297\n",
      "[step: 631] loss: 87.24759674072266\n",
      "[step: 632] loss: 87.1380386352539\n",
      "[step: 633] loss: 87.02754211425781\n",
      "[step: 634] loss: 86.91610717773438\n",
      "[step: 635] loss: 86.80400848388672\n",
      "[step: 636] loss: 86.69145965576172\n",
      "[step: 637] loss: 86.57879638671875\n",
      "[step: 638] loss: 86.46635437011719\n",
      "[step: 639] loss: 86.3546142578125\n",
      "[step: 640] loss: 86.2440414428711\n",
      "[step: 641] loss: 86.13517761230469\n",
      "[step: 642] loss: 86.02848815917969\n",
      "[step: 643] loss: 85.92477416992188\n",
      "[step: 644] loss: 85.82466125488281\n",
      "[step: 645] loss: 85.72881317138672\n",
      "[step: 646] loss: 85.63780975341797\n",
      "[step: 647] loss: 85.5523910522461\n",
      "[step: 648] loss: 85.47303771972656\n",
      "[step: 649] loss: 85.40013885498047\n",
      "[step: 650] loss: 85.33399963378906\n",
      "[step: 651] loss: 85.274658203125\n",
      "[step: 652] loss: 85.22199249267578\n",
      "[step: 653] loss: 85.17542266845703\n",
      "[step: 654] loss: 85.13456726074219\n",
      "[step: 655] loss: 85.09877014160156\n",
      "[step: 656] loss: 85.0670166015625\n",
      "[step: 657] loss: 85.03849792480469\n",
      "[step: 658] loss: 85.012451171875\n",
      "[step: 659] loss: 84.98817443847656\n",
      "[step: 660] loss: 84.9650650024414\n",
      "[step: 661] loss: 84.94271850585938\n",
      "[step: 662] loss: 84.92074584960938\n",
      "[step: 663] loss: 84.89896392822266\n",
      "[step: 664] loss: 84.87721252441406\n",
      "[step: 665] loss: 84.85540008544922\n",
      "[step: 666] loss: 84.83355712890625\n",
      "[step: 667] loss: 84.81153869628906\n",
      "[step: 668] loss: 84.78936767578125\n",
      "[step: 669] loss: 84.76699829101562\n",
      "[step: 670] loss: 84.74431610107422\n",
      "[step: 671] loss: 84.72149658203125\n",
      "[step: 672] loss: 84.69856262207031\n",
      "[step: 673] loss: 84.67561340332031\n",
      "[step: 674] loss: 84.65281677246094\n",
      "[step: 675] loss: 84.63024139404297\n",
      "[step: 676] loss: 84.60813903808594\n",
      "[step: 677] loss: 84.5865249633789\n",
      "[step: 678] loss: 84.56553649902344\n",
      "[step: 679] loss: 84.54518127441406\n",
      "[step: 680] loss: 84.52555084228516\n",
      "[step: 681] loss: 84.50653076171875\n",
      "[step: 682] loss: 84.48825073242188\n",
      "[step: 683] loss: 84.47050476074219\n",
      "[step: 684] loss: 84.45313262939453\n",
      "[step: 685] loss: 84.43624877929688\n",
      "[step: 686] loss: 84.41963195800781\n",
      "[step: 687] loss: 84.40327453613281\n",
      "[step: 688] loss: 84.38701629638672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 689] loss: 84.3709487915039\n",
      "[step: 690] loss: 84.35496520996094\n",
      "[step: 691] loss: 84.33908081054688\n",
      "[step: 692] loss: 84.3232650756836\n",
      "[step: 693] loss: 84.30751037597656\n",
      "[step: 694] loss: 84.29188537597656\n",
      "[step: 695] loss: 84.27622985839844\n",
      "[step: 696] loss: 84.26071166992188\n",
      "[step: 697] loss: 84.24526977539062\n",
      "[step: 698] loss: 84.22987365722656\n",
      "[step: 699] loss: 84.21464538574219\n",
      "[step: 700] loss: 84.19947052001953\n",
      "[step: 701] loss: 84.18438720703125\n",
      "[step: 702] loss: 84.16932678222656\n",
      "[step: 703] loss: 84.15435791015625\n",
      "[step: 704] loss: 84.13954162597656\n",
      "[step: 705] loss: 84.12477111816406\n",
      "[step: 706] loss: 84.11009979248047\n",
      "[step: 707] loss: 84.09549713134766\n",
      "[step: 708] loss: 84.08106231689453\n",
      "[step: 709] loss: 84.06666564941406\n",
      "[step: 710] loss: 84.0523910522461\n",
      "[step: 711] loss: 84.03821563720703\n",
      "[step: 712] loss: 84.02406311035156\n",
      "[step: 713] loss: 84.01007843017578\n",
      "[step: 714] loss: 83.99614715576172\n",
      "[step: 715] loss: 83.9823226928711\n",
      "[step: 716] loss: 83.96852111816406\n",
      "[step: 717] loss: 83.95480346679688\n",
      "[step: 718] loss: 83.94116973876953\n",
      "[step: 719] loss: 83.92765808105469\n",
      "[step: 720] loss: 83.91411590576172\n",
      "[step: 721] loss: 83.900634765625\n",
      "[step: 722] loss: 83.88728332519531\n",
      "[step: 723] loss: 83.87396240234375\n",
      "[step: 724] loss: 83.86067962646484\n",
      "[step: 725] loss: 83.84747314453125\n",
      "[step: 726] loss: 83.83430480957031\n",
      "[step: 727] loss: 83.82122802734375\n",
      "[step: 728] loss: 83.80825805664062\n",
      "[step: 729] loss: 83.79525756835938\n",
      "[step: 730] loss: 83.78239440917969\n",
      "[step: 731] loss: 83.76956176757812\n",
      "[step: 732] loss: 83.75679016113281\n",
      "[step: 733] loss: 83.74409484863281\n",
      "[step: 734] loss: 83.73139953613281\n",
      "[step: 735] loss: 83.7187728881836\n",
      "[step: 736] loss: 83.70620727539062\n",
      "[step: 737] loss: 83.6937484741211\n",
      "[step: 738] loss: 83.68130493164062\n",
      "[step: 739] loss: 83.66893768310547\n",
      "[step: 740] loss: 83.65666198730469\n",
      "[step: 741] loss: 83.64434814453125\n",
      "[step: 742] loss: 83.63218688964844\n",
      "[step: 743] loss: 83.62004852294922\n",
      "[step: 744] loss: 83.60792541503906\n",
      "[step: 745] loss: 83.5958251953125\n",
      "[step: 746] loss: 83.58386993408203\n",
      "[step: 747] loss: 83.57192993164062\n",
      "[step: 748] loss: 83.55998229980469\n",
      "[step: 749] loss: 83.54816436767578\n",
      "[step: 750] loss: 83.53634643554688\n",
      "[step: 751] loss: 83.52461242675781\n",
      "[step: 752] loss: 83.51288604736328\n",
      "[step: 753] loss: 83.50128173828125\n",
      "[step: 754] loss: 83.48966217041016\n",
      "[step: 755] loss: 83.47805786132812\n",
      "[step: 756] loss: 83.46660614013672\n",
      "[step: 757] loss: 83.45514678955078\n",
      "[step: 758] loss: 83.44371032714844\n",
      "[step: 759] loss: 83.43231964111328\n",
      "[step: 760] loss: 83.42100524902344\n",
      "[step: 761] loss: 83.40972900390625\n",
      "[step: 762] loss: 83.39849090576172\n",
      "[step: 763] loss: 83.38729095458984\n",
      "[step: 764] loss: 83.37611389160156\n",
      "[step: 765] loss: 83.3650131225586\n",
      "[step: 766] loss: 83.35386657714844\n",
      "[step: 767] loss: 83.34280395507812\n",
      "[step: 768] loss: 83.33184051513672\n",
      "[step: 769] loss: 83.32090759277344\n",
      "[step: 770] loss: 83.30999755859375\n",
      "[step: 771] loss: 83.29908752441406\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={\n",
    "                                X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    \n",
    "    test_predict = scaler2.inverse_transform(test_predict)\n",
    "    dataY2 = scaler2.inverse_transform(dataY2) # 데이터 있을때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9856fec8a1fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' predict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataY2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'raw data'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 데이터 있을때\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time Period\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot predictions\n",
    "plt.title(data + ' predict')\n",
    "plt.plot(dataY2, label='raw data') # 데이터 있을때\n",
    "plt.plot(test_predict, label='predict')\n",
    "plt.xlabel(\"Time Period\")\n",
    "# plt.ylabel(\"CAS_SWR\")\n",
    "# plt.ylabel(\"photometric\")\n",
    "plt.ylabel(\"cct\")\n",
    "plt.xticks(np.arange(0, 772, step = 56), ['6', '7', '8', '9', '10', '11', '12', '1', '2', '3', '4', '5', '6', '7']) # v2 모델\n",
    "# plt.yticks(np.arange(0, 50, step = 5))\n",
    "# plt.yticks(np.arange(0, 140000, step=10000))\n",
    "# plt.yticks(np.arange(0, 60000, step=5000))\n",
    "plt.yticks(np.arange(0, 10000, step=2000))\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
