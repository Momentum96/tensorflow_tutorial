{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 그래프 외부에 출력\n",
    "%matplotlib tk\n",
    "\n",
    "tf.set_random_seed(777)  # 어느 컴퓨터에서 이 코드를 실행해도 학습 방향이 같도록, 다시 수행해도 같도록\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3564, 4, 6)\n",
      "(3564, 1)\n"
     ]
    }
   ],
   "source": [
    "# train Parameters\n",
    "# seq_length = 9\n",
    "seq_length = 4\n",
    "data_dim = 6\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 892\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1)) # 데이터 일반화\n",
    "scaler2 = MinMaxScaler(feature_range=(0, 1)) # 데이터 일반화\n",
    "\n",
    "# xy = np.loadtxt('./v3data/train_v3_data_cct.csv', delimiter=',')\n",
    "# cct, cas_swr, 446to477, uvb, ptmt (892행 5열)\n",
    "xy = np.loadtxt('./v6data/train_v6_data.csv', delimiter=',')\n",
    "\n",
    "x = scaler.fit_transform(xy[:, [0, 1, 3, 4, 5, 6]]) # x = 맨 마지막 ptmt 제외 모든 것\n",
    "y = scaler2.fit_transform(xy[:, [2]]) \n",
    "\n",
    "# build a dataset\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "# print(len(x))\n",
    "# print(len(y))\n",
    "# print(x[0])\n",
    "# print(y[0])\n",
    "\n",
    "for i in range(0, len(x) - seq_length): # 한 행씩 dataX, Y에 추가\n",
    "    _x = x[i:i + seq_length]\n",
    "    _y = y[i + seq_length]  # Next close price\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "#     print(np.shape(_x))\n",
    "\n",
    "print(np.shape(dataX))\n",
    "print(np.shape(dataY))\n",
    "\n",
    "date = '2018-04-21'\n",
    "# xy2 = np.loadtxt('./v3data/'+ data +'.csv',delimiter=',')\n",
    "# xy2 = np.loadtxt('./v3data/test_v3_data_cct_180331.csv', delimiter=',')\n",
    "xy2 = np.loadtxt('./v6data/new/20180421.csv', delimiter=',') # 예측할 날짜\n",
    "# xy2 = np.loadtxt('./v6data/test_v6_data_180331.csv', delimiter=',')\n",
    "\n",
    "# x2 = scaler.fit_transform(xy2) # 데이터 없을때 (데이터란 ptmt 값)\n",
    "x2 = scaler.fit_transform(xy2[:, [0, 1, 3, 4, 5, 6]]) # 데이터 있을때\n",
    "y2 = scaler2.fit_transform(xy2[:, [2]]) #데이터 있을때\n",
    "\n",
    "# print(np.shape(x2))\n",
    "\n",
    "dataX2 = []\n",
    "dataY2 = [] #데이터 있을때\n",
    "\n",
    "for i in range(0, len(x2) - seq_length):\n",
    "    _x2 = x2[i:i+seq_length]\n",
    "    _y2 = y2[i+seq_length] #데이터 있을때\n",
    "    dataX2.append(_x2)\n",
    "    dataY2.append(_y2) #데이터 있을때\n",
    "# train/test split\n",
    "\n",
    "train_size = len(dataX)\n",
    "test_size = len(dataX2)\n",
    "\n",
    "# print(train_size)\n",
    "# print(test_size)\n",
    "\n",
    "trainX, testX = np.array(dataX[0:train_size]),np.array(dataX2[0:test_size])\n",
    "trainY = np.array(dataY[0:train_size])\n",
    "\n",
    "# print(np.shape(trainX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gw/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 371.25750732421875\n",
      "[step: 1] loss: 201.5220947265625\n",
      "[step: 2] loss: 97.53817749023438\n",
      "[step: 3] loss: 48.93126678466797\n",
      "[step: 4] loss: 44.872840881347656\n",
      "[step: 5] loss: 63.458282470703125\n",
      "[step: 6] loss: 78.24053192138672\n",
      "[step: 7] loss: 78.75645446777344\n",
      "[step: 8] loss: 67.96855163574219\n",
      "[step: 9] loss: 52.98954772949219\n",
      "[step: 10] loss: 40.23509216308594\n",
      "[step: 11] loss: 33.639503479003906\n",
      "[step: 12] loss: 34.0942268371582\n",
      "[step: 13] loss: 39.33544158935547\n",
      "[step: 14] loss: 45.0288200378418\n",
      "[step: 15] loss: 47.705806732177734\n",
      "[step: 16] loss: 46.504112243652344\n",
      "[step: 17] loss: 42.580596923828125\n",
      "[step: 18] loss: 37.864356994628906\n",
      "[step: 19] loss: 34.080078125\n",
      "[step: 20] loss: 32.19560623168945\n",
      "[step: 21] loss: 32.26582717895508\n",
      "[step: 22] loss: 33.6153450012207\n",
      "[step: 23] loss: 35.24607849121094\n",
      "[step: 24] loss: 36.28779602050781\n",
      "[step: 25] loss: 36.30300521850586\n",
      "[step: 26] loss: 35.3519401550293\n",
      "[step: 27] loss: 33.854400634765625\n",
      "[step: 28] loss: 32.356910705566406\n",
      "[step: 29] loss: 31.311180114746094\n",
      "[step: 30] loss: 30.932165145874023\n",
      "[step: 31] loss: 31.162954330444336\n",
      "[step: 32] loss: 31.739696502685547\n",
      "[step: 33] loss: 32.32167053222656\n",
      "[step: 34] loss: 32.63172912597656\n",
      "[step: 35] loss: 32.55154037475586\n",
      "[step: 36] loss: 32.13924026489258\n",
      "[step: 37] loss: 31.574922561645508\n",
      "[step: 38] loss: 31.068452835083008\n",
      "[step: 39] loss: 30.77238655090332\n",
      "[step: 40] loss: 30.732458114624023\n",
      "[step: 41] loss: 30.887727737426758\n",
      "[step: 42] loss: 31.11214256286621\n",
      "[step: 43] loss: 31.27476692199707\n",
      "[step: 44] loss: 31.292394638061523\n",
      "[step: 45] loss: 31.155014038085938\n",
      "[step: 46] loss: 30.918079376220703\n",
      "[step: 47] loss: 30.66985321044922\n",
      "[step: 48] loss: 30.490131378173828\n",
      "[step: 49] loss: 30.41843605041504\n",
      "[step: 50] loss: 30.443273544311523\n",
      "[step: 51] loss: 30.51449966430664\n",
      "[step: 52] loss: 30.57086181640625\n",
      "[step: 53] loss: 30.56873321533203\n",
      "[step: 54] loss: 30.49909210205078\n",
      "[step: 55] loss: 30.386350631713867\n",
      "[step: 56] loss: 30.27202796936035\n",
      "[step: 57] loss: 30.192827224731445\n",
      "[step: 58] loss: 30.16426658630371\n",
      "[step: 59] loss: 30.17653465270996\n",
      "[step: 60] loss: 30.202943801879883\n",
      "[step: 61] loss: 30.215089797973633\n",
      "[step: 62] loss: 30.196653366088867\n",
      "[step: 63] loss: 30.149301528930664\n",
      "[step: 64] loss: 30.089094161987305\n",
      "[step: 65] loss: 30.03622055053711\n",
      "[step: 66] loss: 30.004226684570312\n",
      "[step: 67] loss: 29.994102478027344\n",
      "[step: 68] loss: 29.99560546875\n",
      "[step: 69] loss: 29.994279861450195\n",
      "[step: 70] loss: 29.97966766357422\n",
      "[step: 71] loss: 29.950286865234375\n",
      "[step: 72] loss: 29.913021087646484\n",
      "[step: 73] loss: 29.878137588500977\n",
      "[step: 74] loss: 29.85316276550293\n",
      "[step: 75] loss: 29.839204788208008\n",
      "[step: 76] loss: 29.831314086914062\n",
      "[step: 77] loss: 29.822240829467773\n",
      "[step: 78] loss: 29.80678939819336\n",
      "[step: 79] loss: 29.784456253051758\n",
      "[step: 80] loss: 29.758913040161133\n",
      "[step: 81] loss: 29.735214233398438\n",
      "[step: 82] loss: 29.716604232788086\n",
      "[step: 83] loss: 29.70282745361328\n",
      "[step: 84] loss: 29.690826416015625\n",
      "[step: 85] loss: 29.67696189880371\n",
      "[step: 86] loss: 29.659259796142578\n",
      "[step: 87] loss: 29.638364791870117\n",
      "[step: 88] loss: 29.616666793823242\n",
      "[step: 89] loss: 29.596574783325195\n",
      "[step: 90] loss: 29.578937530517578\n",
      "[step: 91] loss: 29.562814712524414\n",
      "[step: 92] loss: 29.546321868896484\n",
      "[step: 93] loss: 29.527982711791992\n",
      "[step: 94] loss: 29.507619857788086\n",
      "[step: 95] loss: 29.48624610900879\n",
      "[step: 96] loss: 29.46520233154297\n",
      "[step: 97] loss: 29.44523048400879\n",
      "[step: 98] loss: 29.4260311126709\n",
      "[step: 99] loss: 29.406644821166992\n",
      "[step: 100] loss: 29.38618278503418\n",
      "[step: 101] loss: 29.364410400390625\n",
      "[step: 102] loss: 29.341777801513672\n",
      "[step: 103] loss: 29.31900978088379\n",
      "[step: 104] loss: 29.2965030670166\n",
      "[step: 105] loss: 29.274133682250977\n",
      "[step: 106] loss: 29.25139617919922\n",
      "[step: 107] loss: 29.227821350097656\n",
      "[step: 108] loss: 29.203310012817383\n",
      "[step: 109] loss: 29.178136825561523\n",
      "[step: 110] loss: 29.152664184570312\n",
      "[step: 111] loss: 29.127098083496094\n",
      "[step: 112] loss: 29.101303100585938\n",
      "[step: 113] loss: 29.07499122619629\n",
      "[step: 114] loss: 29.04791831970215\n",
      "[step: 115] loss: 29.020082473754883\n",
      "[step: 116] loss: 28.99165153503418\n",
      "[step: 117] loss: 28.96278953552246\n",
      "[step: 118] loss: 28.933549880981445\n",
      "[step: 119] loss: 28.903785705566406\n",
      "[step: 120] loss: 28.87333869934082\n",
      "[step: 121] loss: 28.84211540222168\n",
      "[step: 122] loss: 28.810157775878906\n",
      "[step: 123] loss: 28.777555465698242\n",
      "[step: 124] loss: 28.74437141418457\n",
      "[step: 125] loss: 28.71057891845703\n",
      "[step: 126] loss: 28.676067352294922\n",
      "[step: 127] loss: 28.640748977661133\n",
      "[step: 128] loss: 28.60460090637207\n",
      "[step: 129] loss: 28.567659378051758\n",
      "[step: 130] loss: 28.52996063232422\n",
      "[step: 131] loss: 28.491477966308594\n",
      "[step: 132] loss: 28.4521484375\n",
      "[step: 133] loss: 28.411890029907227\n",
      "[step: 134] loss: 28.370670318603516\n",
      "[step: 135] loss: 28.328489303588867\n",
      "[step: 136] loss: 28.28536605834961\n",
      "[step: 137] loss: 28.241273880004883\n",
      "[step: 138] loss: 28.196144104003906\n",
      "[step: 139] loss: 28.14993667602539\n",
      "[step: 140] loss: 28.10259246826172\n",
      "[step: 141] loss: 28.054096221923828\n",
      "[step: 142] loss: 28.004430770874023\n",
      "[step: 143] loss: 27.953554153442383\n",
      "[step: 144] loss: 27.901399612426758\n",
      "[step: 145] loss: 27.84790802001953\n",
      "[step: 146] loss: 27.79302406311035\n",
      "[step: 147] loss: 27.736692428588867\n",
      "[step: 148] loss: 27.678884506225586\n",
      "[step: 149] loss: 27.619522094726562\n",
      "[step: 150] loss: 27.558544158935547\n",
      "[step: 151] loss: 27.495845794677734\n",
      "[step: 152] loss: 27.431360244750977\n",
      "[step: 153] loss: 27.365022659301758\n",
      "[step: 154] loss: 27.296722412109375\n",
      "[step: 155] loss: 27.226364135742188\n",
      "[step: 156] loss: 27.153837203979492\n",
      "[step: 157] loss: 27.079017639160156\n",
      "[step: 158] loss: 27.00178337097168\n",
      "[step: 159] loss: 26.921995162963867\n",
      "[step: 160] loss: 26.839523315429688\n",
      "[step: 161] loss: 26.754180908203125\n",
      "[step: 162] loss: 26.66580581665039\n",
      "[step: 163] loss: 26.574222564697266\n",
      "[step: 164] loss: 26.47922706604004\n",
      "[step: 165] loss: 26.380603790283203\n",
      "[step: 166] loss: 26.27812385559082\n",
      "[step: 167] loss: 26.17155647277832\n",
      "[step: 168] loss: 26.060640335083008\n",
      "[step: 169] loss: 25.94512176513672\n",
      "[step: 170] loss: 25.82473373413086\n",
      "[step: 171] loss: 25.699199676513672\n",
      "[step: 172] loss: 25.568233489990234\n",
      "[step: 173] loss: 25.43157958984375\n",
      "[step: 174] loss: 25.288972854614258\n",
      "[step: 175] loss: 25.14016342163086\n",
      "[step: 176] loss: 24.984949111938477\n",
      "[step: 177] loss: 24.823163986206055\n",
      "[step: 178] loss: 24.6546573638916\n",
      "[step: 179] loss: 24.479366302490234\n",
      "[step: 180] loss: 24.297237396240234\n",
      "[step: 181] loss: 24.108274459838867\n",
      "[step: 182] loss: 23.912517547607422\n",
      "[step: 183] loss: 23.71004867553711\n",
      "[step: 184] loss: 23.5009708404541\n",
      "[step: 185] loss: 23.285465240478516\n",
      "[step: 186] loss: 23.063806533813477\n",
      "[step: 187] loss: 22.83649444580078\n",
      "[step: 188] loss: 22.604387283325195\n",
      "[step: 189] loss: 22.36874771118164\n",
      "[step: 190] loss: 22.131351470947266\n",
      "[step: 191] loss: 21.894329071044922\n",
      "[step: 192] loss: 21.660001754760742\n",
      "[step: 193] loss: 21.43057632446289\n",
      "[step: 194] loss: 21.20779037475586\n",
      "[step: 195] loss: 20.99301528930664\n",
      "[step: 196] loss: 20.787353515625\n",
      "[step: 197] loss: 20.591840744018555\n",
      "[step: 198] loss: 20.407438278198242\n",
      "[step: 199] loss: 20.234350204467773\n",
      "[step: 200] loss: 20.07170295715332\n",
      "[step: 201] loss: 19.918102264404297\n",
      "[step: 202] loss: 19.779199600219727\n",
      "[step: 203] loss: 19.77776527404785\n",
      "[step: 204] loss: 21.629011154174805\n",
      "[step: 205] loss: 24.564390182495117\n",
      "[step: 206] loss: 19.351158142089844\n",
      "[step: 207] loss: 23.65925407409668\n",
      "[step: 208] loss: 19.239421844482422\n",
      "[step: 209] loss: 22.82074737548828\n",
      "[step: 210] loss: 19.20159339904785\n",
      "[step: 211] loss: 21.930593490600586\n",
      "[step: 212] loss: 19.58445930480957\n",
      "[step: 213] loss: 20.652008056640625\n",
      "[step: 214] loss: 20.22923469543457\n",
      "[step: 215] loss: 19.374290466308594\n",
      "[step: 216] loss: 20.523900985717773\n",
      "[step: 217] loss: 18.86124038696289\n",
      "[step: 218] loss: 20.129758834838867\n",
      "[step: 219] loss: 19.113479614257812\n",
      "[step: 220] loss: 19.30421257019043\n",
      "[step: 221] loss: 19.469562530517578\n",
      "[step: 222] loss: 18.697616577148438\n",
      "[step: 223] loss: 19.44203758239746\n",
      "[step: 224] loss: 18.585180282592773\n",
      "[step: 225] loss: 19.063146591186523\n",
      "[step: 226] loss: 18.71902847290039\n",
      "[step: 227] loss: 18.629642486572266\n",
      "[step: 228] loss: 18.78976821899414\n",
      "[step: 229] loss: 18.346776962280273\n",
      "[step: 230] loss: 18.709346771240234\n",
      "[step: 231] loss: 18.21198272705078\n",
      "[step: 232] loss: 18.537097930908203\n",
      "[step: 233] loss: 18.141204833984375\n",
      "[step: 234] loss: 18.347858428955078\n",
      "[step: 235] loss: 18.068561553955078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 236] loss: 18.182344436645508\n",
      "[step: 237] loss: 17.97770881652832\n",
      "[step: 238] loss: 18.046297073364258\n",
      "[step: 239] loss: 17.86602020263672\n",
      "[step: 240] loss: 17.937345504760742\n",
      "[step: 241] loss: 17.748851776123047\n",
      "[step: 242] loss: 17.842609405517578\n",
      "[step: 243] loss: 17.63726806640625\n",
      "[step: 244] loss: 17.74315643310547\n",
      "[step: 245] loss: 17.554271697998047\n",
      "[step: 246] loss: 17.61712646484375\n",
      "[step: 247] loss: 17.514455795288086\n",
      "[step: 248] loss: 17.464990615844727\n",
      "[step: 249] loss: 17.486448287963867\n",
      "[step: 250] loss: 17.35027503967285\n",
      "[step: 251] loss: 17.3880558013916\n",
      "[step: 252] loss: 17.32945442199707\n",
      "[step: 253] loss: 17.24526596069336\n",
      "[step: 254] loss: 17.278392791748047\n",
      "[step: 255] loss: 17.215959548950195\n",
      "[step: 256] loss: 17.144500732421875\n",
      "[step: 257] loss: 17.161949157714844\n",
      "[step: 258] loss: 17.13714027404785\n",
      "[step: 259] loss: 17.063053131103516\n",
      "[step: 260] loss: 17.035560607910156\n",
      "[step: 261] loss: 17.045686721801758\n",
      "[step: 262] loss: 17.028274536132812\n",
      "[step: 263] loss: 16.974090576171875\n",
      "[step: 264] loss: 16.926246643066406\n",
      "[step: 265] loss: 16.90558624267578\n",
      "[step: 266] loss: 16.90480613708496\n",
      "[step: 267] loss: 16.911766052246094\n",
      "[step: 268] loss: 16.918701171875\n",
      "[step: 269] loss: 16.938112258911133\n",
      "[step: 270] loss: 16.97333526611328\n",
      "[step: 271] loss: 17.07269859313965\n",
      "[step: 272] loss: 17.23587989807129\n",
      "[step: 273] loss: 17.58762550354004\n",
      "[step: 274] loss: 17.81952667236328\n",
      "[step: 275] loss: 17.97098731994629\n",
      "[step: 276] loss: 17.29712677001953\n",
      "[step: 277] loss: 16.757169723510742\n",
      "[step: 278] loss: 16.747278213500977\n",
      "[step: 279] loss: 17.12079429626465\n",
      "[step: 280] loss: 17.377328872680664\n",
      "[step: 281] loss: 16.98926544189453\n",
      "[step: 282] loss: 16.649887084960938\n",
      "[step: 283] loss: 16.723134994506836\n",
      "[step: 284] loss: 16.968069076538086\n",
      "[step: 285] loss: 17.002487182617188\n",
      "[step: 286] loss: 16.70545768737793\n",
      "[step: 287] loss: 16.57626724243164\n",
      "[step: 288] loss: 16.714496612548828\n",
      "[step: 289] loss: 16.82657814025879\n",
      "[step: 290] loss: 16.760726928710938\n",
      "[step: 291] loss: 16.573421478271484\n",
      "[step: 292] loss: 16.52730369567871\n",
      "[step: 293] loss: 16.62534523010254\n",
      "[step: 294] loss: 16.694164276123047\n",
      "[step: 295] loss: 16.66473960876465\n",
      "[step: 296] loss: 16.540674209594727\n",
      "[step: 297] loss: 16.465097427368164\n",
      "[step: 298] loss: 16.4778995513916\n",
      "[step: 299] loss: 16.537508010864258\n",
      "[step: 300] loss: 16.589683532714844\n",
      "[step: 301] loss: 16.57248878479004\n",
      "[step: 302] loss: 16.52556610107422\n",
      "[step: 303] loss: 16.453651428222656\n",
      "[step: 304] loss: 16.40268325805664\n",
      "[step: 305] loss: 16.380666732788086\n",
      "[step: 306] loss: 16.384889602661133\n",
      "[step: 307] loss: 16.407550811767578\n",
      "[step: 308] loss: 16.440507888793945\n",
      "[step: 309] loss: 16.49601173400879\n",
      "[step: 310] loss: 16.553565979003906\n",
      "[step: 311] loss: 16.66741180419922\n",
      "[step: 312] loss: 16.747745513916016\n",
      "[step: 313] loss: 16.915151596069336\n",
      "[step: 314] loss: 16.883140563964844\n",
      "[step: 315] loss: 16.865224838256836\n",
      "[step: 316] loss: 16.59383201599121\n",
      "[step: 317] loss: 16.38239860534668\n",
      "[step: 318] loss: 16.270797729492188\n",
      "[step: 319] loss: 16.304868698120117\n",
      "[step: 320] loss: 16.426952362060547\n",
      "[step: 321] loss: 16.509105682373047\n",
      "[step: 322] loss: 16.543060302734375\n",
      "[step: 323] loss: 16.420698165893555\n",
      "[step: 324] loss: 16.29899024963379\n",
      "[step: 325] loss: 16.22130012512207\n",
      "[step: 326] loss: 16.226362228393555\n",
      "[step: 327] loss: 16.28663444519043\n",
      "[step: 328] loss: 16.343046188354492\n",
      "[step: 329] loss: 16.384740829467773\n",
      "[step: 330] loss: 16.345653533935547\n",
      "[step: 331] loss: 16.293479919433594\n",
      "[step: 332] loss: 16.22052574157715\n",
      "[step: 333] loss: 16.172212600708008\n",
      "[step: 334] loss: 16.149932861328125\n",
      "[step: 335] loss: 16.151981353759766\n",
      "[step: 336] loss: 16.171602249145508\n",
      "[step: 337] loss: 16.198904037475586\n",
      "[step: 338] loss: 16.24154281616211\n",
      "[step: 339] loss: 16.27981185913086\n",
      "[step: 340] loss: 16.356830596923828\n",
      "[step: 341] loss: 16.40512466430664\n",
      "[step: 342] loss: 16.52339744567871\n",
      "[step: 343] loss: 16.5247745513916\n",
      "[step: 344] loss: 16.584558486938477\n",
      "[step: 345] loss: 16.44148826599121\n",
      "[step: 346] loss: 16.331981658935547\n",
      "[step: 347] loss: 16.168012619018555\n",
      "[step: 348] loss: 16.07489776611328\n",
      "[step: 349] loss: 16.05409812927246\n",
      "[step: 350] loss: 16.0933780670166\n",
      "[step: 351] loss: 16.1665096282959\n",
      "[step: 352] loss: 16.21576690673828\n",
      "[step: 353] loss: 16.264081954956055\n",
      "[step: 354] loss: 16.225313186645508\n",
      "[step: 355] loss: 16.187992095947266\n",
      "[step: 356] loss: 16.107303619384766\n",
      "[step: 357] loss: 16.04850196838379\n",
      "[step: 358] loss: 16.006772994995117\n",
      "[step: 359] loss: 15.990042686462402\n",
      "[step: 360] loss: 15.993111610412598\n",
      "[step: 361] loss: 16.009870529174805\n",
      "[step: 362] loss: 16.040203094482422\n",
      "[step: 363] loss: 16.07476043701172\n",
      "[step: 364] loss: 16.139657974243164\n",
      "[step: 365] loss: 16.195276260375977\n",
      "[step: 366] loss: 16.324054718017578\n",
      "[step: 367] loss: 16.375476837158203\n",
      "[step: 368] loss: 16.52996063232422\n",
      "[step: 369] loss: 16.43378257751465\n",
      "[step: 370] loss: 16.394899368286133\n",
      "[step: 371] loss: 16.156829833984375\n",
      "[step: 372] loss: 15.998129844665527\n",
      "[step: 373] loss: 15.916561126708984\n",
      "[step: 374] loss: 15.93803882598877\n",
      "[step: 375] loss: 16.02618408203125\n",
      "[step: 376] loss: 16.096529006958008\n",
      "[step: 377] loss: 16.155746459960938\n",
      "[step: 378] loss: 16.0891170501709\n",
      "[step: 379] loss: 16.019350051879883\n",
      "[step: 380] loss: 15.927355766296387\n",
      "[step: 381] loss: 15.877695083618164\n",
      "[step: 382] loss: 15.870809555053711\n",
      "[step: 383] loss: 15.896597862243652\n",
      "[step: 384] loss: 15.942696571350098\n",
      "[step: 385] loss: 15.979721069335938\n",
      "[step: 386] loss: 16.03022575378418\n",
      "[step: 387] loss: 16.03488540649414\n",
      "[step: 388] loss: 16.062740325927734\n",
      "[step: 389] loss: 16.028396606445312\n",
      "[step: 390] loss: 16.019474029541016\n",
      "[step: 391] loss: 15.963196754455566\n",
      "[step: 392] loss: 15.92902660369873\n",
      "[step: 393] loss: 15.88100528717041\n",
      "[step: 394] loss: 15.849678039550781\n",
      "[step: 395] loss: 15.822571754455566\n",
      "[step: 396] loss: 15.804980278015137\n",
      "[step: 397] loss: 15.792726516723633\n",
      "[step: 398] loss: 15.784623146057129\n",
      "[step: 399] loss: 15.779197692871094\n",
      "[step: 400] loss: 15.775789260864258\n",
      "[step: 401] loss: 15.774740219116211\n",
      "[step: 402] loss: 15.777416229248047\n",
      "[step: 403] loss: 15.789346694946289\n",
      "[step: 404] loss: 15.817801475524902\n",
      "[step: 405] loss: 15.899335861206055\n",
      "[step: 406] loss: 16.05571174621582\n",
      "[step: 407] loss: 16.51645278930664\n",
      "[step: 408] loss: 16.967370986938477\n",
      "[step: 409] loss: 18.196985244750977\n",
      "[step: 410] loss: 16.896512985229492\n",
      "[step: 411] loss: 16.089710235595703\n",
      "[step: 412] loss: 15.746530532836914\n",
      "[step: 413] loss: 16.222259521484375\n",
      "[step: 414] loss: 16.870330810546875\n",
      "[step: 415] loss: 16.130693435668945\n",
      "[step: 416] loss: 15.754178047180176\n",
      "[step: 417] loss: 16.044565200805664\n",
      "[step: 418] loss: 16.181432723999023\n",
      "[step: 419] loss: 15.99304485321045\n",
      "[step: 420] loss: 15.718974113464355\n",
      "[step: 421] loss: 15.895078659057617\n",
      "[step: 422] loss: 16.118694305419922\n",
      "[step: 423] loss: 15.837302207946777\n",
      "[step: 424] loss: 15.700512886047363\n",
      "[step: 425] loss: 15.860400199890137\n",
      "[step: 426] loss: 15.924492835998535\n",
      "[step: 427] loss: 15.821818351745605\n",
      "[step: 428] loss: 15.68466854095459\n",
      "[step: 429] loss: 15.73933219909668\n",
      "[step: 430] loss: 15.857952117919922\n",
      "[step: 431] loss: 15.792072296142578\n",
      "[step: 432] loss: 15.681769371032715\n",
      "[step: 433] loss: 15.66187858581543\n",
      "[step: 434] loss: 15.728175163269043\n",
      "[step: 435] loss: 15.765710830688477\n",
      "[step: 436] loss: 15.69896125793457\n",
      "[step: 437] loss: 15.639966011047363\n",
      "[step: 438] loss: 15.643448829650879\n",
      "[step: 439] loss: 15.683074951171875\n",
      "[step: 440] loss: 15.701896667480469\n",
      "[step: 441] loss: 15.66276741027832\n",
      "[step: 442] loss: 15.621923446655273\n",
      "[step: 443] loss: 15.607733726501465\n",
      "[step: 444] loss: 15.622333526611328\n",
      "[step: 445] loss: 15.644887924194336\n",
      "[step: 446] loss: 15.645496368408203\n",
      "[step: 447] loss: 15.631572723388672\n",
      "[step: 448] loss: 15.604804992675781\n",
      "[step: 449] loss: 15.58436393737793\n",
      "[step: 450] loss: 15.574788093566895\n",
      "[step: 451] loss: 15.575960159301758\n",
      "[step: 452] loss: 15.582843780517578\n",
      "[step: 453] loss: 15.588748931884766\n",
      "[step: 454] loss: 15.594200134277344\n",
      "[step: 455] loss: 15.59205436706543\n",
      "[step: 456] loss: 15.590890884399414\n",
      "[step: 457] loss: 15.583253860473633\n",
      "[step: 458] loss: 15.579144477844238\n",
      "[step: 459] loss: 15.571903228759766\n",
      "[step: 460] loss: 15.570798873901367\n",
      "[step: 461] loss: 15.569096565246582\n",
      "[step: 462] loss: 15.577940940856934\n",
      "[step: 463] loss: 15.589102745056152\n",
      "[step: 464] loss: 15.626953125\n",
      "[step: 465] loss: 15.674068450927734\n",
      "[step: 466] loss: 15.81058406829834\n",
      "[step: 467] loss: 15.939136505126953\n",
      "[step: 468] loss: 16.330076217651367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 469] loss: 16.35099220275879\n",
      "[step: 470] loss: 16.69294548034668\n",
      "[step: 471] loss: 16.070232391357422\n",
      "[step: 472] loss: 15.707510948181152\n",
      "[step: 473] loss: 15.497954368591309\n",
      "[step: 474] loss: 15.606108665466309\n",
      "[step: 475] loss: 15.890439987182617\n",
      "[step: 476] loss: 15.89218521118164\n",
      "[step: 477] loss: 15.811735153198242\n",
      "[step: 478] loss: 15.546810150146484\n",
      "[step: 479] loss: 15.480217933654785\n",
      "[step: 480] loss: 15.601374626159668\n",
      "[step: 481] loss: 15.717720031738281\n",
      "[step: 482] loss: 15.798253059387207\n",
      "[step: 483] loss: 15.632641792297363\n",
      "[step: 484] loss: 15.49848461151123\n",
      "[step: 485] loss: 15.451786994934082\n",
      "[step: 486] loss: 15.50240421295166\n",
      "[step: 487] loss: 15.602952003479004\n",
      "[step: 488] loss: 15.640477180480957\n",
      "[step: 489] loss: 15.651548385620117\n",
      "[step: 490] loss: 15.548269271850586\n",
      "[step: 491] loss: 15.473236083984375\n",
      "[step: 492] loss: 15.427308082580566\n",
      "[step: 493] loss: 15.429673194885254\n",
      "[step: 494] loss: 15.466784477233887\n",
      "[step: 495] loss: 15.50152587890625\n",
      "[step: 496] loss: 15.53971004486084\n",
      "[step: 497] loss: 15.533415794372559\n",
      "[step: 498] loss: 15.535487174987793\n",
      "[step: 499] loss: 15.49932861328125\n",
      "[step: 500] loss: 15.475738525390625\n",
      "[step: 501] loss: 15.440154075622559\n",
      "[step: 502] loss: 15.418272972106934\n",
      "[step: 503] loss: 15.399431228637695\n",
      "[step: 504] loss: 15.387763023376465\n",
      "[step: 505] loss: 15.379802703857422\n",
      "[step: 506] loss: 15.37369155883789\n",
      "[step: 507] loss: 15.36927604675293\n",
      "[step: 508] loss: 15.365534782409668\n",
      "[step: 509] loss: 15.362253189086914\n",
      "[step: 510] loss: 15.360188484191895\n",
      "[step: 511] loss: 15.359296798706055\n",
      "[step: 512] loss: 15.36125659942627\n",
      "[step: 513] loss: 15.371261596679688\n",
      "[step: 514] loss: 15.39579963684082\n",
      "[step: 515] loss: 15.469510078430176\n",
      "[step: 516] loss: 15.610776901245117\n",
      "[step: 517] loss: 16.058645248413086\n",
      "[step: 518] loss: 16.47953224182129\n",
      "[step: 519] loss: 17.930728912353516\n",
      "[step: 520] loss: 16.411609649658203\n",
      "[step: 521] loss: 15.667518615722656\n",
      "[step: 522] loss: 15.37250804901123\n",
      "[step: 523] loss: 15.82687759399414\n",
      "[step: 524] loss: 16.541757583618164\n",
      "[step: 525] loss: 15.777661323547363\n",
      "[step: 526] loss: 15.387494087219238\n",
      "[step: 527] loss: 15.619290351867676\n",
      "[step: 528] loss: 15.868706703186035\n",
      "[step: 529] loss: 15.886760711669922\n",
      "[step: 530] loss: 15.425768852233887\n",
      "[step: 531] loss: 15.415522575378418\n",
      "[step: 532] loss: 15.719677925109863\n",
      "[step: 533] loss: 15.690362930297852\n",
      "[step: 534] loss: 15.52669906616211\n",
      "[step: 535] loss: 15.326522827148438\n",
      "[step: 536] loss: 15.377396583557129\n",
      "[step: 537] loss: 15.539726257324219\n",
      "[step: 538] loss: 15.52041244506836\n",
      "[step: 539] loss: 15.425004005432129\n",
      "[step: 540] loss: 15.303466796875\n",
      "[step: 541] loss: 15.318466186523438\n",
      "[step: 542] loss: 15.404868125915527\n",
      "[step: 543] loss: 15.41511344909668\n",
      "[step: 544] loss: 15.37431526184082\n",
      "[step: 545] loss: 15.295961380004883\n",
      "[step: 546] loss: 15.276789665222168\n",
      "[step: 547] loss: 15.309453010559082\n",
      "[step: 548] loss: 15.343879699707031\n",
      "[step: 549] loss: 15.363333702087402\n",
      "[step: 550] loss: 15.327741622924805\n",
      "[step: 551] loss: 15.291457176208496\n",
      "[step: 552] loss: 15.25986099243164\n",
      "[step: 553] loss: 15.251401901245117\n",
      "[step: 554] loss: 15.264354705810547\n",
      "[step: 555] loss: 15.28309154510498\n",
      "[step: 556] loss: 15.305146217346191\n",
      "[step: 557] loss: 15.307236671447754\n",
      "[step: 558] loss: 15.307007789611816\n",
      "[step: 559] loss: 15.286722183227539\n",
      "[step: 560] loss: 15.271071434020996\n",
      "[step: 561] loss: 15.249892234802246\n",
      "[step: 562] loss: 15.23496150970459\n",
      "[step: 563] loss: 15.222671508789062\n",
      "[step: 564] loss: 15.215559959411621\n",
      "[step: 565] loss: 15.211980819702148\n",
      "[step: 566] loss: 15.210646629333496\n",
      "[step: 567] loss: 15.212209701538086\n",
      "[step: 568] loss: 15.216285705566406\n",
      "[step: 569] loss: 15.227302551269531\n",
      "[step: 570] loss: 15.246891021728516\n",
      "[step: 571] loss: 15.297863006591797\n",
      "[step: 572] loss: 15.375521659851074\n",
      "[step: 573] loss: 15.601059913635254\n",
      "[step: 574] loss: 15.805376052856445\n",
      "[step: 575] loss: 16.51601219177246\n",
      "[step: 576] loss: 16.211647033691406\n",
      "[step: 577] loss: 16.345129013061523\n",
      "[step: 578] loss: 15.46313762664795\n",
      "[step: 579] loss: 15.21247673034668\n",
      "[step: 580] loss: 15.495352745056152\n",
      "[step: 581] loss: 15.730363845825195\n",
      "[step: 582] loss: 15.89801025390625\n",
      "[step: 583] loss: 15.353084564208984\n",
      "[step: 584] loss: 15.208311080932617\n",
      "[step: 585] loss: 15.496763229370117\n",
      "[step: 586] loss: 15.64712905883789\n",
      "[step: 587] loss: 15.740910530090332\n",
      "[step: 588] loss: 15.334156036376953\n",
      "[step: 589] loss: 15.17374324798584\n",
      "[step: 590] loss: 15.346870422363281\n",
      "[step: 591] loss: 15.536539077758789\n",
      "[step: 592] loss: 15.728349685668945\n",
      "[step: 593] loss: 15.414947509765625\n",
      "[step: 594] loss: 15.183026313781738\n",
      "[step: 595] loss: 15.196541786193848\n",
      "[step: 596] loss: 15.357146263122559\n",
      "[step: 597] loss: 15.52491283416748\n",
      "[step: 598] loss: 15.372506141662598\n",
      "[step: 599] loss: 15.196157455444336\n",
      "[step: 600] loss: 15.157132148742676\n",
      "[step: 601] loss: 15.266054153442383\n",
      "[step: 602] loss: 15.403369903564453\n",
      "[step: 603] loss: 15.36609935760498\n",
      "[step: 604] loss: 15.26916790008545\n",
      "[step: 605] loss: 15.149614334106445\n",
      "[step: 606] loss: 15.147250175476074\n",
      "[step: 607] loss: 15.205883979797363\n",
      "[step: 608] loss: 15.277405738830566\n",
      "[step: 609] loss: 15.326370239257812\n",
      "[step: 610] loss: 15.24084758758545\n",
      "[step: 611] loss: 15.18242359161377\n",
      "[step: 612] loss: 15.124720573425293\n",
      "[step: 613] loss: 15.113012313842773\n",
      "[step: 614] loss: 15.135140419006348\n",
      "[step: 615] loss: 15.159316062927246\n",
      "[step: 616] loss: 15.192021369934082\n",
      "[step: 617] loss: 15.187652587890625\n",
      "[step: 618] loss: 15.187032699584961\n",
      "[step: 619] loss: 15.156688690185547\n",
      "[step: 620] loss: 15.129252433776855\n",
      "[step: 621] loss: 15.107168197631836\n",
      "[step: 622] loss: 15.09232234954834\n",
      "[step: 623] loss: 15.084854125976562\n",
      "[step: 624] loss: 15.084282875061035\n",
      "[step: 625] loss: 15.085009574890137\n",
      "[step: 626] loss: 15.091870307922363\n",
      "[step: 627] loss: 15.104162216186523\n",
      "[step: 628] loss: 15.123005867004395\n",
      "[step: 629] loss: 15.167799949645996\n",
      "[step: 630] loss: 15.217109680175781\n",
      "[step: 631] loss: 15.359636306762695\n",
      "[step: 632] loss: 15.46799373626709\n",
      "[step: 633] loss: 15.852359771728516\n",
      "[step: 634] loss: 15.754016876220703\n",
      "[step: 635] loss: 15.95320987701416\n",
      "[step: 636] loss: 15.421305656433105\n",
      "[step: 637] loss: 15.163004875183105\n",
      "[step: 638] loss: 15.073731422424316\n",
      "[step: 639] loss: 15.183201789855957\n",
      "[step: 640] loss: 15.412009239196777\n",
      "[step: 641] loss: 15.403255462646484\n",
      "[step: 642] loss: 15.4124755859375\n",
      "[step: 643] loss: 15.200190544128418\n",
      "[step: 644] loss: 15.08603572845459\n",
      "[step: 645] loss: 15.047242164611816\n",
      "[step: 646] loss: 15.08449649810791\n",
      "[step: 647] loss: 15.177634239196777\n",
      "[step: 648] loss: 15.250894546508789\n",
      "[step: 649] loss: 15.393065452575684\n",
      "[step: 650] loss: 15.361690521240234\n",
      "[step: 651] loss: 15.422165870666504\n",
      "[step: 652] loss: 15.259363174438477\n",
      "[step: 653] loss: 15.167132377624512\n",
      "[step: 654] loss: 15.063538551330566\n",
      "[step: 655] loss: 15.02988338470459\n",
      "[step: 656] loss: 15.055571556091309\n",
      "[step: 657] loss: 15.108585357666016\n",
      "[step: 658] loss: 15.184203147888184\n",
      "[step: 659] loss: 15.195452690124512\n",
      "[step: 660] loss: 15.24319839477539\n",
      "[step: 661] loss: 15.19647216796875\n",
      "[step: 662] loss: 15.198107719421387\n",
      "[step: 663] loss: 15.136839866638184\n",
      "[step: 664] loss: 15.112220764160156\n",
      "[step: 665] loss: 15.069286346435547\n",
      "[step: 666] loss: 15.047865867614746\n",
      "[step: 667] loss: 15.026957511901855\n",
      "[step: 668] loss: 15.014991760253906\n",
      "[step: 669] loss: 15.00527572631836\n",
      "[step: 670] loss: 14.998995780944824\n",
      "[step: 671] loss: 14.994529724121094\n",
      "[step: 672] loss: 14.991259574890137\n",
      "[step: 673] loss: 14.988323211669922\n",
      "[step: 674] loss: 14.985580444335938\n",
      "[step: 675] loss: 14.983179092407227\n",
      "[step: 676] loss: 14.981099128723145\n",
      "[step: 677] loss: 14.97902774810791\n",
      "[step: 678] loss: 14.97676944732666\n",
      "[step: 679] loss: 14.974449157714844\n",
      "[step: 680] loss: 14.972220420837402\n",
      "[step: 681] loss: 14.970051765441895\n",
      "[step: 682] loss: 14.967803955078125\n",
      "[step: 683] loss: 14.96549129486084\n",
      "[step: 684] loss: 14.963269233703613\n",
      "[step: 685] loss: 14.961203575134277\n",
      "[step: 686] loss: 14.959284782409668\n",
      "[step: 687] loss: 14.957670211791992\n",
      "[step: 688] loss: 14.957192420959473\n",
      "[step: 689] loss: 14.960190773010254\n",
      "[step: 690] loss: 14.975044250488281\n",
      "[step: 691] loss: 15.022696495056152\n",
      "[step: 692] loss: 15.205583572387695\n",
      "[step: 693] loss: 15.629741668701172\n",
      "[step: 694] loss: 17.38955307006836\n",
      "[step: 695] loss: 16.772130966186523\n",
      "[step: 696] loss: 17.559892654418945\n",
      "[step: 697] loss: 15.113192558288574\n",
      "[step: 698] loss: 16.702239990234375\n",
      "[step: 699] loss: 20.116254806518555\n",
      "[step: 700] loss: 16.291006088256836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 701] loss: 23.04505157470703\n",
      "[step: 702] loss: 21.90999984741211\n",
      "[step: 703] loss: 24.107955932617188\n",
      "[step: 704] loss: 20.1329402923584\n",
      "[step: 705] loss: 19.7996768951416\n",
      "[step: 706] loss: 20.002473831176758\n",
      "[step: 707] loss: 21.34316635131836\n",
      "[step: 708] loss: 19.286113739013672\n",
      "[step: 709] loss: 18.196996688842773\n",
      "[step: 710] loss: 20.3976993560791\n",
      "[step: 711] loss: 16.97800064086914\n",
      "[step: 712] loss: 17.21344566345215\n",
      "[step: 713] loss: 18.153051376342773\n",
      "[step: 714] loss: 16.412551879882812\n",
      "[step: 715] loss: 16.15606117248535\n",
      "[step: 716] loss: 17.4416446685791\n",
      "[step: 717] loss: 16.088537216186523\n",
      "[step: 718] loss: 15.910279273986816\n",
      "[step: 719] loss: 16.67540168762207\n",
      "[step: 720] loss: 16.014211654663086\n",
      "[step: 721] loss: 15.571945190429688\n",
      "[step: 722] loss: 15.859790802001953\n",
      "[step: 723] loss: 16.001798629760742\n",
      "[step: 724] loss: 15.182401657104492\n",
      "[step: 725] loss: 15.745439529418945\n",
      "[step: 726] loss: 15.769617080688477\n",
      "[step: 727] loss: 15.421369552612305\n",
      "[step: 728] loss: 15.415053367614746\n",
      "[step: 729] loss: 15.608502388000488\n",
      "[step: 730] loss: 15.452740669250488\n",
      "[step: 731] loss: 15.181023597717285\n",
      "[step: 732] loss: 15.45858097076416\n",
      "[step: 733] loss: 15.268293380737305\n",
      "[step: 734] loss: 15.18342113494873\n",
      "[step: 735] loss: 15.14515495300293\n",
      "[step: 736] loss: 15.274764060974121\n",
      "[step: 737] loss: 15.143901824951172\n",
      "[step: 738] loss: 15.094003677368164\n",
      "[step: 739] loss: 15.24927043914795\n",
      "[step: 740] loss: 15.134056091308594\n",
      "[step: 741] loss: 15.11080551147461\n",
      "[step: 742] loss: 15.125645637512207\n",
      "[step: 743] loss: 15.1338472366333\n",
      "[step: 744] loss: 15.080748558044434\n",
      "[step: 745] loss: 15.03906536102295\n",
      "[step: 746] loss: 15.10618782043457\n",
      "[step: 747] loss: 15.037161827087402\n",
      "[step: 748] loss: 15.019415855407715\n",
      "[step: 749] loss: 15.04214096069336\n",
      "[step: 750] loss: 15.027872085571289\n",
      "[step: 751] loss: 15.01191234588623\n",
      "[step: 752] loss: 14.995489120483398\n",
      "[step: 753] loss: 15.023452758789062\n",
      "[step: 754] loss: 14.990716934204102\n",
      "[step: 755] loss: 14.978331565856934\n",
      "[step: 756] loss: 14.995226860046387\n",
      "[step: 757] loss: 14.975748062133789\n",
      "[step: 758] loss: 14.970333099365234\n",
      "[step: 759] loss: 14.966611862182617\n",
      "[step: 760] loss: 14.969592094421387\n",
      "[step: 761] loss: 14.961525917053223\n",
      "[step: 762] loss: 14.950439453125\n",
      "[step: 763] loss: 14.9602632522583\n",
      "[step: 764] loss: 14.949418067932129\n",
      "[step: 765] loss: 14.939188957214355\n",
      "[step: 766] loss: 14.944705963134766\n",
      "[step: 767] loss: 14.93631649017334\n",
      "[step: 768] loss: 14.929788589477539\n",
      "[step: 769] loss: 14.929200172424316\n",
      "[step: 770] loss: 14.925357818603516\n",
      "[step: 771] loss: 14.92236042022705\n",
      "[step: 772] loss: 14.917497634887695\n",
      "[step: 773] loss: 14.916434288024902\n",
      "[step: 774] loss: 14.914775848388672\n",
      "[step: 775] loss: 14.90816879272461\n",
      "[step: 776] loss: 14.907186508178711\n",
      "[step: 777] loss: 14.905546188354492\n",
      "[step: 778] loss: 14.89928913116455\n",
      "[step: 779] loss: 14.897675514221191\n",
      "[step: 780] loss: 14.895966529846191\n",
      "[step: 781] loss: 14.89101505279541\n",
      "[step: 782] loss: 14.888792991638184\n",
      "[step: 783] loss: 14.8870267868042\n",
      "[step: 784] loss: 14.883294105529785\n",
      "[step: 785] loss: 14.880743026733398\n",
      "[step: 786] loss: 14.878658294677734\n",
      "[step: 787] loss: 14.875563621520996\n",
      "[step: 788] loss: 14.8728666305542\n",
      "[step: 789] loss: 14.870552062988281\n",
      "[step: 790] loss: 14.867813110351562\n",
      "[step: 791] loss: 14.865188598632812\n",
      "[step: 792] loss: 14.862792015075684\n",
      "[step: 793] loss: 14.860248565673828\n",
      "[step: 794] loss: 14.857725143432617\n",
      "[step: 795] loss: 14.855334281921387\n",
      "[step: 796] loss: 14.852869033813477\n",
      "[step: 797] loss: 14.850366592407227\n",
      "[step: 798] loss: 14.84796142578125\n",
      "[step: 799] loss: 14.84556770324707\n",
      "[step: 800] loss: 14.843092918395996\n",
      "[step: 801] loss: 14.840690612792969\n",
      "[step: 802] loss: 14.838400840759277\n",
      "[step: 803] loss: 14.836026191711426\n",
      "[step: 804] loss: 14.8336181640625\n",
      "[step: 805] loss: 14.831387519836426\n",
      "[step: 806] loss: 14.829141616821289\n",
      "[step: 807] loss: 14.826743125915527\n",
      "[step: 808] loss: 14.824459075927734\n",
      "[step: 809] loss: 14.822297096252441\n",
      "[step: 810] loss: 14.819982528686523\n",
      "[step: 811] loss: 14.817642211914062\n",
      "[step: 812] loss: 14.815476417541504\n",
      "[step: 813] loss: 14.813289642333984\n",
      "[step: 814] loss: 14.810989379882812\n",
      "[step: 815] loss: 14.808775901794434\n",
      "[step: 816] loss: 14.806638717651367\n",
      "[step: 817] loss: 14.804439544677734\n",
      "[step: 818] loss: 14.802225112915039\n",
      "[step: 819] loss: 14.800065994262695\n",
      "[step: 820] loss: 14.797908782958984\n",
      "[step: 821] loss: 14.795755386352539\n",
      "[step: 822] loss: 14.793608665466309\n",
      "[step: 823] loss: 14.791446685791016\n",
      "[step: 824] loss: 14.789316177368164\n",
      "[step: 825] loss: 14.78720760345459\n",
      "[step: 826] loss: 14.785089492797852\n",
      "[step: 827] loss: 14.782953262329102\n",
      "[step: 828] loss: 14.780848503112793\n",
      "[step: 829] loss: 14.778759956359863\n",
      "[step: 830] loss: 14.776663780212402\n",
      "[step: 831] loss: 14.774574279785156\n",
      "[step: 832] loss: 14.772485733032227\n",
      "[step: 833] loss: 14.770400047302246\n",
      "[step: 834] loss: 14.76833438873291\n",
      "[step: 835] loss: 14.766274452209473\n",
      "[step: 836] loss: 14.764204978942871\n",
      "[step: 837] loss: 14.762144088745117\n",
      "[step: 838] loss: 14.760092735290527\n",
      "[step: 839] loss: 14.758045196533203\n",
      "[step: 840] loss: 14.755999565124512\n",
      "[step: 841] loss: 14.7539644241333\n",
      "[step: 842] loss: 14.751921653747559\n",
      "[step: 843] loss: 14.749882698059082\n",
      "[step: 844] loss: 14.747857093811035\n",
      "[step: 845] loss: 14.745830535888672\n",
      "[step: 846] loss: 14.74380874633789\n",
      "[step: 847] loss: 14.741790771484375\n",
      "[step: 848] loss: 14.739773750305176\n",
      "[step: 849] loss: 14.737754821777344\n",
      "[step: 850] loss: 14.735742568969727\n",
      "[step: 851] loss: 14.733732223510742\n",
      "[step: 852] loss: 14.731724739074707\n",
      "[step: 853] loss: 14.729719161987305\n",
      "[step: 854] loss: 14.727718353271484\n",
      "[step: 855] loss: 14.725716590881348\n",
      "[step: 856] loss: 14.723715782165527\n",
      "[step: 857] loss: 14.721715927124023\n",
      "[step: 858] loss: 14.719722747802734\n",
      "[step: 859] loss: 14.717724800109863\n",
      "[step: 860] loss: 14.715731620788574\n",
      "[step: 861] loss: 14.713736534118652\n",
      "[step: 862] loss: 14.711747169494629\n",
      "[step: 863] loss: 14.709754943847656\n",
      "[step: 864] loss: 14.70776081085205\n",
      "[step: 865] loss: 14.705769538879395\n",
      "[step: 866] loss: 14.703775405883789\n",
      "[step: 867] loss: 14.701789855957031\n",
      "[step: 868] loss: 14.699795722961426\n",
      "[step: 869] loss: 14.697806358337402\n",
      "[step: 870] loss: 14.69581413269043\n",
      "[step: 871] loss: 14.693824768066406\n",
      "[step: 872] loss: 14.6918306350708\n",
      "[step: 873] loss: 14.689838409423828\n",
      "[step: 874] loss: 14.687849044799805\n",
      "[step: 875] loss: 14.685860633850098\n",
      "[step: 876] loss: 14.683871269226074\n",
      "[step: 877] loss: 14.681891441345215\n",
      "[step: 878] loss: 14.679925918579102\n",
      "[step: 879] loss: 14.677980422973633\n",
      "[step: 880] loss: 14.676092147827148\n",
      "[step: 881] loss: 14.674311637878418\n",
      "[step: 882] loss: 14.672740936279297\n",
      "[step: 883] loss: 14.671648979187012\n",
      "[step: 884] loss: 14.671462059020996\n",
      "[step: 885] loss: 14.673572540283203\n",
      "[step: 886] loss: 14.679719924926758\n",
      "[step: 887] loss: 14.697940826416016\n",
      "[step: 888] loss: 14.732901573181152\n",
      "[step: 889] loss: 14.834921836853027\n",
      "[step: 890] loss: 14.969544410705566\n",
      "[step: 891] loss: 15.42055892944336\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={\n",
    "                                X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    \n",
    "    test_predict = scaler2.inverse_transform(test_predict)\n",
    "    dataY2 = scaler2.inverse_transform(dataY2) # 데이터 있을때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "plt.title(date + ' predict')\n",
    "plt.plot(dataY2, label='raw data') # 데이터 있을때\n",
    "plt.plot(test_predict, label='predict')\n",
    "plt.xlabel(\"Time Period\")\n",
    "# plt.ylabel(\"photometric\")\n",
    "plt.ylabel(\"cct\")\n",
    "# plt.ylabel(\"CAS_SWR\")\n",
    "# plt.ylabel(\"446to477\")\n",
    "# plt.ylabel(\"UV-B\")\n",
    "plt.xticks(np.arange(0, 892, step = 56), ['5', '6', '7', '8', '9', '10', '11', '12', '1', '2', '3', '4', '5', '6', '7', '8']) # v2 모델\n",
    "# plt.yticks(np.arange(0, 140000, step=10000)) # ptmt\n",
    "plt.yticks(np.arange(0, 60000, step=10000)) # cct\n",
    "# plt.yticks(np.arange(0, 2.0, step=0.2)) # uvb\n",
    "# plt.ylim(0, 100) #swr, 446to477\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
