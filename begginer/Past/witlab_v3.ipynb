{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 그래프 외부에 출력\n",
    "%matplotlib tk\n",
    "\n",
    "tf.set_random_seed(777)  # 어느 컴퓨터에서 이 코드를 실행해도 학습 방향이 같도록, 다시 수행해도 같도록\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312, 24, 2)\n"
     ]
    }
   ],
   "source": [
    "# train Parameters\n",
    "# seq_length = 4\n",
    "seq_length = 24\n",
    "data_dim = 2\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 200\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1)) # 데이터 일반화\n",
    "scaler2 = MinMaxScaler(feature_range=(0,1)) # 데이터 일반화\n",
    "\n",
    "# xy = np.loadtxt('./v3data/train_v3_data_cct.csv', delimiter=',')\n",
    "# cct, cas_swr, 446to477, uvb, ptmt (892행 5열)\n",
    "xy = np.loadtxt('./v3data/data2.csv', delimiter=',')\n",
    "\n",
    "x = scaler.fit_transform(xy[:, [0, 1]]) # x = 맨 마지막 ptmt 제외 모든 것\n",
    "y = scaler2.fit_transform(xy[:, [2]]) \n",
    "\n",
    "# build a dataset\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "# print(len(x))\n",
    "# print(len(y))\n",
    "# print(x[0])\n",
    "# print(y[0])\n",
    "\n",
    "for i in range(0, len(x) - seq_length): # 한 행씩 dataX, Y에 추가\n",
    "    _x = x[i:i + seq_length]\n",
    "    _y = y[i + seq_length]  # Next close price\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "#     print(np.shape(_x))\n",
    "\n",
    "# print(train_size)\n",
    "# print(test_size)\n",
    "train_size = int(len(dataY) - 24)\n",
    "test_size = len(dataY) - train_size\n",
    "trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:len(dataX)])\n",
    "trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:len(dataY)])\n",
    "\n",
    "print(np.shape(trainX))\n",
    "\n",
    "# print(np.shape(trainX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gw/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 185.86099243164062\n",
      "[step: 1] loss: 135.4376220703125\n",
      "[step: 2] loss: 99.07176971435547\n",
      "[step: 3] loss: 76.78659057617188\n",
      "[step: 4] loss: 70.45518493652344\n",
      "[step: 5] loss: 76.30339813232422\n",
      "[step: 6] loss: 81.62855529785156\n",
      "[step: 7] loss: 81.40160369873047\n",
      "[step: 8] loss: 77.65081024169922\n",
      "[step: 9] loss: 73.04732513427734\n",
      "[step: 10] loss: 69.2232666015625\n",
      "[step: 11] loss: 66.79263305664062\n",
      "[step: 12] loss: 65.7138442993164\n",
      "[step: 13] loss: 65.62071228027344\n",
      "[step: 14] loss: 66.04625701904297\n",
      "[step: 15] loss: 66.55865478515625\n",
      "[step: 16] loss: 66.83866882324219\n",
      "[step: 17] loss: 66.71289825439453\n",
      "[step: 18] loss: 66.14936828613281\n",
      "[step: 19] loss: 65.22634887695312\n",
      "[step: 20] loss: 64.08866882324219\n",
      "[step: 21] loss: 62.90377426147461\n",
      "[step: 22] loss: 61.82448959350586\n",
      "[step: 23] loss: 60.9610595703125\n",
      "[step: 24] loss: 60.363616943359375\n",
      "[step: 25] loss: 60.016109466552734\n",
      "[step: 26] loss: 59.843421936035156\n",
      "[step: 27] loss: 59.73265838623047\n",
      "[step: 28] loss: 59.56553268432617\n",
      "[step: 29] loss: 59.25377655029297\n",
      "[step: 30] loss: 58.76472473144531\n",
      "[step: 31] loss: 58.126983642578125\n",
      "[step: 32] loss: 57.414302825927734\n",
      "[step: 33] loss: 56.71575164794922\n",
      "[step: 34] loss: 56.10456085205078\n",
      "[step: 35] loss: 55.61663055419922\n",
      "[step: 36] loss: 55.24409484863281\n",
      "[step: 37] loss: 54.94399642944336\n",
      "[step: 38] loss: 54.657466888427734\n",
      "[step: 39] loss: 54.332481384277344\n",
      "[step: 40] loss: 53.942569732666016\n",
      "[step: 41] loss: 53.49566650390625\n",
      "[step: 42] loss: 53.030033111572266\n",
      "[step: 43] loss: 52.5981330871582\n",
      "[step: 44] loss: 52.243431091308594\n",
      "[step: 45] loss: 51.97901153564453\n",
      "[step: 46] loss: 51.77923583984375\n",
      "[step: 47] loss: 51.592674255371094\n",
      "[step: 48] loss: 51.37273025512695\n",
      "[step: 49] loss: 51.10664367675781\n",
      "[step: 50] loss: 50.82136154174805\n",
      "[step: 51] loss: 50.56209182739258\n",
      "[step: 52] loss: 50.36042404174805\n",
      "[step: 53] loss: 50.21464538574219\n",
      "[step: 54] loss: 50.093994140625\n",
      "[step: 55] loss: 49.96146011352539\n",
      "[step: 56] loss: 49.798255920410156\n",
      "[step: 57] loss: 49.61354064941406\n",
      "[step: 58] loss: 49.434226989746094\n",
      "[step: 59] loss: 49.283409118652344\n",
      "[step: 60] loss: 49.163307189941406\n",
      "[step: 61] loss: 49.05543518066406\n",
      "[step: 62] loss: 48.937164306640625\n",
      "[step: 63] loss: 48.79998016357422\n",
      "[step: 64] loss: 48.65410614013672\n",
      "[step: 65] loss: 48.51739501953125\n",
      "[step: 66] loss: 48.399932861328125\n",
      "[step: 67] loss: 48.29752731323242\n",
      "[step: 68] loss: 48.19755935668945\n",
      "[step: 69] loss: 48.090309143066406\n",
      "[step: 70] loss: 47.97606658935547\n",
      "[step: 71] loss: 47.86308670043945\n",
      "[step: 72] loss: 47.75957489013672\n",
      "[step: 73] loss: 47.66688537597656\n",
      "[step: 74] loss: 47.579322814941406\n",
      "[step: 75] loss: 47.4898681640625\n",
      "[step: 76] loss: 47.396278381347656\n",
      "[step: 77] loss: 47.30209732055664\n",
      "[step: 78] loss: 47.212379455566406\n",
      "[step: 79] loss: 47.128700256347656\n",
      "[step: 80] loss: 47.04789352416992\n",
      "[step: 81] loss: 46.96525192260742\n",
      "[step: 82] loss: 46.87852096557617\n",
      "[step: 83] loss: 46.78925704956055\n",
      "[step: 84] loss: 46.700592041015625\n",
      "[step: 85] loss: 46.613861083984375\n",
      "[step: 86] loss: 46.52732467651367\n",
      "[step: 87] loss: 46.43800354003906\n",
      "[step: 88] loss: 46.34456253051758\n",
      "[step: 89] loss: 46.248291015625\n",
      "[step: 90] loss: 46.151336669921875\n",
      "[step: 91] loss: 46.05438995361328\n",
      "[step: 92] loss: 45.9564094543457\n",
      "[step: 93] loss: 45.85639953613281\n",
      "[step: 94] loss: 45.75513458251953\n",
      "[step: 95] loss: 45.654884338378906\n",
      "[step: 96] loss: 45.557498931884766\n",
      "[step: 97] loss: 45.46324157714844\n",
      "[step: 98] loss: 45.37168502807617\n",
      "[step: 99] loss: 45.28343200683594\n",
      "[step: 100] loss: 45.20043182373047\n",
      "[step: 101] loss: 45.124473571777344\n",
      "[step: 102] loss: 45.055885314941406\n",
      "[step: 103] loss: 44.99407958984375\n",
      "[step: 104] loss: 44.93890380859375\n",
      "[step: 105] loss: 44.89093017578125\n",
      "[step: 106] loss: 44.85036087036133\n",
      "[step: 107] loss: 44.816436767578125\n",
      "[step: 108] loss: 44.78804397583008\n",
      "[step: 109] loss: 44.76457977294922\n",
      "[step: 110] loss: 44.74557876586914\n",
      "[step: 111] loss: 44.729957580566406\n",
      "[step: 112] loss: 44.716060638427734\n",
      "[step: 113] loss: 44.70262145996094\n",
      "[step: 114] loss: 44.68901824951172\n",
      "[step: 115] loss: 44.67469024658203\n",
      "[step: 116] loss: 44.658870697021484\n",
      "[step: 117] loss: 44.64112854003906\n",
      "[step: 118] loss: 44.621883392333984\n",
      "[step: 119] loss: 44.60198211669922\n",
      "[step: 120] loss: 44.582000732421875\n",
      "[step: 121] loss: 44.562110900878906\n",
      "[step: 122] loss: 44.54241943359375\n",
      "[step: 123] loss: 44.522911071777344\n",
      "[step: 124] loss: 44.50330352783203\n",
      "[step: 125] loss: 44.483314514160156\n",
      "[step: 126] loss: 44.46302795410156\n",
      "[step: 127] loss: 44.44296646118164\n",
      "[step: 128] loss: 44.42365264892578\n",
      "[step: 129] loss: 44.40535354614258\n",
      "[step: 130] loss: 44.388126373291016\n",
      "[step: 131] loss: 44.37178421020508\n",
      "[step: 132] loss: 44.355865478515625\n",
      "[step: 133] loss: 44.339778900146484\n",
      "[step: 134] loss: 44.32312774658203\n",
      "[step: 135] loss: 44.30583953857422\n",
      "[step: 136] loss: 44.288055419921875\n",
      "[step: 137] loss: 44.26999282836914\n",
      "[step: 138] loss: 44.251914978027344\n",
      "[step: 139] loss: 44.234039306640625\n",
      "[step: 140] loss: 44.2164306640625\n",
      "[step: 141] loss: 44.19905090332031\n",
      "[step: 142] loss: 44.18183517456055\n",
      "[step: 143] loss: 44.164825439453125\n",
      "[step: 144] loss: 44.14809036254883\n",
      "[step: 145] loss: 44.1317138671875\n",
      "[step: 146] loss: 44.11575698852539\n",
      "[step: 147] loss: 44.10020065307617\n",
      "[step: 148] loss: 44.0849609375\n",
      "[step: 149] loss: 44.06987762451172\n",
      "[step: 150] loss: 44.054786682128906\n",
      "[step: 151] loss: 44.039608001708984\n",
      "[step: 152] loss: 44.02434158325195\n",
      "[step: 153] loss: 44.00901794433594\n",
      "[step: 154] loss: 43.993682861328125\n",
      "[step: 155] loss: 43.97838592529297\n",
      "[step: 156] loss: 43.963138580322266\n",
      "[step: 157] loss: 43.94792175292969\n",
      "[step: 158] loss: 43.93271255493164\n",
      "[step: 159] loss: 43.91754913330078\n",
      "[step: 160] loss: 43.90244674682617\n",
      "[step: 161] loss: 43.8874397277832\n",
      "[step: 162] loss: 43.87253189086914\n",
      "[step: 163] loss: 43.85770034790039\n",
      "[step: 164] loss: 43.842918395996094\n",
      "[step: 165] loss: 43.8281364440918\n",
      "[step: 166] loss: 43.81332015991211\n",
      "[step: 167] loss: 43.79847717285156\n",
      "[step: 168] loss: 43.783599853515625\n",
      "[step: 169] loss: 43.76871109008789\n",
      "[step: 170] loss: 43.753807067871094\n",
      "[step: 171] loss: 43.7388916015625\n",
      "[step: 172] loss: 43.72397232055664\n",
      "[step: 173] loss: 43.709041595458984\n",
      "[step: 174] loss: 43.694114685058594\n",
      "[step: 175] loss: 43.67920684814453\n",
      "[step: 176] loss: 43.66432189941406\n",
      "[step: 177] loss: 43.649452209472656\n",
      "[step: 178] loss: 43.63459777832031\n",
      "[step: 179] loss: 43.61974334716797\n",
      "[step: 180] loss: 43.60487365722656\n",
      "[step: 181] loss: 43.590003967285156\n",
      "[step: 182] loss: 43.57512283325195\n",
      "[step: 183] loss: 43.56024169921875\n",
      "[step: 184] loss: 43.54536437988281\n",
      "[step: 185] loss: 43.53048324584961\n",
      "[step: 186] loss: 43.51560592651367\n",
      "[step: 187] loss: 43.50072479248047\n",
      "[step: 188] loss: 43.485843658447266\n",
      "[step: 189] loss: 43.47095489501953\n",
      "[step: 190] loss: 43.45606231689453\n",
      "[step: 191] loss: 43.441162109375\n",
      "[step: 192] loss: 43.42624282836914\n",
      "[step: 193] loss: 43.41130828857422\n",
      "[step: 194] loss: 43.396339416503906\n",
      "[step: 195] loss: 43.381343841552734\n",
      "[step: 196] loss: 43.36631774902344\n",
      "[step: 197] loss: 43.351253509521484\n",
      "[step: 198] loss: 43.33615493774414\n",
      "[step: 199] loss: 43.321006774902344\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={\n",
    "                                X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    \n",
    "    test_predict = scaler2.inverse_transform(test_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "# plt.title(data + ' predict')\n",
    "# plt.plot(dataY2, label='raw data') # 데이터 있을때\n",
    "plt.plot(test_predict, label='predict')\n",
    "plt.xlabel(\"Time Period\")\n",
    "# plt.ylabel(\"photometric\")\n",
    "plt.ylabel(\"cct\")\n",
    "# plt.ylabel(\"CAS_SWR\")\n",
    "# plt.ylabel(\"446to477\")\n",
    "# plt.ylabel(\"UV-B\")\n",
    "# plt.xticks(np.arange(0, 892, step = 56), ['5', '6', '7', '8', '9', '10', '11', '12', '1', '2', '3', '4', '5', '6', '7', '8']) # v2 모델\n",
    "# plt.yticks(np.arange(0, 140000, step=10000)) # ptmt\n",
    "# plt.yticks(np.arange(0, 60000, step=10000)) # cct\n",
    "# plt.yticks(np.arange(0, 2.0, step=0.2)) # uvb\n",
    "# plt.ylim(0, 100) #swr, 446to477\n",
    "plt.grid(True)\n",
    "# plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
