{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 그래프 외부에 출력\n",
    "%matplotlib tk\n",
    "\n",
    "tf.set_random_seed(777)  # 어느 컴퓨터에서 이 코드를 실행해도 학습 방향이 같도록, 다시 수행해도 같도록\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Parameters\n",
    "seq_length = 4\n",
    "data_dim = 4\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 892\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1)) # 데이터 일반화\n",
    "scaler2 = MinMaxScaler(feature_range=(0, 1)) # 데이터 일반화\n",
    "\n",
    "# xy = np.loadtxt('./v3data/train_v3_data_cct.csv', delimiter=',')\n",
    "# cct, cas_swr, 446to477, uvb, temp, ptmt (892행 5열)\n",
    "xy = np.loadtxt('./v4data/train_v4_data.csv', delimiter=',')\n",
    "x = scaler.fit_transform(xy[:, 0:-1]) # x = 맨 마지막 ptmt 제외 모든 것\n",
    "y = scaler2.fit_transform(xy[:, [-1]])  # y = ptmt\n",
    "\n",
    "# build a dataset\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "# print(len(x))\n",
    "# print(len(y))\n",
    "# print(x[0])\n",
    "# print(y[0])\n",
    "\n",
    "for i in range(0, len(x) - seq_length): # 한 행씩 dataX, Y에 추가\n",
    "    _x = x[i:i + seq_length]\n",
    "    _y = y[i + seq_length]  # Next close price\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "#     print(np.shape(_x))\n",
    "\n",
    "# print(np.shape(dataX))\n",
    "# print(np.shape(dataY))\n",
    "\n",
    "data = '2018-02-27'\n",
    "# xy2 = np.loadtxt('./v3data/'+ data +'.csv',delimiter=',')\n",
    "# xy2 = np.loadtxt('./v3data/test_v3_data_cct_180331.csv', delimiter=',')\n",
    "xy2 = np.loadtxt('./v4data/test_v4_data_180227.csv', delimiter=',') # 예측할 날짜\n",
    "\n",
    "# x2 = scaler.fit_transform(xy2) # 데이터 없을때 (데이터란 ptmt 값)\n",
    "x2 = scaler.fit_transform(xy2[:, 0:-1]) # 데이터 있을때\n",
    "y2 = scaler2.fit_transform(xy2[:, [-1]]) #데이터 있을때\n",
    "\n",
    "# print(np.shape(x2))\n",
    "\n",
    "dataX2 = []\n",
    "dataY2 = [] #데이터 있을때\n",
    "\n",
    "for i in range(0, len(x2) - seq_length):\n",
    "    _x2 = x2[i:i+seq_length]\n",
    "    _y2 = y2[i+seq_length] #데이터 있을때\n",
    "    dataX2.append(_x2)\n",
    "    dataY2.append(_y2) #데이터 있을때\n",
    "# train/test split\n",
    "\n",
    "train_size = len(dataX)\n",
    "test_size = len(dataX2)\n",
    "\n",
    "# print(train_size)\n",
    "# print(test_size)\n",
    "\n",
    "trainX, testX = np.array(dataX[0:train_size]),np.array(dataX2[0:test_size])\n",
    "trainY = np.array(dataY[0:train_size])\n",
    "\n",
    "# print(np.shape(trainX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gw/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 3435.13818359375\n",
      "[step: 1] loss: 2961.3837890625\n",
      "[step: 2] loss: 2521.8017578125\n",
      "[step: 3] loss: 2112.0439453125\n",
      "[step: 4] loss: 1731.340087890625\n",
      "[step: 5] loss: 1381.1416015625\n",
      "[step: 6] loss: 1064.47607421875\n",
      "[step: 7] loss: 785.85888671875\n",
      "[step: 8] loss: 551.671875\n",
      "[step: 9] loss: 370.0962829589844\n",
      "[step: 10] loss: 249.84840393066406\n",
      "[step: 11] loss: 196.5001678466797\n",
      "[step: 12] loss: 204.7476043701172\n",
      "[step: 13] loss: 250.81629943847656\n",
      "[step: 14] loss: 300.6269226074219\n",
      "[step: 15] loss: 328.70263671875\n",
      "[step: 16] loss: 326.30242919921875\n",
      "[step: 17] loss: 297.65966796875\n",
      "[step: 18] loss: 252.90843200683594\n",
      "[step: 19] loss: 202.72052001953125\n",
      "[step: 20] loss: 155.48779296875\n",
      "[step: 21] loss: 116.39830780029297\n",
      "[step: 22] loss: 87.63726043701172\n",
      "[step: 23] loss: 69.11192321777344\n",
      "[step: 24] loss: 59.30617904663086\n",
      "[step: 25] loss: 56.03329849243164\n",
      "[step: 26] loss: 56.99007797241211\n",
      "[step: 27] loss: 60.10298156738281\n",
      "[step: 28] loss: 63.70323944091797\n",
      "[step: 29] loss: 66.5828857421875\n",
      "[step: 30] loss: 67.9783706665039\n",
      "[step: 31] loss: 67.51618194580078\n",
      "[step: 32] loss: 65.1422119140625\n",
      "[step: 33] loss: 61.047576904296875\n",
      "[step: 34] loss: 55.59675979614258\n",
      "[step: 35] loss: 49.260292053222656\n",
      "[step: 36] loss: 42.55329513549805\n",
      "[step: 37] loss: 35.97995376586914\n",
      "[step: 38] loss: 29.985136032104492\n",
      "[step: 39] loss: 24.91460609436035\n",
      "[step: 40] loss: 20.986318588256836\n",
      "[step: 41] loss: 18.2757625579834\n",
      "[step: 42] loss: 16.71746253967285\n",
      "[step: 43] loss: 16.12377166748047\n",
      "[step: 44] loss: 16.219364166259766\n",
      "[step: 45] loss: 16.68684196472168\n",
      "[step: 46] loss: 17.216238021850586\n",
      "[step: 47] loss: 17.54983901977539\n",
      "[step: 48] loss: 17.514358520507812\n",
      "[step: 49] loss: 17.035520553588867\n",
      "[step: 50] loss: 16.133846282958984\n",
      "[step: 51] loss: 14.904806137084961\n",
      "[step: 52] loss: 13.489338874816895\n",
      "[step: 53] loss: 12.041742324829102\n",
      "[step: 54] loss: 10.701251029968262\n",
      "[step: 55] loss: 9.57174301147461\n",
      "[step: 56] loss: 8.711318016052246\n",
      "[step: 57] loss: 8.13149356842041\n",
      "[step: 58] loss: 7.803962230682373\n",
      "[step: 59] loss: 7.672234535217285\n",
      "[step: 60] loss: 7.66522741317749\n",
      "[step: 61] loss: 7.710323333740234\n",
      "[step: 62] loss: 7.744139671325684\n",
      "[step: 63] loss: 7.719902992248535\n",
      "[step: 64] loss: 7.6110968589782715\n",
      "[step: 65] loss: 7.411640167236328\n",
      "[step: 66] loss: 7.133049011230469\n",
      "[step: 67] loss: 6.799621105194092\n",
      "[step: 68] loss: 6.442521572113037\n",
      "[step: 69] loss: 6.093707084655762\n",
      "[step: 70] loss: 5.7806806564331055\n",
      "[step: 71] loss: 5.522627353668213\n",
      "[step: 72] loss: 5.32839822769165\n",
      "[step: 73] loss: 5.196436882019043\n",
      "[step: 74] loss: 5.116424560546875\n",
      "[step: 75] loss: 5.072210311889648\n",
      "[step: 76] loss: 5.0453386306762695\n",
      "[step: 77] loss: 5.018524169921875\n",
      "[step: 78] loss: 4.978433609008789\n",
      "[step: 79] loss: 4.917362689971924\n",
      "[step: 80] loss: 4.83363151550293\n",
      "[step: 81] loss: 4.730836391448975\n",
      "[step: 82] loss: 4.616236686706543\n",
      "[step: 83] loss: 4.498729228973389\n",
      "[step: 84] loss: 4.38685941696167\n",
      "[step: 85] loss: 4.287245750427246\n",
      "[step: 86] loss: 4.203627109527588\n",
      "[step: 87] loss: 4.1366353034973145\n",
      "[step: 88] loss: 4.084169387817383\n",
      "[step: 89] loss: 4.042242527008057\n",
      "[step: 90] loss: 4.006012439727783\n",
      "[step: 91] loss: 3.970808744430542\n",
      "[step: 92] loss: 3.9329521656036377\n",
      "[step: 93] loss: 3.8902459144592285\n",
      "[step: 94] loss: 3.8421170711517334\n",
      "[step: 95] loss: 3.7894160747528076\n",
      "[step: 96] loss: 3.734001636505127\n",
      "[step: 97] loss: 3.678194761276245\n",
      "[step: 98] loss: 3.6242294311523438\n",
      "[step: 99] loss: 3.5738179683685303\n",
      "[step: 100] loss: 3.5278868675231934\n",
      "[step: 101] loss: 3.486509084701538\n",
      "[step: 102] loss: 3.4490232467651367\n",
      "[step: 103] loss: 3.4142704010009766\n",
      "[step: 104] loss: 3.380908727645874\n",
      "[step: 105] loss: 3.3477113246917725\n",
      "[step: 106] loss: 3.3137805461883545\n",
      "[step: 107] loss: 3.278681993484497\n",
      "[step: 108] loss: 3.242443323135376\n",
      "[step: 109] loss: 3.2054672241210938\n",
      "[step: 110] loss: 3.1683669090270996\n",
      "[step: 111] loss: 3.1318087577819824\n",
      "[step: 112] loss: 3.0963454246520996\n",
      "[step: 113] loss: 3.062319278717041\n",
      "[step: 114] loss: 3.029822826385498\n",
      "[step: 115] loss: 2.9987146854400635\n",
      "[step: 116] loss: 2.968691825866699\n",
      "[step: 117] loss: 2.9393794536590576\n",
      "[step: 118] loss: 2.91041898727417\n",
      "[step: 119] loss: 2.881545066833496\n",
      "[step: 120] loss: 2.8526222705841064\n",
      "[step: 121] loss: 2.8236501216888428\n",
      "[step: 122] loss: 2.7947397232055664\n",
      "[step: 123] loss: 2.7660648822784424\n",
      "[step: 124] loss: 2.737812042236328\n",
      "[step: 125] loss: 2.7101376056671143\n",
      "[step: 126] loss: 2.683131456375122\n",
      "[step: 127] loss: 2.656811475753784\n",
      "[step: 128] loss: 2.631126880645752\n",
      "[step: 129] loss: 2.605982780456543\n",
      "[step: 130] loss: 2.5812666416168213\n",
      "[step: 131] loss: 2.5568783283233643\n",
      "[step: 132] loss: 2.5327444076538086\n",
      "[step: 133] loss: 2.5088396072387695\n",
      "[step: 134] loss: 2.485170602798462\n",
      "[step: 135] loss: 2.461777687072754\n",
      "[step: 136] loss: 2.4387123584747314\n",
      "[step: 137] loss: 2.4160263538360596\n",
      "[step: 138] loss: 2.39375638961792\n",
      "[step: 139] loss: 2.3719146251678467\n",
      "[step: 140] loss: 2.350494146347046\n",
      "[step: 141] loss: 2.3294694423675537\n",
      "[step: 142] loss: 2.308807373046875\n",
      "[step: 143] loss: 2.288470983505249\n",
      "[step: 144] loss: 2.268434762954712\n",
      "[step: 145] loss: 2.248680830001831\n",
      "[step: 146] loss: 2.2292072772979736\n",
      "[step: 147] loss: 2.2100210189819336\n",
      "[step: 148] loss: 2.1911346912384033\n",
      "[step: 149] loss: 2.1725611686706543\n",
      "[step: 150] loss: 2.1543099880218506\n",
      "[step: 151] loss: 2.1363842487335205\n",
      "[step: 152] loss: 2.1187806129455566\n",
      "[step: 153] loss: 2.1014885902404785\n",
      "[step: 154] loss: 2.0844967365264893\n",
      "[step: 155] loss: 2.0677897930145264\n",
      "[step: 156] loss: 2.051358222961426\n",
      "[step: 157] loss: 2.0351929664611816\n",
      "[step: 158] loss: 2.019291639328003\n",
      "[step: 159] loss: 2.0036535263061523\n",
      "[step: 160] loss: 1.9882787466049194\n",
      "[step: 161] loss: 1.973170518875122\n",
      "[step: 162] loss: 1.9583287239074707\n",
      "[step: 163] loss: 1.943751573562622\n",
      "[step: 164] loss: 1.9294356107711792\n",
      "[step: 165] loss: 1.9153755903244019\n",
      "[step: 166] loss: 1.9015647172927856\n",
      "[step: 167] loss: 1.8879966735839844\n",
      "[step: 168] loss: 1.8746657371520996\n",
      "[step: 169] loss: 1.8615666627883911\n",
      "[step: 170] loss: 1.848698377609253\n",
      "[step: 171] loss: 1.8360565900802612\n",
      "[step: 172] loss: 1.8236401081085205\n",
      "[step: 173] loss: 1.8114486932754517\n",
      "[step: 174] loss: 1.7994787693023682\n",
      "[step: 175] loss: 1.787726879119873\n",
      "[step: 176] loss: 1.7761905193328857\n",
      "[step: 177] loss: 1.7648646831512451\n",
      "[step: 178] loss: 1.7537457942962646\n",
      "[step: 179] loss: 1.742830753326416\n",
      "[step: 180] loss: 1.7321139574050903\n",
      "[step: 181] loss: 1.721592664718628\n",
      "[step: 182] loss: 1.7112658023834229\n",
      "[step: 183] loss: 1.7011277675628662\n",
      "[step: 184] loss: 1.6911782026290894\n",
      "[step: 185] loss: 1.6814138889312744\n",
      "[step: 186] loss: 1.671831488609314\n",
      "[step: 187] loss: 1.6624302864074707\n",
      "[step: 188] loss: 1.653205156326294\n",
      "[step: 189] loss: 1.6441528797149658\n",
      "[step: 190] loss: 1.6352710723876953\n",
      "[step: 191] loss: 1.6265554428100586\n",
      "[step: 192] loss: 1.6180049180984497\n",
      "[step: 193] loss: 1.6096158027648926\n",
      "[step: 194] loss: 1.6013849973678589\n",
      "[step: 195] loss: 1.5933113098144531\n",
      "[step: 196] loss: 1.585391640663147\n",
      "[step: 197] loss: 1.5776221752166748\n",
      "[step: 198] loss: 1.5700021982192993\n",
      "[step: 199] loss: 1.5625275373458862\n",
      "[step: 200] loss: 1.555196762084961\n",
      "[step: 201] loss: 1.548006534576416\n",
      "[step: 202] loss: 1.5409547090530396\n",
      "[step: 203] loss: 1.5340383052825928\n",
      "[step: 204] loss: 1.5272550582885742\n",
      "[step: 205] loss: 1.5206025838851929\n",
      "[step: 206] loss: 1.5140783786773682\n",
      "[step: 207] loss: 1.5076818466186523\n",
      "[step: 208] loss: 1.501407504081726\n",
      "[step: 209] loss: 1.4952561855316162\n",
      "[step: 210] loss: 1.4892233610153198\n",
      "[step: 211] loss: 1.483307957649231\n",
      "[step: 212] loss: 1.4775075912475586\n",
      "[step: 213] loss: 1.47182035446167\n",
      "[step: 214] loss: 1.4662433862686157\n",
      "[step: 215] loss: 1.4607751369476318\n",
      "[step: 216] loss: 1.4554133415222168\n",
      "[step: 217] loss: 1.4501556158065796\n",
      "[step: 218] loss: 1.4450013637542725\n",
      "[step: 219] loss: 1.4399467706680298\n",
      "[step: 220] loss: 1.434990644454956\n",
      "[step: 221] loss: 1.4301316738128662\n",
      "[step: 222] loss: 1.4253668785095215\n",
      "[step: 223] loss: 1.4206949472427368\n",
      "[step: 224] loss: 1.4161137342453003\n",
      "[step: 225] loss: 1.4116220474243164\n",
      "[step: 226] loss: 1.407218337059021\n",
      "[step: 227] loss: 1.4028992652893066\n",
      "[step: 228] loss: 1.3986634016036987\n",
      "[step: 229] loss: 1.3945114612579346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 230] loss: 1.3904385566711426\n",
      "[step: 231] loss: 1.3864455223083496\n",
      "[step: 232] loss: 1.382529377937317\n",
      "[step: 233] loss: 1.3786883354187012\n",
      "[step: 234] loss: 1.374922275543213\n",
      "[step: 235] loss: 1.3712284564971924\n",
      "[step: 236] loss: 1.3676055669784546\n",
      "[step: 237] loss: 1.3640518188476562\n",
      "[step: 238] loss: 1.3605663776397705\n",
      "[step: 239] loss: 1.3571480512619019\n",
      "[step: 240] loss: 1.3537944555282593\n",
      "[step: 241] loss: 1.350504755973816\n",
      "[step: 242] loss: 1.347277045249939\n",
      "[step: 243] loss: 1.3441107273101807\n",
      "[step: 244] loss: 1.3410041332244873\n",
      "[step: 245] loss: 1.3379563093185425\n",
      "[step: 246] loss: 1.3349655866622925\n",
      "[step: 247] loss: 1.3320304155349731\n",
      "[step: 248] loss: 1.3291505575180054\n",
      "[step: 249] loss: 1.3263237476348877\n",
      "[step: 250] loss: 1.3235491514205933\n",
      "[step: 251] loss: 1.3208262920379639\n",
      "[step: 252] loss: 1.3181531429290771\n",
      "[step: 253] loss: 1.3155288696289062\n",
      "[step: 254] loss: 1.3129523992538452\n",
      "[step: 255] loss: 1.3104227781295776\n",
      "[step: 256] loss: 1.307938575744629\n",
      "[step: 257] loss: 1.3054991960525513\n",
      "[step: 258] loss: 1.3031034469604492\n",
      "[step: 259] loss: 1.3007508516311646\n",
      "[step: 260] loss: 1.2984390258789062\n",
      "[step: 261] loss: 1.2961686849594116\n",
      "[step: 262] loss: 1.2939373254776\n",
      "[step: 263] loss: 1.2917453050613403\n",
      "[step: 264] loss: 1.2895911931991577\n",
      "[step: 265] loss: 1.287474274635315\n",
      "[step: 266] loss: 1.2853937149047852\n",
      "[step: 267] loss: 1.283347249031067\n",
      "[step: 268] loss: 1.2813366651535034\n",
      "[step: 269] loss: 1.2793596982955933\n",
      "[step: 270] loss: 1.2774152755737305\n",
      "[step: 271] loss: 1.2755030393600464\n",
      "[step: 272] loss: 1.2736221551895142\n",
      "[step: 273] loss: 1.271772027015686\n",
      "[step: 274] loss: 1.2699517011642456\n",
      "[step: 275] loss: 1.2681610584259033\n",
      "[step: 276] loss: 1.2663979530334473\n",
      "[step: 277] loss: 1.2646631002426147\n",
      "[step: 278] loss: 1.262955904006958\n",
      "[step: 279] loss: 1.2612743377685547\n",
      "[step: 280] loss: 1.259619116783142\n",
      "[step: 281] loss: 1.2579892873764038\n",
      "[step: 282] loss: 1.2563835382461548\n",
      "[step: 283] loss: 1.2548022270202637\n",
      "[step: 284] loss: 1.2532440423965454\n",
      "[step: 285] loss: 1.251708984375\n",
      "[step: 286] loss: 1.2501955032348633\n",
      "[step: 287] loss: 1.2487043142318726\n",
      "[step: 288] loss: 1.2472347021102905\n",
      "[step: 289] loss: 1.2457852363586426\n",
      "[step: 290] loss: 1.2443559169769287\n",
      "[step: 291] loss: 1.2429466247558594\n",
      "[step: 292] loss: 1.2415562868118286\n",
      "[step: 293] loss: 1.2401846647262573\n",
      "[step: 294] loss: 1.2388309240341187\n",
      "[step: 295] loss: 1.23749577999115\n",
      "[step: 296] loss: 1.2361772060394287\n",
      "[step: 297] loss: 1.2348759174346924\n",
      "[step: 298] loss: 1.2335907220840454\n",
      "[step: 299] loss: 1.2323215007781982\n",
      "[step: 300] loss: 1.2310683727264404\n",
      "[step: 301] loss: 1.2298307418823242\n",
      "[step: 302] loss: 1.2286072969436646\n",
      "[step: 303] loss: 1.2273987531661987\n",
      "[step: 304] loss: 1.2262046337127686\n",
      "[step: 305] loss: 1.2250235080718994\n",
      "[step: 306] loss: 1.2238566875457764\n",
      "[step: 307] loss: 1.222701907157898\n",
      "[step: 308] loss: 1.2215608358383179\n",
      "[step: 309] loss: 1.220431923866272\n",
      "[step: 310] loss: 1.2193152904510498\n",
      "[step: 311] loss: 1.2182097434997559\n",
      "[step: 312] loss: 1.217116355895996\n",
      "[step: 313] loss: 1.216033935546875\n",
      "[step: 314] loss: 1.2149627208709717\n",
      "[step: 315] loss: 1.2139016389846802\n",
      "[step: 316] loss: 1.2128510475158691\n",
      "[step: 317] loss: 1.2118111848831177\n",
      "[step: 318] loss: 1.2107805013656616\n",
      "[step: 319] loss: 1.2097598314285278\n",
      "[step: 320] loss: 1.2087478637695312\n",
      "[step: 321] loss: 1.2077453136444092\n",
      "[step: 322] loss: 1.2067521810531616\n",
      "[step: 323] loss: 1.205767035484314\n",
      "[step: 324] loss: 1.2047908306121826\n",
      "[step: 325] loss: 1.2038222551345825\n",
      "[step: 326] loss: 1.2028613090515137\n",
      "[step: 327] loss: 1.201909065246582\n",
      "[step: 328] loss: 1.200964093208313\n",
      "[step: 329] loss: 1.200026512145996\n",
      "[step: 330] loss: 1.1990963220596313\n",
      "[step: 331] loss: 1.1981728076934814\n",
      "[step: 332] loss: 1.197256326675415\n",
      "[step: 333] loss: 1.196346402168274\n",
      "[step: 334] loss: 1.195443034172058\n",
      "[step: 335] loss: 1.1945463418960571\n",
      "[step: 336] loss: 1.1936551332473755\n",
      "[step: 337] loss: 1.1927703619003296\n",
      "[step: 338] loss: 1.1918914318084717\n",
      "[step: 339] loss: 1.1910182237625122\n",
      "[step: 340] loss: 1.190150260925293\n",
      "[step: 341] loss: 1.1892880201339722\n",
      "[step: 342] loss: 1.188430666923523\n",
      "[step: 343] loss: 1.1875791549682617\n",
      "[step: 344] loss: 1.1867318153381348\n",
      "[step: 345] loss: 1.1858899593353271\n",
      "[step: 346] loss: 1.1850526332855225\n",
      "[step: 347] loss: 1.184219479560852\n",
      "[step: 348] loss: 1.1833921670913696\n",
      "[step: 349] loss: 1.1825684309005737\n",
      "[step: 350] loss: 1.181748628616333\n",
      "[step: 351] loss: 1.1809332370758057\n",
      "[step: 352] loss: 1.180121898651123\n",
      "[step: 353] loss: 1.1793148517608643\n",
      "[step: 354] loss: 1.1785118579864502\n",
      "[step: 355] loss: 1.1777116060256958\n",
      "[step: 356] loss: 1.1769161224365234\n",
      "[step: 357] loss: 1.1761237382888794\n",
      "[step: 358] loss: 1.1753346920013428\n",
      "[step: 359] loss: 1.1745493412017822\n",
      "[step: 360] loss: 1.1737674474716187\n",
      "[step: 361] loss: 1.1729886531829834\n",
      "[step: 362] loss: 1.1722131967544556\n",
      "[step: 363] loss: 1.1714401245117188\n",
      "[step: 364] loss: 1.1706708669662476\n",
      "[step: 365] loss: 1.1699044704437256\n",
      "[step: 366] loss: 1.1691404581069946\n",
      "[step: 367] loss: 1.168379545211792\n",
      "[step: 368] loss: 1.167621374130249\n",
      "[step: 369] loss: 1.1668657064437866\n",
      "[step: 370] loss: 1.166113018989563\n",
      "[step: 371] loss: 1.1653629541397095\n",
      "[step: 372] loss: 1.1646143198013306\n",
      "[step: 373] loss: 1.1638689041137695\n",
      "[step: 374] loss: 1.16312575340271\n",
      "[step: 375] loss: 1.162385106086731\n",
      "[step: 376] loss: 1.1616466045379639\n",
      "[step: 377] loss: 1.1609100103378296\n",
      "[step: 378] loss: 1.1601756811141968\n",
      "[step: 379] loss: 1.1594432592391968\n",
      "[step: 380] loss: 1.1587132215499878\n",
      "[step: 381] loss: 1.157984733581543\n",
      "[step: 382] loss: 1.157258152961731\n",
      "[step: 383] loss: 1.1565340757369995\n",
      "[step: 384] loss: 1.1558115482330322\n",
      "[step: 385] loss: 1.1550910472869873\n",
      "[step: 386] loss: 1.1543723344802856\n",
      "[step: 387] loss: 1.1536555290222168\n",
      "[step: 388] loss: 1.1529399156570435\n",
      "[step: 389] loss: 1.1522260904312134\n",
      "[step: 390] loss: 1.151513934135437\n",
      "[step: 391] loss: 1.1508033275604248\n",
      "[step: 392] loss: 1.1500941514968872\n",
      "[step: 393] loss: 1.1493862867355347\n",
      "[step: 394] loss: 1.148680329322815\n",
      "[step: 395] loss: 1.1479756832122803\n",
      "[step: 396] loss: 1.1472729444503784\n",
      "[step: 397] loss: 1.1465706825256348\n",
      "[step: 398] loss: 1.145870566368103\n",
      "[step: 399] loss: 1.1451714038848877\n",
      "[step: 400] loss: 1.144473671913147\n",
      "[step: 401] loss: 1.1437773704528809\n",
      "[step: 402] loss: 1.1430822610855103\n",
      "[step: 403] loss: 1.1423877477645874\n",
      "[step: 404] loss: 1.141695261001587\n",
      "[step: 405] loss: 1.1410033702850342\n",
      "[step: 406] loss: 1.1403130292892456\n",
      "[step: 407] loss: 1.1396232843399048\n",
      "[step: 408] loss: 1.1389353275299072\n",
      "[step: 409] loss: 1.1382482051849365\n",
      "[step: 410] loss: 1.1375619173049927\n",
      "[step: 411] loss: 1.1368768215179443\n",
      "[step: 412] loss: 1.1361924409866333\n",
      "[step: 413] loss: 1.1355096101760864\n",
      "[step: 414] loss: 1.1348278522491455\n",
      "[step: 415] loss: 1.1341464519500732\n",
      "[step: 416] loss: 1.133466362953186\n",
      "[step: 417] loss: 1.1327874660491943\n",
      "[step: 418] loss: 1.132109522819519\n",
      "[step: 419] loss: 1.1314316987991333\n",
      "[step: 420] loss: 1.1307553052902222\n",
      "[step: 421] loss: 1.1300796270370483\n",
      "[step: 422] loss: 1.129404902458191\n",
      "[step: 423] loss: 1.1287306547164917\n",
      "[step: 424] loss: 1.128057599067688\n",
      "[step: 425] loss: 1.1273850202560425\n",
      "[step: 426] loss: 1.1267139911651611\n",
      "[step: 427] loss: 1.1260432004928589\n",
      "[step: 428] loss: 1.1253732442855835\n",
      "[step: 429] loss: 1.1247038841247559\n",
      "[step: 430] loss: 1.1240350008010864\n",
      "[step: 431] loss: 1.1233675479888916\n",
      "[step: 432] loss: 1.1227002143859863\n",
      "[step: 433] loss: 1.1220340728759766\n",
      "[step: 434] loss: 1.1213687658309937\n",
      "[step: 435] loss: 1.1207035779953003\n",
      "[step: 436] loss: 1.120039701461792\n",
      "[step: 437] loss: 1.1193759441375732\n",
      "[step: 438] loss: 1.1187127828598022\n",
      "[step: 439] loss: 1.1180509328842163\n",
      "[step: 440] loss: 1.11738920211792\n",
      "[step: 441] loss: 1.1167279481887817\n",
      "[step: 442] loss: 1.1160672903060913\n",
      "[step: 443] loss: 1.115407943725586\n",
      "[step: 444] loss: 1.1147488355636597\n",
      "[step: 445] loss: 1.1140904426574707\n",
      "[step: 446] loss: 1.1134326457977295\n",
      "[step: 447] loss: 1.112775206565857\n",
      "[step: 448] loss: 1.1121184825897217\n",
      "[step: 449] loss: 1.1114615201950073\n",
      "[step: 450] loss: 1.1108064651489258\n",
      "[step: 451] loss: 1.1101510524749756\n",
      "[step: 452] loss: 1.1094969511032104\n",
      "[step: 453] loss: 1.1088430881500244\n",
      "[step: 454] loss: 1.108189582824707\n",
      "[step: 455] loss: 1.107536792755127\n",
      "[step: 456] loss: 1.1068840026855469\n",
      "[step: 457] loss: 1.1062322854995728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 458] loss: 1.1055810451507568\n",
      "[step: 459] loss: 1.1049299240112305\n",
      "[step: 460] loss: 1.104279637336731\n",
      "[step: 461] loss: 1.1036299467086792\n",
      "[step: 462] loss: 1.1029807329177856\n",
      "[step: 463] loss: 1.102331519126892\n",
      "[step: 464] loss: 1.1016836166381836\n",
      "[step: 465] loss: 1.1010360717773438\n",
      "[step: 466] loss: 1.100388526916504\n",
      "[step: 467] loss: 1.0997415781021118\n",
      "[step: 468] loss: 1.0990949869155884\n",
      "[step: 469] loss: 1.098449468612671\n",
      "[step: 470] loss: 1.0978039503097534\n",
      "[step: 471] loss: 1.0971590280532837\n",
      "[step: 472] loss: 1.0965144634246826\n",
      "[step: 473] loss: 1.0958702564239502\n",
      "[step: 474] loss: 1.0952268838882446\n",
      "[step: 475] loss: 1.094583511352539\n",
      "[step: 476] loss: 1.0939406156539917\n",
      "[step: 477] loss: 1.0932986736297607\n",
      "[step: 478] loss: 1.0926568508148193\n",
      "[step: 479] loss: 1.0920155048370361\n",
      "[step: 480] loss: 1.0913745164871216\n",
      "[step: 481] loss: 1.0907344818115234\n",
      "[step: 482] loss: 1.0900943279266357\n",
      "[step: 483] loss: 1.0894546508789062\n",
      "[step: 484] loss: 1.0888152122497559\n",
      "[step: 485] loss: 1.0881763696670532\n",
      "[step: 486] loss: 1.0875381231307983\n",
      "[step: 487] loss: 1.0869004726409912\n",
      "[step: 488] loss: 1.0862631797790527\n",
      "[step: 489] loss: 1.0856261253356934\n",
      "[step: 490] loss: 1.084989070892334\n",
      "[step: 491] loss: 1.0843534469604492\n",
      "[step: 492] loss: 1.083717942237854\n",
      "[step: 493] loss: 1.0830824375152588\n",
      "[step: 494] loss: 1.0824474096298218\n",
      "[step: 495] loss: 1.081812858581543\n",
      "[step: 496] loss: 1.081178903579712\n",
      "[step: 497] loss: 1.08054518699646\n",
      "[step: 498] loss: 1.0799119472503662\n",
      "[step: 499] loss: 1.0792789459228516\n",
      "[step: 500] loss: 1.0786466598510742\n",
      "[step: 501] loss: 1.0780149698257446\n",
      "[step: 502] loss: 1.0773835182189941\n",
      "[step: 503] loss: 1.0767525434494019\n",
      "[step: 504] loss: 1.0761221647262573\n",
      "[step: 505] loss: 1.0754915475845337\n",
      "[step: 506] loss: 1.0748612880706787\n",
      "[step: 507] loss: 1.0742316246032715\n",
      "[step: 508] loss: 1.0736029148101807\n",
      "[step: 509] loss: 1.0729745626449585\n",
      "[step: 510] loss: 1.0723463296890259\n",
      "[step: 511] loss: 1.071718454360962\n",
      "[step: 512] loss: 1.0710914134979248\n",
      "[step: 513] loss: 1.0704642534255981\n",
      "[step: 514] loss: 1.0698379278182983\n",
      "[step: 515] loss: 1.0692121982574463\n",
      "[step: 516] loss: 1.0685861110687256\n",
      "[step: 517] loss: 1.0679612159729004\n",
      "[step: 518] loss: 1.0673363208770752\n",
      "[step: 519] loss: 1.0667121410369873\n",
      "[step: 520] loss: 1.0660881996154785\n",
      "[step: 521] loss: 1.0654648542404175\n",
      "[step: 522] loss: 1.064841389656067\n",
      "[step: 523] loss: 1.06421959400177\n",
      "[step: 524] loss: 1.0635970830917358\n",
      "[step: 525] loss: 1.0629754066467285\n",
      "[step: 526] loss: 1.062354326248169\n",
      "[step: 527] loss: 1.061733365058899\n",
      "[step: 528] loss: 1.0611132383346558\n",
      "[step: 529] loss: 1.0604931116104126\n",
      "[step: 530] loss: 1.0598739385604858\n",
      "[step: 531] loss: 1.05925452709198\n",
      "[step: 532] loss: 1.0586363077163696\n",
      "[step: 533] loss: 1.0580179691314697\n",
      "[step: 534] loss: 1.0573999881744385\n",
      "[step: 535] loss: 1.056783676147461\n",
      "[step: 536] loss: 1.0561665296554565\n",
      "[step: 537] loss: 1.0555505752563477\n",
      "[step: 538] loss: 1.0549347400665283\n",
      "[step: 539] loss: 1.0543196201324463\n",
      "[step: 540] loss: 1.0537046194076538\n",
      "[step: 541] loss: 1.053090214729309\n",
      "[step: 542] loss: 1.052476406097412\n",
      "[step: 543] loss: 1.0518630743026733\n",
      "[step: 544] loss: 1.0512502193450928\n",
      "[step: 545] loss: 1.0506378412246704\n",
      "[step: 546] loss: 1.0500258207321167\n",
      "[step: 547] loss: 1.0494145154953003\n",
      "[step: 548] loss: 1.0488040447235107\n",
      "[step: 549] loss: 1.0481939315795898\n",
      "[step: 550] loss: 1.0475839376449585\n",
      "[step: 551] loss: 1.046974778175354\n",
      "[step: 552] loss: 1.0463662147521973\n",
      "[step: 553] loss: 1.0457578897476196\n",
      "[step: 554] loss: 1.0451500415802002\n",
      "[step: 555] loss: 1.0445424318313599\n",
      "[step: 556] loss: 1.0439362525939941\n",
      "[step: 557] loss: 1.0433299541473389\n",
      "[step: 558] loss: 1.0427249670028687\n",
      "[step: 559] loss: 1.0421196222305298\n",
      "[step: 560] loss: 1.041515588760376\n",
      "[step: 561] loss: 1.0409115552902222\n",
      "[step: 562] loss: 1.0403085947036743\n",
      "[step: 563] loss: 1.0397058725357056\n",
      "[step: 564] loss: 1.0391039848327637\n",
      "[step: 565] loss: 1.0385019779205322\n",
      "[step: 566] loss: 1.0379010438919067\n",
      "[step: 567] loss: 1.0373008251190186\n",
      "[step: 568] loss: 1.0367010831832886\n",
      "[step: 569] loss: 1.0361018180847168\n",
      "[step: 570] loss: 1.035503625869751\n",
      "[step: 571] loss: 1.0349057912826538\n",
      "[step: 572] loss: 1.0343087911605835\n",
      "[step: 573] loss: 1.0337116718292236\n",
      "[step: 574] loss: 1.0331159830093384\n",
      "[step: 575] loss: 1.0325210094451904\n",
      "[step: 576] loss: 1.031926155090332\n",
      "[step: 577] loss: 1.0313323736190796\n",
      "[step: 578] loss: 1.0307388305664062\n",
      "[step: 579] loss: 1.0301460027694702\n",
      "[step: 580] loss: 1.0295543670654297\n",
      "[step: 581] loss: 1.0289630889892578\n",
      "[step: 582] loss: 1.0283725261688232\n",
      "[step: 583] loss: 1.0277825593948364\n",
      "[step: 584] loss: 1.0271934270858765\n",
      "[step: 585] loss: 1.026605248451233\n",
      "[step: 586] loss: 1.026017427444458\n",
      "[step: 587] loss: 1.02543044090271\n",
      "[step: 588] loss: 1.0248442888259888\n",
      "[step: 589] loss: 1.0242589712142944\n",
      "[step: 590] loss: 1.0236743688583374\n",
      "[step: 591] loss: 1.023090124130249\n",
      "[step: 592] loss: 1.0225070714950562\n",
      "[step: 593] loss: 1.0219244956970215\n",
      "[step: 594] loss: 1.0213433504104614\n",
      "[step: 595] loss: 1.0207624435424805\n",
      "[step: 596] loss: 1.0201823711395264\n",
      "[step: 597] loss: 1.0196031332015991\n",
      "[step: 598] loss: 1.0190247297286987\n",
      "[step: 599] loss: 1.0184473991394043\n",
      "[step: 600] loss: 1.0178710222244263\n",
      "[step: 601] loss: 1.017295241355896\n",
      "[step: 602] loss: 1.0167194604873657\n",
      "[step: 603] loss: 1.016145944595337\n",
      "[step: 604] loss: 1.015572428703308\n",
      "[step: 605] loss: 1.015000343322754\n",
      "[step: 606] loss: 1.0144288539886475\n",
      "[step: 607] loss: 1.0138587951660156\n",
      "[step: 608] loss: 1.0132886171340942\n",
      "[step: 609] loss: 1.0127204656600952\n",
      "[step: 610] loss: 1.0121527910232544\n",
      "[step: 611] loss: 1.0115855932235718\n",
      "[step: 612] loss: 1.011020302772522\n",
      "[step: 613] loss: 1.01045560836792\n",
      "[step: 614] loss: 1.0098912715911865\n",
      "[step: 615] loss: 1.0093286037445068\n",
      "[step: 616] loss: 1.0087664127349854\n",
      "[step: 617] loss: 1.0082063674926758\n",
      "[step: 618] loss: 1.0076466798782349\n",
      "[step: 619] loss: 1.0070879459381104\n",
      "[step: 620] loss: 1.0065298080444336\n",
      "[step: 621] loss: 1.0059735774993896\n",
      "[step: 622] loss: 1.0054174661636353\n",
      "[step: 623] loss: 1.0048636198043823\n",
      "[step: 624] loss: 1.004310131072998\n",
      "[step: 625] loss: 1.0037575960159302\n",
      "[step: 626] loss: 1.003206491470337\n",
      "[step: 627] loss: 1.0026568174362183\n",
      "[step: 628] loss: 1.0021071434020996\n",
      "[step: 629] loss: 1.0015597343444824\n",
      "[step: 630] loss: 1.0010132789611816\n",
      "[step: 631] loss: 1.0004682540893555\n",
      "[step: 632] loss: 0.9999237656593323\n",
      "[step: 633] loss: 0.9993802905082703\n",
      "[step: 634] loss: 0.998839259147644\n",
      "[step: 635] loss: 0.9982985854148865\n",
      "[step: 636] loss: 0.9977591037750244\n",
      "[step: 637] loss: 0.9972212910652161\n",
      "[step: 638] loss: 0.9966843128204346\n",
      "[step: 639] loss: 0.996148943901062\n",
      "[step: 640] loss: 0.9956146478652954\n",
      "[step: 641] loss: 0.995081901550293\n",
      "[step: 642] loss: 0.9945504069328308\n",
      "[step: 643] loss: 0.9940198659896851\n",
      "[step: 644] loss: 0.9934914112091064\n",
      "[step: 645] loss: 0.9929636120796204\n",
      "[step: 646] loss: 0.9924370050430298\n",
      "[step: 647] loss: 0.9919125437736511\n",
      "[step: 648] loss: 0.991388738155365\n",
      "[step: 649] loss: 0.9908667802810669\n",
      "[step: 650] loss: 0.9903461337089539\n",
      "[step: 651] loss: 0.9898270964622498\n",
      "[step: 652] loss: 0.9893096089363098\n",
      "[step: 653] loss: 0.9887933135032654\n",
      "[step: 654] loss: 0.9882783889770508\n",
      "[step: 655] loss: 0.9877652525901794\n",
      "[step: 656] loss: 0.9872531890869141\n",
      "[step: 657] loss: 0.9867429137229919\n",
      "[step: 658] loss: 0.9862340688705444\n",
      "[step: 659] loss: 0.9857269525527954\n",
      "[step: 660] loss: 0.9852213263511658\n",
      "[step: 661] loss: 0.9847172498703003\n",
      "[step: 662] loss: 0.9842145442962646\n",
      "[step: 663] loss: 0.9837138652801514\n",
      "[step: 664] loss: 0.9832144975662231\n",
      "[step: 665] loss: 0.9827167987823486\n",
      "[step: 666] loss: 0.9822207689285278\n",
      "[step: 667] loss: 0.9817265272140503\n",
      "[step: 668] loss: 0.9812338352203369\n",
      "[step: 669] loss: 0.9807423949241638\n",
      "[step: 670] loss: 0.9802533984184265\n",
      "[step: 671] loss: 0.9797654747962952\n",
      "[step: 672] loss: 0.9792799949645996\n",
      "[step: 673] loss: 0.978796124458313\n",
      "[step: 674] loss: 0.978313148021698\n",
      "[step: 675] loss: 0.9778322577476501\n",
      "[step: 676] loss: 0.9773532152175903\n",
      "[step: 677] loss: 0.9768766760826111\n",
      "[step: 678] loss: 0.976401150226593\n",
      "[step: 679] loss: 0.9759275913238525\n",
      "[step: 680] loss: 0.9754554033279419\n",
      "[step: 681] loss: 0.9749860763549805\n",
      "[step: 682] loss: 0.9745176434516907\n",
      "[step: 683] loss: 0.9740517735481262\n",
      "[step: 684] loss: 0.9735875129699707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 685] loss: 0.9731251001358032\n",
      "[step: 686] loss: 0.9726645350456238\n",
      "[step: 687] loss: 0.9722058773040771\n",
      "[step: 688] loss: 0.9717493057250977\n",
      "[step: 689] loss: 0.9712944030761719\n",
      "[step: 690] loss: 0.9708419442176819\n",
      "[step: 691] loss: 0.9703912734985352\n",
      "[step: 692] loss: 0.9699423909187317\n",
      "[step: 693] loss: 0.9694954752922058\n",
      "[step: 694] loss: 0.9690502285957336\n",
      "[step: 695] loss: 0.9686080813407898\n",
      "[step: 696] loss: 0.9681674242019653\n",
      "[step: 697] loss: 0.9677286744117737\n",
      "[step: 698] loss: 0.9672918319702148\n",
      "[step: 699] loss: 0.9668571352958679\n",
      "[step: 700] loss: 0.966424822807312\n",
      "[step: 701] loss: 0.965993344783783\n",
      "[step: 702] loss: 0.96556556224823\n",
      "[step: 703] loss: 0.9651392102241516\n",
      "[step: 704] loss: 0.9647149443626404\n",
      "[step: 705] loss: 0.9642931222915649\n",
      "[step: 706] loss: 0.963873565196991\n",
      "[step: 707] loss: 0.9634553790092468\n",
      "[step: 708] loss: 0.9630399942398071\n",
      "[step: 709] loss: 0.9626261591911316\n",
      "[step: 710] loss: 0.9622151851654053\n",
      "[step: 711] loss: 0.9618062376976013\n",
      "[step: 712] loss: 0.9613995552062988\n",
      "[step: 713] loss: 0.9609946608543396\n",
      "[step: 714] loss: 0.9605917930603027\n",
      "[step: 715] loss: 0.9601917862892151\n",
      "[step: 716] loss: 0.9597934484481812\n",
      "[step: 717] loss: 0.9593982100486755\n",
      "[step: 718] loss: 0.9590045213699341\n",
      "[step: 719] loss: 0.958613395690918\n",
      "[step: 720] loss: 0.9582241177558899\n",
      "[step: 721] loss: 0.9578375816345215\n",
      "[step: 722] loss: 0.9574528336524963\n",
      "[step: 723] loss: 0.9570711851119995\n",
      "[step: 724] loss: 0.9566909074783325\n",
      "[step: 725] loss: 0.95631343126297\n",
      "[step: 726] loss: 0.9559381604194641\n",
      "[step: 727] loss: 0.9555649161338806\n",
      "[step: 728] loss: 0.9551944136619568\n",
      "[step: 729] loss: 0.9548263549804688\n",
      "[step: 730] loss: 0.9544603228569031\n",
      "[step: 731] loss: 0.9540964365005493\n",
      "[step: 732] loss: 0.9537355899810791\n",
      "[step: 733] loss: 0.9533762335777283\n",
      "[step: 734] loss: 0.9530194401741028\n",
      "[step: 735] loss: 0.9526649117469788\n",
      "[step: 736] loss: 0.9523131251335144\n",
      "[step: 737] loss: 0.9519636034965515\n",
      "[step: 738] loss: 0.9516162872314453\n",
      "[step: 739] loss: 0.9512715935707092\n",
      "[step: 740] loss: 0.9509291648864746\n",
      "[step: 741] loss: 0.9505890011787415\n",
      "[step: 742] loss: 0.9502512812614441\n",
      "[step: 743] loss: 0.9499156475067139\n",
      "[step: 744] loss: 0.949583113193512\n",
      "[step: 745] loss: 0.9492528438568115\n",
      "[step: 746] loss: 0.9489246606826782\n",
      "[step: 747] loss: 0.9485984444618225\n",
      "[step: 748] loss: 0.9482747912406921\n",
      "[step: 749] loss: 0.9479539394378662\n",
      "[step: 750] loss: 0.9476355314254761\n",
      "[step: 751] loss: 0.9473188519477844\n",
      "[step: 752] loss: 0.9470052123069763\n",
      "[step: 753] loss: 0.9466937184333801\n",
      "[step: 754] loss: 0.9463846683502197\n",
      "[step: 755] loss: 0.9460775852203369\n",
      "[step: 756] loss: 0.9457733035087585\n",
      "[step: 757] loss: 0.9454712271690369\n",
      "[step: 758] loss: 0.9451717138290405\n",
      "[step: 759] loss: 0.9448745250701904\n",
      "[step: 760] loss: 0.9445797204971313\n",
      "[step: 761] loss: 0.9442874193191528\n",
      "[step: 762] loss: 0.943997323513031\n",
      "[step: 763] loss: 0.9437093138694763\n",
      "[step: 764] loss: 0.9434239268302917\n",
      "[step: 765] loss: 0.9431405067443848\n",
      "[step: 766] loss: 0.9428598284721375\n",
      "[step: 767] loss: 0.9425818920135498\n",
      "[step: 768] loss: 0.9423061609268188\n",
      "[step: 769] loss: 0.9420322179794312\n",
      "[step: 770] loss: 0.9417610764503479\n",
      "[step: 771] loss: 0.9414919018745422\n",
      "[step: 772] loss: 0.9412256479263306\n",
      "[step: 773] loss: 0.9409612417221069\n",
      "[step: 774] loss: 0.9406992793083191\n",
      "[step: 775] loss: 0.940438985824585\n",
      "[step: 776] loss: 0.9401815533638\n",
      "[step: 777] loss: 0.9399269223213196\n",
      "[step: 778] loss: 0.9396736025810242\n",
      "[step: 779] loss: 0.9394235014915466\n",
      "[step: 780] loss: 0.9391754865646362\n",
      "[step: 781] loss: 0.9389291405677795\n",
      "[step: 782] loss: 0.9386855363845825\n",
      "[step: 783] loss: 0.9384436011314392\n",
      "[step: 784] loss: 0.9382045269012451\n",
      "[step: 785] loss: 0.9379671812057495\n",
      "[step: 786] loss: 0.9377323985099792\n",
      "[step: 787] loss: 0.9375003576278687\n",
      "[step: 788] loss: 0.9372698068618774\n",
      "[step: 789] loss: 0.9370414614677429\n",
      "[step: 790] loss: 0.9368154406547546\n",
      "[step: 791] loss: 0.9365914463996887\n",
      "[step: 792] loss: 0.9363700151443481\n",
      "[step: 793] loss: 0.9361506700515747\n",
      "[step: 794] loss: 0.9359327554702759\n",
      "[step: 795] loss: 0.9357179999351501\n",
      "[step: 796] loss: 0.9355043768882751\n",
      "[step: 797] loss: 0.935293972492218\n",
      "[step: 798] loss: 0.935084879398346\n",
      "[step: 799] loss: 0.9348779320716858\n",
      "[step: 800] loss: 0.9346728324890137\n",
      "[step: 801] loss: 0.9344702363014221\n",
      "[step: 802] loss: 0.9342697858810425\n",
      "[step: 803] loss: 0.9340712428092957\n",
      "[step: 804] loss: 0.9338743686676025\n",
      "[step: 805] loss: 0.9336799383163452\n",
      "[step: 806] loss: 0.9334871768951416\n",
      "[step: 807] loss: 0.9332968592643738\n",
      "[step: 808] loss: 0.9331080913543701\n",
      "[step: 809] loss: 0.932921826839447\n",
      "[step: 810] loss: 0.9327362775802612\n",
      "[step: 811] loss: 0.9325544238090515\n",
      "[step: 812] loss: 0.9323733448982239\n",
      "[step: 813] loss: 0.9321942329406738\n",
      "[step: 814] loss: 0.9320172667503357\n",
      "[step: 815] loss: 0.931842029094696\n",
      "[step: 816] loss: 0.9316690564155579\n",
      "[step: 817] loss: 0.93149733543396\n",
      "[step: 818] loss: 0.9313279986381531\n",
      "[step: 819] loss: 0.9311605095863342\n",
      "[step: 820] loss: 0.9309939742088318\n",
      "[step: 821] loss: 0.9308302998542786\n",
      "[step: 822] loss: 0.9306679368019104\n",
      "[step: 823] loss: 0.930507481098175\n",
      "[step: 824] loss: 0.9303485155105591\n",
      "[step: 825] loss: 0.9301910400390625\n",
      "[step: 826] loss: 0.930036187171936\n",
      "[step: 827] loss: 0.9298824667930603\n",
      "[step: 828] loss: 0.9297305345535278\n",
      "[step: 829] loss: 0.9295803308486938\n",
      "[step: 830] loss: 0.9294313788414001\n",
      "[step: 831] loss: 0.9292850494384766\n",
      "[step: 832] loss: 0.9291391968727112\n",
      "[step: 833] loss: 0.9289956092834473\n",
      "[step: 834] loss: 0.9288535714149475\n",
      "[step: 835] loss: 0.9287130832672119\n",
      "[step: 836] loss: 0.9285742044448853\n",
      "[step: 837] loss: 0.9284367561340332\n",
      "[step: 838] loss: 0.928301215171814\n",
      "[step: 839] loss: 0.9281667470932007\n",
      "[step: 840] loss: 0.928034245967865\n",
      "[step: 841] loss: 0.9279030561447144\n",
      "[step: 842] loss: 0.9277730584144592\n",
      "[step: 843] loss: 0.9276451468467712\n",
      "[step: 844] loss: 0.9275182485580444\n",
      "[step: 845] loss: 0.9273931384086609\n",
      "[step: 846] loss: 0.927268922328949\n",
      "[step: 847] loss: 0.9271467328071594\n",
      "[step: 848] loss: 0.9270252585411072\n",
      "[step: 849] loss: 0.9269051551818848\n",
      "[step: 850] loss: 0.9267867803573608\n",
      "[step: 851] loss: 0.9266704320907593\n",
      "[step: 852] loss: 0.9265543818473816\n",
      "[step: 853] loss: 0.9264402985572815\n",
      "[step: 854] loss: 0.9263269305229187\n",
      "[step: 855] loss: 0.9262152910232544\n",
      "[step: 856] loss: 0.9261050820350647\n",
      "[step: 857] loss: 0.9259957075119019\n",
      "[step: 858] loss: 0.9258875846862793\n",
      "[step: 859] loss: 0.9257808327674866\n",
      "[step: 860] loss: 0.9256755709648132\n",
      "[step: 861] loss: 0.9255713820457458\n",
      "[step: 862] loss: 0.9254683256149292\n",
      "[step: 863] loss: 0.9253660440444946\n",
      "[step: 864] loss: 0.9252656102180481\n",
      "[step: 865] loss: 0.9251660108566284\n",
      "[step: 866] loss: 0.9250677227973938\n",
      "[step: 867] loss: 0.9249699711799622\n",
      "[step: 868] loss: 0.9248741269111633\n",
      "[step: 869] loss: 0.9247787594795227\n",
      "[step: 870] loss: 0.9246853590011597\n",
      "[step: 871] loss: 0.9245918989181519\n",
      "[step: 872] loss: 0.9245002269744873\n",
      "[step: 873] loss: 0.9244095087051392\n",
      "[step: 874] loss: 0.9243199229240417\n",
      "[step: 875] loss: 0.9242308735847473\n",
      "[step: 876] loss: 0.9241430163383484\n",
      "[step: 877] loss: 0.9240564107894897\n",
      "[step: 878] loss: 0.9239703416824341\n",
      "[step: 879] loss: 0.923885703086853\n",
      "[step: 880] loss: 0.9238016605377197\n",
      "[step: 881] loss: 0.9237191081047058\n",
      "[step: 882] loss: 0.9236368536949158\n",
      "[step: 883] loss: 0.9235556125640869\n",
      "[step: 884] loss: 0.923475444316864\n",
      "[step: 885] loss: 0.9233962893486023\n",
      "[step: 886] loss: 0.9233171939849854\n",
      "[step: 887] loss: 0.9232404828071594\n",
      "[step: 888] loss: 0.9231637716293335\n",
      "[step: 889] loss: 0.9230875372886658\n",
      "[step: 890] loss: 0.9230129718780518\n",
      "[step: 891] loss: 0.9229384660720825\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={\n",
    "                                X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    \n",
    "    test_predict = scaler2.inverse_transform(test_predict)\n",
    "    dataY2 = scaler2.inverse_transform(dataY2) # 데이터 있을때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "plt.title(data + ' predict')\n",
    "plt.plot(dataY2, label='row data') # 데이터 있을때\n",
    "plt.plot(test_predict, label='predict')\n",
    "plt.xlabel(\"Time Period\")\n",
    "# plt.ylabel(\"uvb\")\n",
    "# plt.ylabel(\"cct\")\n",
    "plt.ylabel(\"swr\")\n",
    "plt.xticks(np.arange(0, 892, step = 56), ['5', '6', '7', '8', '9', '10', '11', '12', '1', '2', '3', '4', '5', '6', '7', '8']) # v2 모델\n",
    "# plt.yticks(np.arange(0, 1.5, step=0.2))\n",
    "plt.ylim(0, 50)\n",
    "# plt.yticks(np.arange(0, 60000, step=10000))\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
