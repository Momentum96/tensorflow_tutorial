{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 그래프 외부에 출력\n",
    "%matplotlib tk\n",
    "\n",
    "tf.set_random_seed(777)  # 어느 컴퓨터에서 이 코드를 실행해도 학습 방향이 같도록, 다시 수행해도 같도록\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22932, 26, 3)\n",
      "(22932, 1)\n"
     ]
    }
   ],
   "source": [
    "# train Parameters\n",
    "seq_length = 26\n",
    "data_dim = 3\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 883\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1)) # 데이터 일반화\n",
    "scaler2 = MinMaxScaler(feature_range=(0, 1)) # 데이터 일반화\n",
    "\n",
    "# xy = np.loadtxt('./v3data/train_v3_data_cct.csv', delimiter=',')\n",
    "# cct, cas_swr, 446to477, uvb (883행 4열)\n",
    "xy = np.loadtxt('./v7data/train_v7_data.csv', delimiter=',')\n",
    "\n",
    "x = scaler.fit_transform(xy[:, [0, 2, 3]]) # x = 맨 마지막 ptmt 제외 모든 것\n",
    "y = scaler2.fit_transform(xy[:, [1]]) \n",
    "\n",
    "# build a dataset\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "# print(len(x))\n",
    "# print(len(y))\n",
    "# print(x[0])\n",
    "# print(y[0])\n",
    "\n",
    "for i in range(0, len(x) - seq_length): # 한 행씩 dataX, Y에 추가\n",
    "    _x = x[i:i + seq_length]\n",
    "    _y = y[i + seq_length]  # Next close price\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "#     print(np.shape(_x))\n",
    "\n",
    "print(np.shape(dataX))\n",
    "print(np.shape(dataY))\n",
    "\n",
    "data = '2018-05-04'\n",
    "# xy2 = np.loadtxt('./v3data/'+ data +'.csv',delimiter=',')\n",
    "# xy2 = np.loadtxt('./v3data/test_v3_data_cct_180331.csv', delimiter=',')\n",
    "# xy2 = np.loadtxt('./v3data/new/20180421.csv', delimiter=',') # 예측할 날짜\n",
    "xy2 = np.loadtxt('./v7data/test_v7_data.csv', delimiter=',')\n",
    "\n",
    "# x2 = scaler.fit_transform(xy2) # 데이터 없을때 (데이터란 ptmt 값)\n",
    "x2 = scaler.fit_transform(xy2[:, [0, 2, 3]]) # 데이터 있을때\n",
    "y2 = scaler2.fit_transform(xy2[:, [1]]) #데이터 있을때\n",
    "\n",
    "# print(np.shape(x2))\n",
    "\n",
    "dataX2 = []\n",
    "dataY2 = [] #데이터 있을때\n",
    "\n",
    "for i in range(0, len(x2) - seq_length):\n",
    "    _x2 = x2[i:i+seq_length]\n",
    "    _y2 = y2[i+seq_length] #데이터 있을때\n",
    "    dataX2.append(_x2)\n",
    "    dataY2.append(_y2) #데이터 있을때\n",
    "# train/test split\n",
    "\n",
    "train_size = len(dataX)\n",
    "test_size = len(dataX2)\n",
    "\n",
    "# print(train_size)\n",
    "# print(test_size)\n",
    "\n",
    "trainX, testX = np.array(dataX[0:train_size]),np.array(dataX2[0:test_size])\n",
    "trainY = np.array(dataY[0:train_size])\n",
    "\n",
    "# print(np.shape(trainX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gw/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 5189.48095703125\n",
      "[step: 1] loss: 3140.64892578125\n",
      "[step: 2] loss: 1693.6829833984375\n",
      "[step: 3] loss: 687.488037109375\n",
      "[step: 4] loss: 330.1980895996094\n",
      "[step: 5] loss: 617.039306640625\n",
      "[step: 6] loss: 760.6430053710938\n",
      "[step: 7] loss: 608.018798828125\n",
      "[step: 8] loss: 371.5423583984375\n",
      "[step: 9] loss: 192.1704559326172\n",
      "[step: 10] loss: 103.29158782958984\n",
      "[step: 11] loss: 84.25955963134766\n",
      "[step: 12] loss: 102.51351928710938\n",
      "[step: 13] loss: 131.05506896972656\n",
      "[step: 14] loss: 152.63963317871094\n",
      "[step: 15] loss: 159.20982360839844\n",
      "[step: 16] loss: 149.83486938476562\n",
      "[step: 17] loss: 128.17083740234375\n",
      "[step: 18] loss: 99.98747253417969\n",
      "[step: 19] loss: 71.18486785888672\n",
      "[step: 20] loss: 46.4925651550293\n",
      "[step: 21] loss: 28.828649520874023\n",
      "[step: 22] loss: 19.174776077270508\n",
      "[step: 23] loss: 16.810956954956055\n",
      "[step: 24] loss: 19.781831741333008\n",
      "[step: 25] loss: 25.49486541748047\n",
      "[step: 26] loss: 31.348669052124023\n",
      "[step: 27] loss: 35.277244567871094\n",
      "[step: 28] loss: 36.102596282958984\n",
      "[step: 29] loss: 33.63312530517578\n",
      "[step: 30] loss: 28.517375946044922\n",
      "[step: 31] loss: 21.929841995239258\n",
      "[step: 32] loss: 15.197672843933105\n",
      "[step: 33] loss: 9.47026252746582\n",
      "[step: 34] loss: 5.497933387756348\n",
      "[step: 35] loss: 3.5438108444213867\n",
      "[step: 36] loss: 3.417302131652832\n",
      "[step: 37] loss: 4.596220970153809\n",
      "[step: 38] loss: 6.3950910568237305\n",
      "[step: 39] loss: 8.137693405151367\n",
      "[step: 40] loss: 9.29847526550293\n",
      "[step: 41] loss: 9.589045524597168\n",
      "[step: 42] loss: 8.980086326599121\n",
      "[step: 43] loss: 7.663774490356445\n",
      "[step: 44] loss: 5.97346305847168\n",
      "[step: 45] loss: 4.284434795379639\n",
      "[step: 46] loss: 2.9202356338500977\n",
      "[step: 47] loss: 2.084601640701294\n",
      "[step: 48] loss: 1.8307666778564453\n",
      "[step: 49] loss: 2.0703227519989014\n",
      "[step: 50] loss: 2.6147449016571045\n",
      "[step: 51] loss: 3.2357001304626465\n",
      "[step: 52] loss: 3.727060317993164\n",
      "[step: 53] loss: 3.952869415283203\n",
      "[step: 54] loss: 3.87048077583313\n",
      "[step: 55] loss: 3.526305913925171\n",
      "[step: 56] loss: 3.028947114944458\n",
      "[step: 57] loss: 2.5103442668914795\n",
      "[step: 58] loss: 2.0870583057403564\n",
      "[step: 59] loss: 1.8320682048797607\n",
      "[step: 60] loss: 1.762776494026184\n",
      "[step: 61] loss: 1.8457250595092773\n",
      "[step: 62] loss: 2.0139942169189453\n",
      "[step: 63] loss: 2.1905689239501953\n",
      "[step: 64] loss: 2.310483694076538\n",
      "[step: 65] loss: 2.336134910583496\n",
      "[step: 66] loss: 2.26273775100708\n",
      "[step: 67] loss: 2.114100933074951\n",
      "[step: 68] loss: 1.9313050508499146\n",
      "[step: 69] loss: 1.7585365772247314\n",
      "[step: 70] loss: 1.6302841901779175\n",
      "[step: 71] loss: 1.5632942914962769\n",
      "[step: 72] loss: 1.5547406673431396\n",
      "[step: 73] loss: 1.5862170457839966\n",
      "[step: 74] loss: 1.6315441131591797\n",
      "[step: 75] loss: 1.665583848953247\n",
      "[step: 76] loss: 1.6713438034057617\n",
      "[step: 77] loss: 1.6435587406158447\n",
      "[step: 78] loss: 1.5882478952407837\n",
      "[step: 79] loss: 1.5189790725708008\n",
      "[step: 80] loss: 1.4515475034713745\n",
      "[step: 81] loss: 1.3988311290740967\n",
      "[step: 82] loss: 1.3674170970916748\n",
      "[step: 83] loss: 1.3566665649414062\n",
      "[step: 84] loss: 1.3601285219192505\n",
      "[step: 85] loss: 1.3684712648391724\n",
      "[step: 86] loss: 1.3728017807006836\n",
      "[step: 87] loss: 1.367274284362793\n",
      "[step: 88] loss: 1.350351333618164\n",
      "[step: 89] loss: 1.3245229721069336\n",
      "[step: 90] loss: 1.2948174476623535\n",
      "[step: 91] loss: 1.266785740852356\n",
      "[step: 92] loss: 1.2446463108062744\n",
      "[step: 93] loss: 1.2301602363586426\n",
      "[step: 94] loss: 1.2224735021591187\n",
      "[step: 95] loss: 1.2188156843185425\n",
      "[step: 96] loss: 1.2157111167907715\n",
      "[step: 97] loss: 1.2102121114730835\n",
      "[step: 98] loss: 1.2007631063461304\n",
      "[step: 99] loss: 1.1874557733535767\n",
      "[step: 100] loss: 1.1717272996902466\n",
      "[step: 101] loss: 1.1556286811828613\n",
      "[step: 102] loss: 1.1410447359085083\n",
      "[step: 103] loss: 1.129088282585144\n",
      "[step: 104] loss: 1.1198428869247437\n",
      "[step: 105] loss: 1.11252760887146\n",
      "[step: 106] loss: 1.1058942079544067\n",
      "[step: 107] loss: 1.0987390279769897\n",
      "[step: 108] loss: 1.090298056602478\n",
      "[step: 109] loss: 1.0804433822631836\n",
      "[step: 110] loss: 1.0696027278900146\n",
      "[step: 111] loss: 1.0585285425186157\n",
      "[step: 112] loss: 1.0479731559753418\n",
      "[step: 113] loss: 1.038422703742981\n",
      "[step: 114] loss: 1.0299763679504395\n",
      "[step: 115] loss: 1.0223766565322876\n",
      "[step: 116] loss: 1.015157699584961\n",
      "[step: 117] loss: 1.0078513622283936\n",
      "[step: 118] loss: 1.0001592636108398\n",
      "[step: 119] loss: 0.9920225143432617\n",
      "[step: 120] loss: 0.9836055040359497\n",
      "[step: 121] loss: 0.9751992225646973\n",
      "[step: 122] loss: 0.9670863747596741\n",
      "[step: 123] loss: 0.9594447612762451\n",
      "[step: 124] loss: 0.9522932767868042\n",
      "[step: 125] loss: 0.9455147385597229\n",
      "[step: 126] loss: 0.938924252986908\n",
      "[step: 127] loss: 0.932341456413269\n",
      "[step: 128] loss: 0.9256699681282043\n",
      "[step: 129] loss: 0.9189054369926453\n",
      "[step: 130] loss: 0.9121273756027222\n",
      "[step: 131] loss: 0.9054503440856934\n",
      "[step: 132] loss: 0.8989745378494263\n",
      "[step: 133] loss: 0.8927555680274963\n",
      "[step: 134] loss: 0.8867732286453247\n",
      "[step: 135] loss: 0.8809716105461121\n",
      "[step: 136] loss: 0.8752726912498474\n",
      "[step: 137] loss: 0.8696179986000061\n",
      "[step: 138] loss: 0.8639824986457825\n",
      "[step: 139] loss: 0.8583806157112122\n",
      "[step: 140] loss: 0.852850079536438\n",
      "[step: 141] loss: 0.8474379181861877\n",
      "[step: 142] loss: 0.842176079750061\n",
      "[step: 143] loss: 0.8370683193206787\n",
      "[step: 144] loss: 0.8320987820625305\n",
      "[step: 145] loss: 0.8272353410720825\n",
      "[step: 146] loss: 0.8224503397941589\n",
      "[step: 147] loss: 0.8177281022071838\n",
      "[step: 148] loss: 0.8130689859390259\n",
      "[step: 149] loss: 0.8084830641746521\n",
      "[step: 150] loss: 0.8039904832839966\n",
      "[step: 151] loss: 0.7996037602424622\n",
      "[step: 152] loss: 0.7953290343284607\n",
      "[step: 153] loss: 0.7911592125892639\n",
      "[step: 154] loss: 0.7870855927467346\n",
      "[step: 155] loss: 0.7830937504768372\n",
      "[step: 156] loss: 0.7791745662689209\n",
      "[step: 157] loss: 0.7753258943557739\n",
      "[step: 158] loss: 0.7715498805046082\n",
      "[step: 159] loss: 0.767855703830719\n",
      "[step: 160] loss: 0.7642485499382019\n",
      "[step: 161] loss: 0.7607288360595703\n",
      "[step: 162] loss: 0.7572929859161377\n",
      "[step: 163] loss: 0.7539376020431519\n",
      "[step: 164] loss: 0.7506560683250427\n",
      "[step: 165] loss: 0.7474467158317566\n",
      "[step: 166] loss: 0.7443017363548279\n",
      "[step: 167] loss: 0.7412261366844177\n",
      "[step: 168] loss: 0.7382203340530396\n",
      "[step: 169] loss: 0.7352869510650635\n",
      "[step: 170] loss: 0.7324248552322388\n",
      "[step: 171] loss: 0.729633092880249\n",
      "[step: 172] loss: 0.726908266544342\n",
      "[step: 173] loss: 0.7242485880851746\n",
      "[step: 174] loss: 0.7216477990150452\n",
      "[step: 175] loss: 0.7191110253334045\n",
      "[step: 176] loss: 0.7166308164596558\n",
      "[step: 177] loss: 0.7142128348350525\n",
      "[step: 178] loss: 0.7118545174598694\n",
      "[step: 179] loss: 0.7095556855201721\n",
      "[step: 180] loss: 0.7073130011558533\n",
      "[step: 181] loss: 0.7051286101341248\n",
      "[step: 182] loss: 0.7029985785484314\n",
      "[step: 183] loss: 0.7009197473526001\n",
      "[step: 184] loss: 0.6988933682441711\n",
      "[step: 185] loss: 0.6969180107116699\n",
      "[step: 186] loss: 0.6949926614761353\n",
      "[step: 187] loss: 0.6931179761886597\n",
      "[step: 188] loss: 0.691292405128479\n",
      "[step: 189] loss: 0.6895136833190918\n",
      "[step: 190] loss: 0.6877822875976562\n",
      "[step: 191] loss: 0.6860961318016052\n",
      "[step: 192] loss: 0.6844519376754761\n",
      "[step: 193] loss: 0.6828534007072449\n",
      "[step: 194] loss: 0.6812975406646729\n",
      "[step: 195] loss: 0.6797811388969421\n",
      "[step: 196] loss: 0.6783066391944885\n",
      "[step: 197] loss: 0.6768717169761658\n",
      "[step: 198] loss: 0.6754749417304993\n",
      "[step: 199] loss: 0.6741170287132263\n",
      "[step: 200] loss: 0.6727953553199768\n",
      "[step: 201] loss: 0.6715097427368164\n",
      "[step: 202] loss: 0.6702588796615601\n",
      "[step: 203] loss: 0.6690437197685242\n",
      "[step: 204] loss: 0.6678619384765625\n",
      "[step: 205] loss: 0.666712760925293\n",
      "[step: 206] loss: 0.6655963659286499\n",
      "[step: 207] loss: 0.6645095944404602\n",
      "[step: 208] loss: 0.6634536981582642\n",
      "[step: 209] loss: 0.6624264717102051\n",
      "[step: 210] loss: 0.6614296436309814\n",
      "[step: 211] loss: 0.6604602932929993\n",
      "[step: 212] loss: 0.6595199108123779\n",
      "[step: 213] loss: 0.6586042046546936\n",
      "[step: 214] loss: 0.6577155590057373\n",
      "[step: 215] loss: 0.6568520665168762\n",
      "[step: 216] loss: 0.6560132503509521\n",
      "[step: 217] loss: 0.6551982760429382\n",
      "[step: 218] loss: 0.6544075012207031\n",
      "[step: 219] loss: 0.6536386013031006\n",
      "[step: 220] loss: 0.6528910398483276\n",
      "[step: 221] loss: 0.652167022228241\n",
      "[step: 222] loss: 0.6514630317687988\n",
      "[step: 223] loss: 0.6507779955863953\n",
      "[step: 224] loss: 0.6501142978668213\n",
      "[step: 225] loss: 0.6494690775871277\n",
      "[step: 226] loss: 0.6488437056541443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 227] loss: 0.6482344269752502\n",
      "[step: 228] loss: 0.6476441025733948\n",
      "[step: 229] loss: 0.6470707654953003\n",
      "[step: 230] loss: 0.6465139985084534\n",
      "[step: 231] loss: 0.6459732055664062\n",
      "[step: 232] loss: 0.645447850227356\n",
      "[step: 233] loss: 0.6449390649795532\n",
      "[step: 234] loss: 0.6444440484046936\n",
      "[step: 235] loss: 0.6439629793167114\n",
      "[step: 236] loss: 0.6434950232505798\n",
      "[step: 237] loss: 0.6430423259735107\n",
      "[step: 238] loss: 0.6426010131835938\n",
      "[step: 239] loss: 0.6421732306480408\n",
      "[step: 240] loss: 0.6417570114135742\n",
      "[step: 241] loss: 0.6413536071777344\n",
      "[step: 242] loss: 0.6409614682197571\n",
      "[step: 243] loss: 0.6405815482139587\n",
      "[step: 244] loss: 0.640209972858429\n",
      "[step: 245] loss: 0.6398496627807617\n",
      "[step: 246] loss: 0.6395002007484436\n",
      "[step: 247] loss: 0.6391599774360657\n",
      "[step: 248] loss: 0.6388290524482727\n",
      "[step: 249] loss: 0.6385077834129333\n",
      "[step: 250] loss: 0.6381944417953491\n",
      "[step: 251] loss: 0.6378909349441528\n",
      "[step: 252] loss: 0.6375947594642639\n",
      "[step: 253] loss: 0.637307345867157\n",
      "[step: 254] loss: 0.637026309967041\n",
      "[step: 255] loss: 0.6367531418800354\n",
      "[step: 256] loss: 0.6364877223968506\n",
      "[step: 257] loss: 0.6362295746803284\n",
      "[step: 258] loss: 0.6359781622886658\n",
      "[step: 259] loss: 0.6357324719429016\n",
      "[step: 260] loss: 0.6354935169219971\n",
      "[step: 261] loss: 0.635261058807373\n",
      "[step: 262] loss: 0.6350342035293579\n",
      "[step: 263] loss: 0.6348129510879517\n",
      "[step: 264] loss: 0.6345969438552856\n",
      "[step: 265] loss: 0.6343864798545837\n",
      "[step: 266] loss: 0.6341813802719116\n",
      "[step: 267] loss: 0.6339802742004395\n",
      "[step: 268] loss: 0.6337851881980896\n",
      "[step: 269] loss: 0.633594810962677\n",
      "[step: 270] loss: 0.6334077715873718\n",
      "[step: 271] loss: 0.6332248449325562\n",
      "[step: 272] loss: 0.633047342300415\n",
      "[step: 273] loss: 0.6328732371330261\n",
      "[step: 274] loss: 0.6327018141746521\n",
      "[step: 275] loss: 0.6325349807739258\n",
      "[step: 276] loss: 0.6323723793029785\n",
      "[step: 277] loss: 0.6322123408317566\n",
      "[step: 278] loss: 0.6320562958717346\n",
      "[step: 279] loss: 0.6319034695625305\n",
      "[step: 280] loss: 0.631753146648407\n",
      "[step: 281] loss: 0.6316052079200745\n",
      "[step: 282] loss: 0.6314611434936523\n",
      "[step: 283] loss: 0.6313192248344421\n",
      "[step: 284] loss: 0.6311798095703125\n",
      "[step: 285] loss: 0.6310436725616455\n",
      "[step: 286] loss: 0.6309095025062561\n",
      "[step: 287] loss: 0.6307777762413025\n",
      "[step: 288] loss: 0.6306485533714294\n",
      "[step: 289] loss: 0.6305213570594788\n",
      "[step: 290] loss: 0.6303961873054504\n",
      "[step: 291] loss: 0.6302732229232788\n",
      "[step: 292] loss: 0.63015216588974\n",
      "[step: 293] loss: 0.630033016204834\n",
      "[step: 294] loss: 0.6299160718917847\n",
      "[step: 295] loss: 0.6298004984855652\n",
      "[step: 296] loss: 0.6296861171722412\n",
      "[step: 297] loss: 0.6295738816261292\n",
      "[step: 298] loss: 0.6294633746147156\n",
      "[step: 299] loss: 0.6293534636497498\n",
      "[step: 300] loss: 0.6292461156845093\n",
      "[step: 301] loss: 0.6291398406028748\n",
      "[step: 302] loss: 0.6290350556373596\n",
      "[step: 303] loss: 0.6289306879043579\n",
      "[step: 304] loss: 0.6288284659385681\n",
      "[step: 305] loss: 0.6287267804145813\n",
      "[step: 306] loss: 0.6286267638206482\n",
      "[step: 307] loss: 0.6285274028778076\n",
      "[step: 308] loss: 0.6284297108650208\n",
      "[step: 309] loss: 0.6283334493637085\n",
      "[step: 310] loss: 0.6282373070716858\n",
      "[step: 311] loss: 0.6281419396400452\n",
      "[step: 312] loss: 0.6280481219291687\n",
      "[step: 313] loss: 0.6279545426368713\n",
      "[step: 314] loss: 0.6278623938560486\n",
      "[step: 315] loss: 0.6277714371681213\n",
      "[step: 316] loss: 0.6276807188987732\n",
      "[step: 317] loss: 0.6275911331176758\n",
      "[step: 318] loss: 0.6275020241737366\n",
      "[step: 319] loss: 0.6274135708808899\n",
      "[step: 320] loss: 0.627326488494873\n",
      "[step: 321] loss: 0.6272394061088562\n",
      "[step: 322] loss: 0.6271533370018005\n",
      "[step: 323] loss: 0.6270679235458374\n",
      "[step: 324] loss: 0.6269829869270325\n",
      "[step: 325] loss: 0.6268982887268066\n",
      "[step: 326] loss: 0.6268140077590942\n",
      "[step: 327] loss: 0.6267306804656982\n",
      "[step: 328] loss: 0.6266481280326843\n",
      "[step: 329] loss: 0.6265658140182495\n",
      "[step: 330] loss: 0.6264840364456177\n",
      "[step: 331] loss: 0.6264030933380127\n",
      "[step: 332] loss: 0.6263220310211182\n",
      "[step: 333] loss: 0.6262418627738953\n",
      "[step: 334] loss: 0.6261621117591858\n",
      "[step: 335] loss: 0.626082718372345\n",
      "[step: 336] loss: 0.6260042190551758\n",
      "[step: 337] loss: 0.6259260177612305\n",
      "[step: 338] loss: 0.6258482336997986\n",
      "[step: 339] loss: 0.6257702708244324\n",
      "[step: 340] loss: 0.62569260597229\n",
      "[step: 341] loss: 0.6256152391433716\n",
      "[step: 342] loss: 0.6255389451980591\n",
      "[step: 343] loss: 0.6254627704620361\n",
      "[step: 344] loss: 0.6253869533538818\n",
      "[step: 345] loss: 0.6253119111061096\n",
      "[step: 346] loss: 0.6252368688583374\n",
      "[step: 347] loss: 0.6251616477966309\n",
      "[step: 348] loss: 0.6250870823860168\n",
      "[step: 349] loss: 0.6250125765800476\n",
      "[step: 350] loss: 0.6249387860298157\n",
      "[step: 351] loss: 0.6248647570610046\n",
      "[step: 352] loss: 0.62479168176651\n",
      "[step: 353] loss: 0.6247190833091736\n",
      "[step: 354] loss: 0.6246466040611267\n",
      "[step: 355] loss: 0.6245741248130798\n",
      "[step: 356] loss: 0.6245018243789673\n",
      "[step: 357] loss: 0.6244298815727234\n",
      "[step: 358] loss: 0.6243582963943481\n",
      "[step: 359] loss: 0.6242865920066833\n",
      "[step: 360] loss: 0.6242157816886902\n",
      "[step: 361] loss: 0.6241443753242493\n",
      "[step: 362] loss: 0.6240739822387695\n",
      "[step: 363] loss: 0.6240037083625793\n",
      "[step: 364] loss: 0.6239331364631653\n",
      "[step: 365] loss: 0.623863160610199\n",
      "[step: 366] loss: 0.6237938404083252\n",
      "[step: 367] loss: 0.6237241625785828\n",
      "[step: 368] loss: 0.623654842376709\n",
      "[step: 369] loss: 0.6235862374305725\n",
      "[step: 370] loss: 0.6235174536705017\n",
      "[step: 371] loss: 0.6234489679336548\n",
      "[step: 372] loss: 0.6233803629875183\n",
      "[step: 373] loss: 0.62331223487854\n",
      "[step: 374] loss: 0.6232448220252991\n",
      "[step: 375] loss: 0.6231770515441895\n",
      "[step: 376] loss: 0.6231098175048828\n",
      "[step: 377] loss: 0.6230428814888\n",
      "[step: 378] loss: 0.6229756474494934\n",
      "[step: 379] loss: 0.6229091286659241\n",
      "[step: 380] loss: 0.6228424906730652\n",
      "[step: 381] loss: 0.6227760910987854\n",
      "[step: 382] loss: 0.6227101683616638\n",
      "[step: 383] loss: 0.6226443648338318\n",
      "[step: 384] loss: 0.6225783824920654\n",
      "[step: 385] loss: 0.6225126385688782\n",
      "[step: 386] loss: 0.6224474906921387\n",
      "[step: 387] loss: 0.6223821640014648\n",
      "[step: 388] loss: 0.622317373752594\n",
      "[step: 389] loss: 0.6222524046897888\n",
      "[step: 390] loss: 0.6221877932548523\n",
      "[step: 391] loss: 0.6221237182617188\n",
      "[step: 392] loss: 0.6220594048500061\n",
      "[step: 393] loss: 0.621995747089386\n",
      "[step: 394] loss: 0.6219319701194763\n",
      "[step: 395] loss: 0.6218681335449219\n",
      "[step: 396] loss: 0.6218050718307495\n",
      "[step: 397] loss: 0.6217419505119324\n",
      "[step: 398] loss: 0.6216791868209839\n",
      "[step: 399] loss: 0.6216162443161011\n",
      "[step: 400] loss: 0.6215539574623108\n",
      "[step: 401] loss: 0.6214911937713623\n",
      "[step: 402] loss: 0.6214287877082825\n",
      "[step: 403] loss: 0.6213667988777161\n",
      "[step: 404] loss: 0.6213047504425049\n",
      "[step: 405] loss: 0.6212431192398071\n",
      "[step: 406] loss: 0.6211813688278198\n",
      "[step: 407] loss: 0.6211200952529907\n",
      "[step: 408] loss: 0.6210589408874512\n",
      "[step: 409] loss: 0.620998203754425\n",
      "[step: 410] loss: 0.6209374070167542\n",
      "[step: 411] loss: 0.6208769083023071\n",
      "[step: 412] loss: 0.6208166480064392\n",
      "[step: 413] loss: 0.6207563877105713\n",
      "[step: 414] loss: 0.6206966042518616\n",
      "[step: 415] loss: 0.6206368803977966\n",
      "[step: 416] loss: 0.6205775141716003\n",
      "[step: 417] loss: 0.6205180883407593\n",
      "[step: 418] loss: 0.6204589009284973\n",
      "[step: 419] loss: 0.6203992366790771\n",
      "[step: 420] loss: 0.6203400492668152\n",
      "[step: 421] loss: 0.6202816963195801\n",
      "[step: 422] loss: 0.620223343372345\n",
      "[step: 423] loss: 0.6201648712158203\n",
      "[step: 424] loss: 0.6201063990592957\n",
      "[step: 425] loss: 0.6200487017631531\n",
      "[step: 426] loss: 0.619990885257721\n",
      "[step: 427] loss: 0.6199325919151306\n",
      "[step: 428] loss: 0.6198752522468567\n",
      "[step: 429] loss: 0.6198177337646484\n",
      "[step: 430] loss: 0.6197608709335327\n",
      "[step: 431] loss: 0.6197037696838379\n",
      "[step: 432] loss: 0.6196473240852356\n",
      "[step: 433] loss: 0.6195906400680542\n",
      "[step: 434] loss: 0.6195341348648071\n",
      "[step: 435] loss: 0.6194779276847839\n",
      "[step: 436] loss: 0.6194220781326294\n",
      "[step: 437] loss: 0.6193661689758301\n",
      "[step: 438] loss: 0.6193104386329651\n",
      "[step: 439] loss: 0.6192550659179688\n",
      "[step: 440] loss: 0.6191995739936829\n",
      "[step: 441] loss: 0.6191439628601074\n",
      "[step: 442] loss: 0.6190888285636902\n",
      "[step: 443] loss: 0.6190337538719177\n",
      "[step: 444] loss: 0.6189790964126587\n",
      "[step: 445] loss: 0.6189243793487549\n",
      "[step: 446] loss: 0.6188703775405884\n",
      "[step: 447] loss: 0.6188163161277771\n",
      "[step: 448] loss: 0.6187624931335449\n",
      "[step: 449] loss: 0.6187084317207336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 450] loss: 0.6186548471450806\n",
      "[step: 451] loss: 0.6186008453369141\n",
      "[step: 452] loss: 0.6185476183891296\n",
      "[step: 453] loss: 0.6184946298599243\n",
      "[step: 454] loss: 0.6184418797492981\n",
      "[step: 455] loss: 0.6183887720108032\n",
      "[step: 456] loss: 0.6183362007141113\n",
      "[step: 457] loss: 0.6182838082313538\n",
      "[step: 458] loss: 0.618231475353241\n",
      "[step: 459] loss: 0.6181796193122864\n",
      "[step: 460] loss: 0.6181278824806213\n",
      "[step: 461] loss: 0.6180760264396667\n",
      "[step: 462] loss: 0.6180243492126465\n",
      "[step: 463] loss: 0.6179726123809814\n",
      "[step: 464] loss: 0.6179211735725403\n",
      "[step: 465] loss: 0.6178702116012573\n",
      "[step: 466] loss: 0.6178191900253296\n",
      "[step: 467] loss: 0.6177684664726257\n",
      "[step: 468] loss: 0.617717981338501\n",
      "[step: 469] loss: 0.617667555809021\n",
      "[step: 470] loss: 0.6176171898841858\n",
      "[step: 471] loss: 0.6175669431686401\n",
      "[step: 472] loss: 0.6175169944763184\n",
      "[step: 473] loss: 0.6174671649932861\n",
      "[step: 474] loss: 0.6174176335334778\n",
      "[step: 475] loss: 0.6173679232597351\n",
      "[step: 476] loss: 0.6173183917999268\n",
      "[step: 477] loss: 0.6172689199447632\n",
      "[step: 478] loss: 0.6172201037406921\n",
      "[step: 479] loss: 0.617171585559845\n",
      "[step: 480] loss: 0.6171228885650635\n",
      "[step: 481] loss: 0.6170741319656372\n",
      "[step: 482] loss: 0.6170259714126587\n",
      "[step: 483] loss: 0.6169776916503906\n",
      "[step: 484] loss: 0.6169295907020569\n",
      "[step: 485] loss: 0.6168817281723022\n",
      "[step: 486] loss: 0.6168342232704163\n",
      "[step: 487] loss: 0.6167865991592407\n",
      "[step: 488] loss: 0.6167390942573547\n",
      "[step: 489] loss: 0.6166920065879822\n",
      "[step: 490] loss: 0.6166443824768066\n",
      "[step: 491] loss: 0.616597592830658\n",
      "[step: 492] loss: 0.6165508031845093\n",
      "[step: 493] loss: 0.6165043711662292\n",
      "[step: 494] loss: 0.6164575815200806\n",
      "[step: 495] loss: 0.6164114475250244\n",
      "[step: 496] loss: 0.6163647770881653\n",
      "[step: 497] loss: 0.6163188219070435\n",
      "[step: 498] loss: 0.6162731051445007\n",
      "[step: 499] loss: 0.6162271499633789\n",
      "[step: 500] loss: 0.6161816120147705\n",
      "[step: 501] loss: 0.616136372089386\n",
      "[step: 502] loss: 0.6160908937454224\n",
      "[step: 503] loss: 0.6160457730293274\n",
      "[step: 504] loss: 0.616000771522522\n",
      "[step: 505] loss: 0.61595618724823\n",
      "[step: 506] loss: 0.6159117221832275\n",
      "[step: 507] loss: 0.6158674359321594\n",
      "[step: 508] loss: 0.6158230900764465\n",
      "[step: 509] loss: 0.6157788634300232\n",
      "[step: 510] loss: 0.615734875202179\n",
      "[step: 511] loss: 0.6156904101371765\n",
      "[step: 512] loss: 0.6156468987464905\n",
      "[step: 513] loss: 0.615603506565094\n",
      "[step: 514] loss: 0.6155604720115662\n",
      "[step: 515] loss: 0.6155173182487488\n",
      "[step: 516] loss: 0.6154741644859314\n",
      "[step: 517] loss: 0.6154308319091797\n",
      "[step: 518] loss: 0.615388035774231\n",
      "[step: 519] loss: 0.6153451800346375\n",
      "[step: 520] loss: 0.6153026223182678\n",
      "[step: 521] loss: 0.6152603030204773\n",
      "[step: 522] loss: 0.6152181029319763\n",
      "[step: 523] loss: 0.6151758432388306\n",
      "[step: 524] loss: 0.615134060382843\n",
      "[step: 525] loss: 0.615091860294342\n",
      "[step: 526] loss: 0.6150503158569336\n",
      "[step: 527] loss: 0.6150087714195251\n",
      "[step: 528] loss: 0.6149676442146301\n",
      "[step: 529] loss: 0.6149261593818665\n",
      "[step: 530] loss: 0.6148849725723267\n",
      "[step: 531] loss: 0.6148443818092346\n",
      "[step: 532] loss: 0.6148033738136292\n",
      "[step: 533] loss: 0.6147626638412476\n",
      "[step: 534] loss: 0.6147220730781555\n",
      "[step: 535] loss: 0.614681601524353\n",
      "[step: 536] loss: 0.6146414279937744\n",
      "[step: 537] loss: 0.6146014332771301\n",
      "[step: 538] loss: 0.6145613789558411\n",
      "[step: 539] loss: 0.6145213842391968\n",
      "[step: 540] loss: 0.6144816279411316\n",
      "[step: 541] loss: 0.6144419312477112\n",
      "[step: 542] loss: 0.6144024729728699\n",
      "[step: 543] loss: 0.6143631339073181\n",
      "[step: 544] loss: 0.6143240332603455\n",
      "[step: 545] loss: 0.6142849922180176\n",
      "[step: 546] loss: 0.6142462491989136\n",
      "[step: 547] loss: 0.6142072677612305\n",
      "[step: 548] loss: 0.6141687631607056\n",
      "[step: 549] loss: 0.6141303181648254\n",
      "[step: 550] loss: 0.6140916347503662\n",
      "[step: 551] loss: 0.6140537261962891\n",
      "[step: 552] loss: 0.6140153408050537\n",
      "[step: 553] loss: 0.6139776110649109\n",
      "[step: 554] loss: 0.6139400601387024\n",
      "[step: 555] loss: 0.6139020323753357\n",
      "[step: 556] loss: 0.6138646602630615\n",
      "[step: 557] loss: 0.6138273477554321\n",
      "[step: 558] loss: 0.6137903332710266\n",
      "[step: 559] loss: 0.6137534976005554\n",
      "[step: 560] loss: 0.6137161254882812\n",
      "[step: 561] loss: 0.613679051399231\n",
      "[step: 562] loss: 0.6136425733566284\n",
      "[step: 563] loss: 0.6136060357093811\n",
      "[step: 564] loss: 0.6135695576667786\n",
      "[step: 565] loss: 0.6135331988334656\n",
      "[step: 566] loss: 0.6134971976280212\n",
      "[step: 567] loss: 0.6134610176086426\n",
      "[step: 568] loss: 0.6134247183799744\n",
      "[step: 569] loss: 0.6133889555931091\n",
      "[step: 570] loss: 0.6133534908294678\n",
      "[step: 571] loss: 0.6133176684379578\n",
      "[step: 572] loss: 0.6132820844650269\n",
      "[step: 573] loss: 0.6132463216781616\n",
      "[step: 574] loss: 0.6132111549377441\n",
      "[step: 575] loss: 0.6131761074066162\n",
      "[step: 576] loss: 0.6131411790847778\n",
      "[step: 577] loss: 0.6131063103675842\n",
      "[step: 578] loss: 0.6130719184875488\n",
      "[step: 579] loss: 0.6130372881889343\n",
      "[step: 580] loss: 0.6130027174949646\n",
      "[step: 581] loss: 0.6129684448242188\n",
      "[step: 582] loss: 0.6129345893859863\n",
      "[step: 583] loss: 0.6129004955291748\n",
      "[step: 584] loss: 0.6128665208816528\n",
      "[step: 585] loss: 0.6128327250480652\n",
      "[step: 586] loss: 0.612798810005188\n",
      "[step: 587] loss: 0.6127653121948242\n",
      "[step: 588] loss: 0.6127315163612366\n",
      "[step: 589] loss: 0.612697958946228\n",
      "[step: 590] loss: 0.6126644611358643\n",
      "[step: 591] loss: 0.6126312613487244\n",
      "[step: 592] loss: 0.612598180770874\n",
      "[step: 593] loss: 0.6125655770301819\n",
      "[step: 594] loss: 0.6125325560569763\n",
      "[step: 595] loss: 0.6124999523162842\n",
      "[step: 596] loss: 0.6124670505523682\n",
      "[step: 597] loss: 0.6124342083930969\n",
      "[step: 598] loss: 0.6124022006988525\n",
      "[step: 599] loss: 0.6123701333999634\n",
      "[step: 600] loss: 0.6123382449150085\n",
      "[step: 601] loss: 0.6123064160346985\n",
      "[step: 602] loss: 0.6122746467590332\n",
      "[step: 603] loss: 0.6122432947158813\n",
      "[step: 604] loss: 0.6122115254402161\n",
      "[step: 605] loss: 0.6121797561645508\n",
      "[step: 606] loss: 0.6121485233306885\n",
      "[step: 607] loss: 0.6121169924736023\n",
      "[step: 608] loss: 0.6120861768722534\n",
      "[step: 609] loss: 0.6120548844337463\n",
      "[step: 610] loss: 0.6120239496231079\n",
      "[step: 611] loss: 0.6119934916496277\n",
      "[step: 612] loss: 0.6119628548622131\n",
      "[step: 613] loss: 0.6119322180747986\n",
      "[step: 614] loss: 0.6119016408920288\n",
      "[step: 615] loss: 0.6118714809417725\n",
      "[step: 616] loss: 0.611841082572937\n",
      "[step: 617] loss: 0.6118111610412598\n",
      "[step: 618] loss: 0.6117810606956482\n",
      "[step: 619] loss: 0.6117510795593262\n",
      "[step: 620] loss: 0.6117213368415833\n",
      "[step: 621] loss: 0.6116915345191956\n",
      "[step: 622] loss: 0.6116617321968079\n",
      "[step: 623] loss: 0.6116324067115784\n",
      "[step: 624] loss: 0.6116029024124146\n",
      "[step: 625] loss: 0.6115736365318298\n",
      "[step: 626] loss: 0.611544668674469\n",
      "[step: 627] loss: 0.6115156412124634\n",
      "[step: 628] loss: 0.6114866733551025\n",
      "[step: 629] loss: 0.6114580035209656\n",
      "[step: 630] loss: 0.6114290356636047\n",
      "[step: 631] loss: 0.6114004254341125\n",
      "[step: 632] loss: 0.6113719940185547\n",
      "[step: 633] loss: 0.611343502998352\n",
      "[step: 634] loss: 0.6113152503967285\n",
      "[step: 635] loss: 0.6112866997718811\n",
      "[step: 636] loss: 0.6112590432167053\n",
      "[step: 637] loss: 0.6112309694290161\n",
      "[step: 638] loss: 0.611203134059906\n",
      "[step: 639] loss: 0.6111752390861511\n",
      "[step: 640] loss: 0.611147403717041\n",
      "[step: 641] loss: 0.6111201047897339\n",
      "[step: 642] loss: 0.6110925078392029\n",
      "[step: 643] loss: 0.6110652089118958\n",
      "[step: 644] loss: 0.6110376119613647\n",
      "[step: 645] loss: 0.6110103130340576\n",
      "[step: 646] loss: 0.61098313331604\n",
      "[step: 647] loss: 0.6109561324119568\n",
      "[step: 648] loss: 0.6109292507171631\n",
      "[step: 649] loss: 0.6109023094177246\n",
      "[step: 650] loss: 0.6108757257461548\n",
      "[step: 651] loss: 0.6108493208885193\n",
      "[step: 652] loss: 0.6108230948448181\n",
      "[step: 653] loss: 0.6107965111732483\n",
      "[step: 654] loss: 0.6107701063156128\n",
      "[step: 655] loss: 0.6107439398765564\n",
      "[step: 656] loss: 0.6107178926467896\n",
      "[step: 657] loss: 0.6106919646263123\n",
      "[step: 658] loss: 0.6106657385826111\n",
      "[step: 659] loss: 0.6106400489807129\n",
      "[step: 660] loss: 0.6106142997741699\n",
      "[step: 661] loss: 0.6105886697769165\n",
      "[step: 662] loss: 0.6105630993843079\n",
      "[step: 663] loss: 0.6105378866195679\n",
      "[step: 664] loss: 0.6105126142501831\n",
      "[step: 665] loss: 0.6104872226715088\n",
      "[step: 666] loss: 0.6104617714881897\n",
      "[step: 667] loss: 0.6104366183280945\n",
      "[step: 668] loss: 0.6104119420051575\n",
      "[step: 669] loss: 0.6103867292404175\n",
      "[step: 670] loss: 0.6103621125221252\n",
      "[step: 671] loss: 0.6103375554084778\n",
      "[step: 672] loss: 0.610312819480896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 673] loss: 0.6102886199951172\n",
      "[step: 674] loss: 0.6102643013000488\n",
      "[step: 675] loss: 0.6102398037910461\n",
      "[step: 676] loss: 0.6102156639099121\n",
      "[step: 677] loss: 0.6101917028427124\n",
      "[step: 678] loss: 0.6101678609848022\n",
      "[step: 679] loss: 0.6101438403129578\n",
      "[step: 680] loss: 0.6101201772689819\n",
      "[step: 681] loss: 0.6100963354110718\n",
      "[step: 682] loss: 0.6100727319717407\n",
      "[step: 683] loss: 0.6100491285324097\n",
      "[step: 684] loss: 0.6100255846977234\n",
      "[step: 685] loss: 0.6100019812583923\n",
      "[step: 686] loss: 0.6099786758422852\n",
      "[step: 687] loss: 0.609955370426178\n",
      "[step: 688] loss: 0.6099321246147156\n",
      "[step: 689] loss: 0.6099090576171875\n",
      "[step: 690] loss: 0.6098862290382385\n",
      "[step: 691] loss: 0.6098632216453552\n",
      "[step: 692] loss: 0.609840452671051\n",
      "[step: 693] loss: 0.6098176836967468\n",
      "[step: 694] loss: 0.6097949147224426\n",
      "[step: 695] loss: 0.6097725033760071\n",
      "[step: 696] loss: 0.6097500324249268\n",
      "[step: 697] loss: 0.6097278594970703\n",
      "[step: 698] loss: 0.6097054481506348\n",
      "[step: 699] loss: 0.609683096408844\n",
      "[step: 700] loss: 0.6096608638763428\n",
      "[step: 701] loss: 0.6096389889717102\n",
      "[step: 702] loss: 0.6096172332763672\n",
      "[step: 703] loss: 0.6095954179763794\n",
      "[step: 704] loss: 0.6095735430717468\n",
      "[step: 705] loss: 0.6095517873764038\n",
      "[step: 706] loss: 0.6095300912857056\n",
      "[step: 707] loss: 0.6095086336135864\n",
      "[step: 708] loss: 0.6094869375228882\n",
      "[step: 709] loss: 0.6094658374786377\n",
      "[step: 710] loss: 0.6094448566436768\n",
      "[step: 711] loss: 0.6094236373901367\n",
      "[step: 712] loss: 0.6094025373458862\n",
      "[step: 713] loss: 0.6093814373016357\n",
      "[step: 714] loss: 0.6093603372573853\n",
      "[step: 715] loss: 0.6093394756317139\n",
      "[step: 716] loss: 0.6093189120292664\n",
      "[step: 717] loss: 0.6092979311943054\n",
      "[step: 718] loss: 0.6092768311500549\n",
      "[step: 719] loss: 0.6092565059661865\n",
      "[step: 720] loss: 0.609235942363739\n",
      "[step: 721] loss: 0.6092156171798706\n",
      "[step: 722] loss: 0.6091954708099365\n",
      "[step: 723] loss: 0.6091747879981995\n",
      "[step: 724] loss: 0.6091547608375549\n",
      "[step: 725] loss: 0.6091344952583313\n",
      "[step: 726] loss: 0.6091143488883972\n",
      "[step: 727] loss: 0.6090940237045288\n",
      "[step: 728] loss: 0.6090741157531738\n",
      "[step: 729] loss: 0.6090546250343323\n",
      "[step: 730] loss: 0.6090350151062012\n",
      "[step: 731] loss: 0.6090152263641357\n",
      "[step: 732] loss: 0.608995795249939\n",
      "[step: 733] loss: 0.608976423740387\n",
      "[step: 734] loss: 0.6089568734169006\n",
      "[step: 735] loss: 0.6089375019073486\n",
      "[step: 736] loss: 0.6089181303977966\n",
      "[step: 737] loss: 0.6088989973068237\n",
      "[step: 738] loss: 0.608879804611206\n",
      "[step: 739] loss: 0.6088607311248779\n",
      "[step: 740] loss: 0.608841598033905\n",
      "[step: 741] loss: 0.6088227033615112\n",
      "[step: 742] loss: 0.6088036894798279\n",
      "[step: 743] loss: 0.6087851524353027\n",
      "[step: 744] loss: 0.6087664365768433\n",
      "[step: 745] loss: 0.6087477803230286\n",
      "[step: 746] loss: 0.608729362487793\n",
      "[step: 747] loss: 0.608710765838623\n",
      "[step: 748] loss: 0.6086921691894531\n",
      "[step: 749] loss: 0.6086740493774414\n",
      "[step: 750] loss: 0.6086556315422058\n",
      "[step: 751] loss: 0.6086375117301941\n",
      "[step: 752] loss: 0.6086193323135376\n",
      "[step: 753] loss: 0.6086011528968811\n",
      "[step: 754] loss: 0.608583390712738\n",
      "[step: 755] loss: 0.6085655093193054\n",
      "[step: 756] loss: 0.6085479855537415\n",
      "[step: 757] loss: 0.6085298657417297\n",
      "[step: 758] loss: 0.6085119843482971\n",
      "[step: 759] loss: 0.6084943413734436\n",
      "[step: 760] loss: 0.6084769368171692\n",
      "[step: 761] loss: 0.6084590554237366\n",
      "[step: 762] loss: 0.6084415912628174\n",
      "[step: 763] loss: 0.6084240674972534\n",
      "[step: 764] loss: 0.6084067821502686\n",
      "[step: 765] loss: 0.6083894968032837\n",
      "[step: 766] loss: 0.6083724498748779\n",
      "[step: 767] loss: 0.6083555221557617\n",
      "[step: 768] loss: 0.608338475227356\n",
      "[step: 769] loss: 0.6083210110664368\n",
      "[step: 770] loss: 0.6083043217658997\n",
      "[step: 771] loss: 0.6082875728607178\n",
      "[step: 772] loss: 0.6082707047462463\n",
      "[step: 773] loss: 0.6082539558410645\n",
      "[step: 774] loss: 0.6082372069358826\n",
      "[step: 775] loss: 0.6082205772399902\n",
      "[step: 776] loss: 0.6082038879394531\n",
      "[step: 777] loss: 0.6081873178482056\n",
      "[step: 778] loss: 0.6081709265708923\n",
      "[step: 779] loss: 0.6081543564796448\n",
      "[step: 780] loss: 0.6081382632255554\n",
      "[step: 781] loss: 0.608121931552887\n",
      "[step: 782] loss: 0.6081059575080872\n",
      "[step: 783] loss: 0.6080898642539978\n",
      "[step: 784] loss: 0.6080736517906189\n",
      "[step: 785] loss: 0.6080577373504639\n",
      "[step: 786] loss: 0.6080417633056641\n",
      "[step: 787] loss: 0.608025848865509\n",
      "[step: 788] loss: 0.6080099940299988\n",
      "[step: 789] loss: 0.6079941987991333\n",
      "[step: 790] loss: 0.6079785227775574\n",
      "[step: 791] loss: 0.6079630255699158\n",
      "[step: 792] loss: 0.6079477667808533\n",
      "[step: 793] loss: 0.6079320907592773\n",
      "[step: 794] loss: 0.6079168319702148\n",
      "[step: 795] loss: 0.607901394367218\n",
      "[step: 796] loss: 0.607886016368866\n",
      "[step: 797] loss: 0.6078709363937378\n",
      "[step: 798] loss: 0.6078555583953857\n",
      "[step: 799] loss: 0.607840359210968\n",
      "[step: 800] loss: 0.607825517654419\n",
      "[step: 801] loss: 0.6078106164932251\n",
      "[step: 802] loss: 0.6077958345413208\n",
      "[step: 803] loss: 0.6077811121940613\n",
      "[step: 804] loss: 0.6077661514282227\n",
      "[step: 805] loss: 0.6077513098716736\n",
      "[step: 806] loss: 0.6077364087104797\n",
      "[step: 807] loss: 0.6077218651771545\n",
      "[step: 808] loss: 0.6077072620391846\n",
      "[step: 809] loss: 0.6076926589012146\n",
      "[step: 810] loss: 0.607678234577179\n",
      "[step: 811] loss: 0.6076636910438538\n",
      "[step: 812] loss: 0.6076495051383972\n",
      "[step: 813] loss: 0.6076349020004272\n",
      "[step: 814] loss: 0.6076207756996155\n",
      "[step: 815] loss: 0.6076065897941589\n",
      "[step: 816] loss: 0.6075922846794128\n",
      "[step: 817] loss: 0.6075782179832458\n",
      "[step: 818] loss: 0.6075639724731445\n",
      "[step: 819] loss: 0.6075500249862671\n",
      "[step: 820] loss: 0.6075363159179688\n",
      "[step: 821] loss: 0.6075223088264465\n",
      "[step: 822] loss: 0.6075083017349243\n",
      "[step: 823] loss: 0.6074946522712708\n",
      "[step: 824] loss: 0.6074809432029724\n",
      "[step: 825] loss: 0.6074674725532532\n",
      "[step: 826] loss: 0.6074538230895996\n",
      "[step: 827] loss: 0.6074403524398804\n",
      "[step: 828] loss: 0.6074270009994507\n",
      "[step: 829] loss: 0.6074135303497314\n",
      "[step: 830] loss: 0.6073998212814331\n",
      "[step: 831] loss: 0.6073865294456482\n",
      "[step: 832] loss: 0.6073729991912842\n",
      "[step: 833] loss: 0.6073598265647888\n",
      "[step: 834] loss: 0.6073468327522278\n",
      "[step: 835] loss: 0.6073338389396667\n",
      "[step: 836] loss: 0.6073208451271057\n",
      "[step: 837] loss: 0.6073077917098999\n",
      "[step: 838] loss: 0.6072946786880493\n",
      "[step: 839] loss: 0.6072816252708435\n",
      "[step: 840] loss: 0.6072689890861511\n",
      "[step: 841] loss: 0.6072559952735901\n",
      "[step: 842] loss: 0.6072432398796082\n",
      "[step: 843] loss: 0.6072303652763367\n",
      "[step: 844] loss: 0.6072178483009338\n",
      "[step: 845] loss: 0.6072054505348206\n",
      "[step: 846] loss: 0.6071929931640625\n",
      "[step: 847] loss: 0.6071803569793701\n",
      "[step: 848] loss: 0.6071677207946777\n",
      "[step: 849] loss: 0.6071553230285645\n",
      "[step: 850] loss: 0.6071426868438721\n",
      "[step: 851] loss: 0.6071303486824036\n",
      "[step: 852] loss: 0.6071180105209351\n",
      "[step: 853] loss: 0.6071059107780457\n",
      "[step: 854] loss: 0.6070936322212219\n",
      "[step: 855] loss: 0.6070813536643982\n",
      "[step: 856] loss: 0.6070690751075745\n",
      "[step: 857] loss: 0.6070571541786194\n",
      "[step: 858] loss: 0.6070452332496643\n",
      "[step: 859] loss: 0.6070331931114197\n",
      "[step: 860] loss: 0.6070213913917542\n",
      "[step: 861] loss: 0.6070096492767334\n",
      "[step: 862] loss: 0.606997549533844\n",
      "[step: 863] loss: 0.6069860458374023\n",
      "[step: 864] loss: 0.606974184513092\n",
      "[step: 865] loss: 0.6069624423980713\n",
      "[step: 866] loss: 0.6069509387016296\n",
      "[step: 867] loss: 0.6069393157958984\n",
      "[step: 868] loss: 0.6069278120994568\n",
      "[step: 869] loss: 0.6069163680076599\n",
      "[step: 870] loss: 0.6069048643112183\n",
      "[step: 871] loss: 0.6068937182426453\n",
      "[step: 872] loss: 0.6068822741508484\n",
      "[step: 873] loss: 0.6068710088729858\n",
      "[step: 874] loss: 0.6068595051765442\n",
      "[step: 875] loss: 0.6068483591079712\n",
      "[step: 876] loss: 0.6068371534347534\n",
      "[step: 877] loss: 0.6068260669708252\n",
      "[step: 878] loss: 0.606814980506897\n",
      "[step: 879] loss: 0.6068040728569031\n",
      "[step: 880] loss: 0.6067929267883301\n",
      "[step: 881] loss: 0.6067819595336914\n",
      "[step: 882] loss: 0.6067710518836975\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={\n",
    "                                X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    \n",
    "    test_predict = scaler2.inverse_transform(test_predict)\n",
    "    dataY2 = scaler2.inverse_transform(dataY2) # 데이터 있을때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "plt.title(data + ' predict')\n",
    "plt.plot(dataY2, label='raw data') # 데이터 있을때\n",
    "plt.plot(test_predict, label='predict')\n",
    "plt.xlabel(\"Time Period\")\n",
    "# plt.ylabel(\"photometric\")\n",
    "# plt.ylabel(\"cct\")\n",
    "plt.ylabel(\"CAS_SWR\")\n",
    "# plt.ylabel(\"446to477\")\n",
    "# plt.ylabel(\"UV-B\")\n",
    "plt.xticks(np.arange(0, 883, step = 56), ['5', '6', '7', '8', '9', '10', '11', '12', '1', '2', '3', '4', '5', '6', '7', '8']) # v2 모델\n",
    "# plt.yticks(np.arange(0, 140000, step=10000)) # ptmt\n",
    "# plt.yticks(np.arange(0, 60000, step=10000)) # cct\n",
    "# plt.yticks(np.arange(0, 2.0, step=0.2)) # uvb\n",
    "# plt.ylim(0, 100) #swr, 446to477\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
