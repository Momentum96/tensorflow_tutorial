{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "font_name = font_manager.FontProperties(fname=\"/usr/share/fonts/truetype/nanum/NanumGothic_Coding_Bold.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "%matplotlib tk\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Parameters\n",
    "seq_length = 9\n",
    "data_dim = 4\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 901\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler2 = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Open, High, Low, Volume, Close\n",
    "xy = np.loadtxt('./v2data/cct/train_v2_data.csv', delimiter=',')\n",
    "x = scaler.fit_transform(xy[:, 0:-1])\n",
    "y = scaler2.fit_transform(xy[:, [-1]])  # Close as label\n",
    "\n",
    "# build a dataset\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, len(x) - seq_length):\n",
    "    _x = x[i:i + seq_length]\n",
    "    _y = y[i + seq_length]  # Next close price\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "\n",
    "xy2 = np.loadtxt('./sun_data/2018-02-27.csv',delimiter=',')\n",
    "\n",
    "x2 = scaler.fit_transform(xy2) # 데이터 없을때\n",
    "# x2 = scaler.fit_transform(xy2[:, 0:-1]) # 데이터 있을때\n",
    "# y2 = scaler2.fit_transform(xy2[:, [-1]]) #데이터 있을때\n",
    "\n",
    "y2 = np.loadtxt('./v2data/cct/2018-02-27.csv',delimiter=',')\n",
    "\n",
    "dataX2 = []\n",
    "dataY2 = [] #데이터 있을때\n",
    "\n",
    "for i in range(0, len(x2) - seq_length):\n",
    "    _x2 = x2[i:i+seq_length]\n",
    "    _y2 = y2[i] #데이터 있을때\n",
    "    dataX2.append(_x2)\n",
    "    dataY2.append(_y2) #데이터 있을때\n",
    "# train/test split\n",
    "\n",
    "train_size = len(dataX)\n",
    "test_size = len(dataX2)\n",
    "\n",
    "trainX, testX = np.array(dataX[0:train_size]),np.array(dataX2[0:test_size])\n",
    "trainY = np.array(dataY[0:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gw/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 625.465576171875\n",
      "[step: 1] loss: 243.216064453125\n",
      "[step: 2] loss: 127.95773315429688\n",
      "[step: 3] loss: 151.13388061523438\n",
      "[step: 4] loss: 161.937744140625\n",
      "[step: 5] loss: 126.19656372070312\n",
      "[step: 6] loss: 76.03811645507812\n",
      "[step: 7] loss: 39.82243347167969\n",
      "[step: 8] loss: 27.3908748626709\n",
      "[step: 9] loss: 34.174259185791016\n",
      "[step: 10] loss: 48.08057403564453\n",
      "[step: 11] loss: 57.171363830566406\n",
      "[step: 12] loss: 56.395240783691406\n",
      "[step: 13] loss: 47.802181243896484\n",
      "[step: 14] loss: 36.51506423950195\n",
      "[step: 15] loss: 27.30362892150879\n",
      "[step: 16] loss: 22.87127685546875\n",
      "[step: 17] loss: 23.444726943969727\n",
      "[step: 18] loss: 27.228235244750977\n",
      "[step: 19] loss: 31.47844123840332\n",
      "[step: 20] loss: 33.84758758544922\n",
      "[step: 21] loss: 33.35789108276367\n",
      "[step: 22] loss: 30.517086029052734\n",
      "[step: 23] loss: 26.7100772857666\n",
      "[step: 24] loss: 23.387014389038086\n",
      "[step: 25] loss: 21.474365234375\n",
      "[step: 26] loss: 21.162368774414062\n",
      "[step: 27] loss: 22.0185604095459\n",
      "[step: 28] loss: 23.289947509765625\n",
      "[step: 29] loss: 24.244070053100586\n",
      "[step: 30] loss: 24.426977157592773\n",
      "[step: 31] loss: 23.7694034576416\n",
      "[step: 32] loss: 22.53668785095215\n",
      "[step: 33] loss: 21.170902252197266\n",
      "[step: 34] loss: 20.098615646362305\n",
      "[step: 35] loss: 19.575275421142578\n",
      "[step: 36] loss: 19.614665985107422\n",
      "[step: 37] loss: 20.018327713012695\n",
      "[step: 38] loss: 20.48311424255371\n",
      "[step: 39] loss: 20.736705780029297\n",
      "[step: 40] loss: 20.642807006835938\n",
      "[step: 41] loss: 20.235305786132812\n",
      "[step: 42] loss: 19.676528930664062\n",
      "[step: 43] loss: 19.16844367980957\n",
      "[step: 44] loss: 18.861852645874023\n",
      "[step: 45] loss: 18.801979064941406\n",
      "[step: 46] loss: 18.927003860473633\n",
      "[step: 47] loss: 19.111331939697266\n",
      "[step: 48] loss: 19.228469848632812\n",
      "[step: 49] loss: 19.204389572143555\n",
      "[step: 50] loss: 19.04060173034668\n",
      "[step: 51] loss: 18.801740646362305\n",
      "[step: 52] loss: 18.577783584594727\n",
      "[step: 53] loss: 18.440471649169922\n",
      "[step: 54] loss: 18.41352081298828\n",
      "[step: 55] loss: 18.468399047851562\n",
      "[step: 56] loss: 18.544769287109375\n",
      "[step: 57] loss: 18.58340072631836\n",
      "[step: 58] loss: 18.554250717163086\n",
      "[step: 59] loss: 18.466604232788086\n",
      "[step: 60] loss: 18.358325958251953\n",
      "[step: 61] loss: 18.272541046142578\n",
      "[step: 62] loss: 18.235248565673828\n",
      "[step: 63] loss: 18.244935989379883\n",
      "[step: 64] loss: 18.27776336669922\n",
      "[step: 65] loss: 18.303089141845703\n",
      "[step: 66] loss: 18.30006217956543\n",
      "[step: 67] loss: 18.26657485961914\n",
      "[step: 68] loss: 18.217260360717773\n",
      "[step: 69] loss: 18.17316436767578\n",
      "[step: 70] loss: 18.149658203125\n",
      "[step: 71] loss: 18.149154663085938\n",
      "[step: 72] loss: 18.161893844604492\n",
      "[step: 73] loss: 18.173095703125\n",
      "[step: 74] loss: 18.171810150146484\n",
      "[step: 75] loss: 18.156211853027344\n",
      "[step: 76] loss: 18.13309097290039\n",
      "[step: 77] loss: 18.112594604492188\n",
      "[step: 78] loss: 18.102001190185547\n",
      "[step: 79] loss: 18.102163314819336\n",
      "[step: 80] loss: 18.108074188232422\n",
      "[step: 81] loss: 18.11269760131836\n",
      "[step: 82] loss: 18.111207962036133\n",
      "[step: 83] loss: 18.10331153869629\n",
      "[step: 84] loss: 18.09259605407715\n",
      "[step: 85] loss: 18.08380889892578\n",
      "[step: 86] loss: 18.079879760742188\n",
      "[step: 87] loss: 18.08053207397461\n",
      "[step: 88] loss: 18.08295440673828\n",
      "[step: 89] loss: 18.0838680267334\n",
      "[step: 90] loss: 18.081478118896484\n",
      "[step: 91] loss: 18.076324462890625\n",
      "[step: 92] loss: 18.07051658630371\n",
      "[step: 93] loss: 18.066198348999023\n",
      "[step: 94] loss: 18.06427574157715\n",
      "[step: 95] loss: 18.064048767089844\n",
      "[step: 96] loss: 18.06393814086914\n",
      "[step: 97] loss: 18.062580108642578\n",
      "[step: 98] loss: 18.05963134765625\n",
      "[step: 99] loss: 18.055797576904297\n",
      "[step: 100] loss: 18.052221298217773\n",
      "[step: 101] loss: 18.049707412719727\n",
      "[step: 102] loss: 18.048274993896484\n",
      "[step: 103] loss: 18.047266006469727\n",
      "[step: 104] loss: 18.045913696289062\n",
      "[step: 105] loss: 18.043806076049805\n",
      "[step: 106] loss: 18.04112434387207\n",
      "[step: 107] loss: 18.03838539123535\n",
      "[step: 108] loss: 18.036094665527344\n",
      "[step: 109] loss: 18.03439712524414\n",
      "[step: 110] loss: 18.033035278320312\n",
      "[step: 111] loss: 18.031612396240234\n",
      "[step: 112] loss: 18.02985954284668\n",
      "[step: 113] loss: 18.027793884277344\n",
      "[step: 114] loss: 18.02565574645996\n",
      "[step: 115] loss: 18.023714065551758\n",
      "[step: 116] loss: 18.022066116333008\n",
      "[step: 117] loss: 18.020631790161133\n",
      "[step: 118] loss: 18.019210815429688\n",
      "[step: 119] loss: 18.017635345458984\n",
      "[step: 120] loss: 18.015914916992188\n",
      "[step: 121] loss: 18.014144897460938\n",
      "[step: 122] loss: 18.01246452331543\n",
      "[step: 123] loss: 18.010942459106445\n",
      "[step: 124] loss: 18.0095272064209\n",
      "[step: 125] loss: 18.008119583129883\n",
      "[step: 126] loss: 18.00664710998535\n",
      "[step: 127] loss: 18.00509262084961\n",
      "[step: 128] loss: 18.00351905822754\n",
      "[step: 129] loss: 18.00199317932129\n",
      "[step: 130] loss: 18.000537872314453\n",
      "[step: 131] loss: 17.9991397857666\n",
      "[step: 132] loss: 17.99774742126465\n",
      "[step: 133] loss: 17.996313095092773\n",
      "[step: 134] loss: 17.994848251342773\n",
      "[step: 135] loss: 17.993371963500977\n",
      "[step: 136] loss: 17.991924285888672\n",
      "[step: 137] loss: 17.99052619934082\n",
      "[step: 138] loss: 17.98914337158203\n",
      "[step: 139] loss: 17.987768173217773\n",
      "[step: 140] loss: 17.98636245727539\n",
      "[step: 141] loss: 17.984947204589844\n",
      "[step: 142] loss: 17.983535766601562\n",
      "[step: 143] loss: 17.982145309448242\n",
      "[step: 144] loss: 17.98076820373535\n",
      "[step: 145] loss: 17.979408264160156\n",
      "[step: 146] loss: 17.978038787841797\n",
      "[step: 147] loss: 17.976661682128906\n",
      "[step: 148] loss: 17.975278854370117\n",
      "[step: 149] loss: 17.97390365600586\n",
      "[step: 150] loss: 17.9725399017334\n",
      "[step: 151] loss: 17.97117805480957\n",
      "[step: 152] loss: 17.969825744628906\n",
      "[step: 153] loss: 17.968461990356445\n",
      "[step: 154] loss: 17.967100143432617\n",
      "[step: 155] loss: 17.965736389160156\n",
      "[step: 156] loss: 17.964370727539062\n",
      "[step: 157] loss: 17.963014602661133\n",
      "[step: 158] loss: 17.961658477783203\n",
      "[step: 159] loss: 17.960304260253906\n",
      "[step: 160] loss: 17.95894432067871\n",
      "[step: 161] loss: 17.957578659057617\n",
      "[step: 162] loss: 17.956220626831055\n",
      "[step: 163] loss: 17.954853057861328\n",
      "[step: 164] loss: 17.9534969329834\n",
      "[step: 165] loss: 17.952133178710938\n",
      "[step: 166] loss: 17.95077133178711\n",
      "[step: 167] loss: 17.94939422607422\n",
      "[step: 168] loss: 17.948022842407227\n",
      "[step: 169] loss: 17.946653366088867\n",
      "[step: 170] loss: 17.94527816772461\n",
      "[step: 171] loss: 17.94390106201172\n",
      "[step: 172] loss: 17.942523956298828\n",
      "[step: 173] loss: 17.941137313842773\n",
      "[step: 174] loss: 17.939754486083984\n",
      "[step: 175] loss: 17.938364028930664\n",
      "[step: 176] loss: 17.93697166442871\n",
      "[step: 177] loss: 17.935569763183594\n",
      "[step: 178] loss: 17.93417739868164\n",
      "[step: 179] loss: 17.93277359008789\n",
      "[step: 180] loss: 17.931360244750977\n",
      "[step: 181] loss: 17.929950714111328\n",
      "[step: 182] loss: 17.92853355407715\n",
      "[step: 183] loss: 17.92711067199707\n",
      "[step: 184] loss: 17.925683975219727\n",
      "[step: 185] loss: 17.924257278442383\n",
      "[step: 186] loss: 17.922821044921875\n",
      "[step: 187] loss: 17.92137908935547\n",
      "[step: 188] loss: 17.919933319091797\n",
      "[step: 189] loss: 17.91848373413086\n",
      "[step: 190] loss: 17.917030334472656\n",
      "[step: 191] loss: 17.915565490722656\n",
      "[step: 192] loss: 17.914104461669922\n",
      "[step: 193] loss: 17.912628173828125\n",
      "[step: 194] loss: 17.911151885986328\n",
      "[step: 195] loss: 17.909666061401367\n",
      "[step: 196] loss: 17.90817642211914\n",
      "[step: 197] loss: 17.906673431396484\n",
      "[step: 198] loss: 17.905170440673828\n",
      "[step: 199] loss: 17.90366554260254\n",
      "[step: 200] loss: 17.902149200439453\n",
      "[step: 201] loss: 17.90062713623047\n",
      "[step: 202] loss: 17.899097442626953\n",
      "[step: 203] loss: 17.897563934326172\n",
      "[step: 204] loss: 17.896015167236328\n",
      "[step: 205] loss: 17.894466400146484\n",
      "[step: 206] loss: 17.892908096313477\n",
      "[step: 207] loss: 17.891347885131836\n",
      "[step: 208] loss: 17.8897705078125\n",
      "[step: 209] loss: 17.8881893157959\n",
      "[step: 210] loss: 17.886611938476562\n",
      "[step: 211] loss: 17.8850154876709\n",
      "[step: 212] loss: 17.88340950012207\n",
      "[step: 213] loss: 17.881797790527344\n",
      "[step: 214] loss: 17.880184173583984\n",
      "[step: 215] loss: 17.878555297851562\n",
      "[step: 216] loss: 17.876922607421875\n",
      "[step: 217] loss: 17.87527847290039\n",
      "[step: 218] loss: 17.873626708984375\n",
      "[step: 219] loss: 17.87196922302246\n",
      "[step: 220] loss: 17.87030601501465\n",
      "[step: 221] loss: 17.868623733520508\n",
      "[step: 222] loss: 17.866943359375\n",
      "[step: 223] loss: 17.865251541137695\n",
      "[step: 224] loss: 17.863544464111328\n",
      "[step: 225] loss: 17.861833572387695\n",
      "[step: 226] loss: 17.860118865966797\n",
      "[step: 227] loss: 17.858388900756836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 228] loss: 17.856651306152344\n",
      "[step: 229] loss: 17.85490608215332\n",
      "[step: 230] loss: 17.8531494140625\n",
      "[step: 231] loss: 17.851383209228516\n",
      "[step: 232] loss: 17.849611282348633\n",
      "[step: 233] loss: 17.84782600402832\n",
      "[step: 234] loss: 17.846033096313477\n",
      "[step: 235] loss: 17.844228744506836\n",
      "[step: 236] loss: 17.8424129486084\n",
      "[step: 237] loss: 17.840593338012695\n",
      "[step: 238] loss: 17.838764190673828\n",
      "[step: 239] loss: 17.836925506591797\n",
      "[step: 240] loss: 17.835073471069336\n",
      "[step: 241] loss: 17.833209991455078\n",
      "[step: 242] loss: 17.83133888244629\n",
      "[step: 243] loss: 17.829458236694336\n",
      "[step: 244] loss: 17.827571868896484\n",
      "[step: 245] loss: 17.825666427612305\n",
      "[step: 246] loss: 17.823759078979492\n",
      "[step: 247] loss: 17.82183837890625\n",
      "[step: 248] loss: 17.819910049438477\n",
      "[step: 249] loss: 17.817970275878906\n",
      "[step: 250] loss: 17.816022872924805\n",
      "[step: 251] loss: 17.81406021118164\n",
      "[step: 252] loss: 17.812091827392578\n",
      "[step: 253] loss: 17.81011199951172\n",
      "[step: 254] loss: 17.808120727539062\n",
      "[step: 255] loss: 17.80611801147461\n",
      "[step: 256] loss: 17.80411148071289\n",
      "[step: 257] loss: 17.80208969116211\n",
      "[step: 258] loss: 17.80006217956543\n",
      "[step: 259] loss: 17.798017501831055\n",
      "[step: 260] loss: 17.795974731445312\n",
      "[step: 261] loss: 17.793914794921875\n",
      "[step: 262] loss: 17.791847229003906\n",
      "[step: 263] loss: 17.789770126342773\n",
      "[step: 264] loss: 17.78767967224121\n",
      "[step: 265] loss: 17.785585403442383\n",
      "[step: 266] loss: 17.783475875854492\n",
      "[step: 267] loss: 17.781360626220703\n",
      "[step: 268] loss: 17.779233932495117\n",
      "[step: 269] loss: 17.777095794677734\n",
      "[step: 270] loss: 17.77495574951172\n",
      "[step: 271] loss: 17.77280044555664\n",
      "[step: 272] loss: 17.77063751220703\n",
      "[step: 273] loss: 17.768468856811523\n",
      "[step: 274] loss: 17.76629066467285\n",
      "[step: 275] loss: 17.764102935791016\n",
      "[step: 276] loss: 17.761905670166016\n",
      "[step: 277] loss: 17.759700775146484\n",
      "[step: 278] loss: 17.757488250732422\n",
      "[step: 279] loss: 17.755266189575195\n",
      "[step: 280] loss: 17.753042221069336\n",
      "[step: 281] loss: 17.750812530517578\n",
      "[step: 282] loss: 17.748571395874023\n",
      "[step: 283] loss: 17.74631690979004\n",
      "[step: 284] loss: 17.744064331054688\n",
      "[step: 285] loss: 17.741804122924805\n",
      "[step: 286] loss: 17.73953628540039\n",
      "[step: 287] loss: 17.737260818481445\n",
      "[step: 288] loss: 17.734981536865234\n",
      "[step: 289] loss: 17.732698440551758\n",
      "[step: 290] loss: 17.730409622192383\n",
      "[step: 291] loss: 17.72811508178711\n",
      "[step: 292] loss: 17.725812911987305\n",
      "[step: 293] loss: 17.723508834838867\n",
      "[step: 294] loss: 17.721202850341797\n",
      "[step: 295] loss: 17.718891143798828\n",
      "[step: 296] loss: 17.716575622558594\n",
      "[step: 297] loss: 17.71426010131836\n",
      "[step: 298] loss: 17.711933135986328\n",
      "[step: 299] loss: 17.70961570739746\n",
      "[step: 300] loss: 17.70728874206543\n",
      "[step: 301] loss: 17.70496368408203\n",
      "[step: 302] loss: 17.702638626098633\n",
      "[step: 303] loss: 17.700315475463867\n",
      "[step: 304] loss: 17.697980880737305\n",
      "[step: 305] loss: 17.69565200805664\n",
      "[step: 306] loss: 17.693326950073242\n",
      "[step: 307] loss: 17.691001892089844\n",
      "[step: 308] loss: 17.688678741455078\n",
      "[step: 309] loss: 17.686351776123047\n",
      "[step: 310] loss: 17.68402671813965\n",
      "[step: 311] loss: 17.681711196899414\n",
      "[step: 312] loss: 17.679397583007812\n",
      "[step: 313] loss: 17.677085876464844\n",
      "[step: 314] loss: 17.674776077270508\n",
      "[step: 315] loss: 17.67247200012207\n",
      "[step: 316] loss: 17.670175552368164\n",
      "[step: 317] loss: 17.66788101196289\n",
      "[step: 318] loss: 17.665592193603516\n",
      "[step: 319] loss: 17.663311004638672\n",
      "[step: 320] loss: 17.66103744506836\n",
      "[step: 321] loss: 17.65876579284668\n",
      "[step: 322] loss: 17.656503677368164\n",
      "[step: 323] loss: 17.654254913330078\n",
      "[step: 324] loss: 17.652008056640625\n",
      "[step: 325] loss: 17.649770736694336\n",
      "[step: 326] loss: 17.647541046142578\n",
      "[step: 327] loss: 17.645326614379883\n",
      "[step: 328] loss: 17.643117904663086\n",
      "[step: 329] loss: 17.640918731689453\n",
      "[step: 330] loss: 17.638734817504883\n",
      "[step: 331] loss: 17.636558532714844\n",
      "[step: 332] loss: 17.634389877319336\n",
      "[step: 333] loss: 17.632238388061523\n",
      "[step: 334] loss: 17.630098342895508\n",
      "[step: 335] loss: 17.627971649169922\n",
      "[step: 336] loss: 17.62584686279297\n",
      "[step: 337] loss: 17.623743057250977\n",
      "[step: 338] loss: 17.621652603149414\n",
      "[step: 339] loss: 17.61957550048828\n",
      "[step: 340] loss: 17.617515563964844\n",
      "[step: 341] loss: 17.615461349487305\n",
      "[step: 342] loss: 17.61342430114746\n",
      "[step: 343] loss: 17.611400604248047\n",
      "[step: 344] loss: 17.609394073486328\n",
      "[step: 345] loss: 17.60740089416504\n",
      "[step: 346] loss: 17.605417251586914\n",
      "[step: 347] loss: 17.603452682495117\n",
      "[step: 348] loss: 17.601503372192383\n",
      "[step: 349] loss: 17.599565505981445\n",
      "[step: 350] loss: 17.59764289855957\n",
      "[step: 351] loss: 17.595735549926758\n",
      "[step: 352] loss: 17.59384536743164\n",
      "[step: 353] loss: 17.59196662902832\n",
      "[step: 354] loss: 17.590103149414062\n",
      "[step: 355] loss: 17.5882568359375\n",
      "[step: 356] loss: 17.5864200592041\n",
      "[step: 357] loss: 17.58460235595703\n",
      "[step: 358] loss: 17.582792282104492\n",
      "[step: 359] loss: 17.58100128173828\n",
      "[step: 360] loss: 17.579221725463867\n",
      "[step: 361] loss: 17.577457427978516\n",
      "[step: 362] loss: 17.575708389282227\n",
      "[step: 363] loss: 17.57396697998047\n",
      "[step: 364] loss: 17.572240829467773\n",
      "[step: 365] loss: 17.570531845092773\n",
      "[step: 366] loss: 17.568830490112305\n",
      "[step: 367] loss: 17.567142486572266\n",
      "[step: 368] loss: 17.565465927124023\n",
      "[step: 369] loss: 17.563806533813477\n",
      "[step: 370] loss: 17.562150955200195\n",
      "[step: 371] loss: 17.560510635375977\n",
      "[step: 372] loss: 17.558879852294922\n",
      "[step: 373] loss: 17.5572566986084\n",
      "[step: 374] loss: 17.555648803710938\n",
      "[step: 375] loss: 17.554046630859375\n",
      "[step: 376] loss: 17.552452087402344\n",
      "[step: 377] loss: 17.55086898803711\n",
      "[step: 378] loss: 17.549291610717773\n",
      "[step: 379] loss: 17.547727584838867\n",
      "[step: 380] loss: 17.546167373657227\n",
      "[step: 381] loss: 17.54460906982422\n",
      "[step: 382] loss: 17.54306411743164\n",
      "[step: 383] loss: 17.54152488708496\n",
      "[step: 384] loss: 17.539993286132812\n",
      "[step: 385] loss: 17.538463592529297\n",
      "[step: 386] loss: 17.536943435668945\n",
      "[step: 387] loss: 17.535423278808594\n",
      "[step: 388] loss: 17.533906936645508\n",
      "[step: 389] loss: 17.532398223876953\n",
      "[step: 390] loss: 17.530885696411133\n",
      "[step: 391] loss: 17.52937889099121\n",
      "[step: 392] loss: 17.527875900268555\n",
      "[step: 393] loss: 17.526376724243164\n",
      "[step: 394] loss: 17.524873733520508\n",
      "[step: 395] loss: 17.52337646484375\n",
      "[step: 396] loss: 17.52187728881836\n",
      "[step: 397] loss: 17.52037811279297\n",
      "[step: 398] loss: 17.518875122070312\n",
      "[step: 399] loss: 17.517379760742188\n",
      "[step: 400] loss: 17.515872955322266\n",
      "[step: 401] loss: 17.514368057250977\n",
      "[step: 402] loss: 17.512859344482422\n",
      "[step: 403] loss: 17.511350631713867\n",
      "[step: 404] loss: 17.509836196899414\n",
      "[step: 405] loss: 17.508319854736328\n",
      "[step: 406] loss: 17.50679588317871\n",
      "[step: 407] loss: 17.505271911621094\n",
      "[step: 408] loss: 17.503738403320312\n",
      "[step: 409] loss: 17.502199172973633\n",
      "[step: 410] loss: 17.50065803527832\n",
      "[step: 411] loss: 17.499101638793945\n",
      "[step: 412] loss: 17.49753761291504\n",
      "[step: 413] loss: 17.495969772338867\n",
      "[step: 414] loss: 17.494396209716797\n",
      "[step: 415] loss: 17.492813110351562\n",
      "[step: 416] loss: 17.491214752197266\n",
      "[step: 417] loss: 17.48960304260254\n",
      "[step: 418] loss: 17.487991333007812\n",
      "[step: 419] loss: 17.486360549926758\n",
      "[step: 420] loss: 17.484718322753906\n",
      "[step: 421] loss: 17.483068466186523\n",
      "[step: 422] loss: 17.481399536132812\n",
      "[step: 423] loss: 17.479721069335938\n",
      "[step: 424] loss: 17.4780216217041\n",
      "[step: 425] loss: 17.4763126373291\n",
      "[step: 426] loss: 17.474584579467773\n",
      "[step: 427] loss: 17.472841262817383\n",
      "[step: 428] loss: 17.471086502075195\n",
      "[step: 429] loss: 17.469301223754883\n",
      "[step: 430] loss: 17.467506408691406\n",
      "[step: 431] loss: 17.465688705444336\n",
      "[step: 432] loss: 17.463851928710938\n",
      "[step: 433] loss: 17.461992263793945\n",
      "[step: 434] loss: 17.460113525390625\n",
      "[step: 435] loss: 17.45821189880371\n",
      "[step: 436] loss: 17.456289291381836\n",
      "[step: 437] loss: 17.454330444335938\n",
      "[step: 438] loss: 17.45235824584961\n",
      "[step: 439] loss: 17.450355529785156\n",
      "[step: 440] loss: 17.448326110839844\n",
      "[step: 441] loss: 17.44626808166504\n",
      "[step: 442] loss: 17.444181442260742\n",
      "[step: 443] loss: 17.442060470581055\n",
      "[step: 444] loss: 17.439912796020508\n",
      "[step: 445] loss: 17.437728881835938\n",
      "[step: 446] loss: 17.435514450073242\n",
      "[step: 447] loss: 17.433263778686523\n",
      "[step: 448] loss: 17.430978775024414\n",
      "[step: 449] loss: 17.428653717041016\n",
      "[step: 450] loss: 17.426288604736328\n",
      "[step: 451] loss: 17.423885345458984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 452] loss: 17.421443939208984\n",
      "[step: 453] loss: 17.418956756591797\n",
      "[step: 454] loss: 17.416423797607422\n",
      "[step: 455] loss: 17.413843154907227\n",
      "[step: 456] loss: 17.41122055053711\n",
      "[step: 457] loss: 17.40854835510254\n",
      "[step: 458] loss: 17.405826568603516\n",
      "[step: 459] loss: 17.403047561645508\n",
      "[step: 460] loss: 17.400217056274414\n",
      "[step: 461] loss: 17.3973331451416\n",
      "[step: 462] loss: 17.39438819885254\n",
      "[step: 463] loss: 17.391387939453125\n",
      "[step: 464] loss: 17.38832664489746\n",
      "[step: 465] loss: 17.385196685791016\n",
      "[step: 466] loss: 17.382009506225586\n",
      "[step: 467] loss: 17.37874984741211\n",
      "[step: 468] loss: 17.375423431396484\n",
      "[step: 469] loss: 17.372032165527344\n",
      "[step: 470] loss: 17.368566513061523\n",
      "[step: 471] loss: 17.365026473999023\n",
      "[step: 472] loss: 17.361412048339844\n",
      "[step: 473] loss: 17.357717514038086\n",
      "[step: 474] loss: 17.353944778442383\n",
      "[step: 475] loss: 17.350095748901367\n",
      "[step: 476] loss: 17.346158981323242\n",
      "[step: 477] loss: 17.342138290405273\n",
      "[step: 478] loss: 17.338035583496094\n",
      "[step: 479] loss: 17.333837509155273\n",
      "[step: 480] loss: 17.329559326171875\n",
      "[step: 481] loss: 17.32517433166504\n",
      "[step: 482] loss: 17.320703506469727\n",
      "[step: 483] loss: 17.31613540649414\n",
      "[step: 484] loss: 17.31147575378418\n",
      "[step: 485] loss: 17.306703567504883\n",
      "[step: 486] loss: 17.30182647705078\n",
      "[step: 487] loss: 17.29684829711914\n",
      "[step: 488] loss: 17.291751861572266\n",
      "[step: 489] loss: 17.286540985107422\n",
      "[step: 490] loss: 17.281211853027344\n",
      "[step: 491] loss: 17.2757511138916\n",
      "[step: 492] loss: 17.270164489746094\n",
      "[step: 493] loss: 17.264436721801758\n",
      "[step: 494] loss: 17.258563995361328\n",
      "[step: 495] loss: 17.25254249572754\n",
      "[step: 496] loss: 17.246362686157227\n",
      "[step: 497] loss: 17.24001121520996\n",
      "[step: 498] loss: 17.233482360839844\n",
      "[step: 499] loss: 17.22677230834961\n",
      "[step: 500] loss: 17.21985626220703\n",
      "[step: 501] loss: 17.212738037109375\n",
      "[step: 502] loss: 17.205392837524414\n",
      "[step: 503] loss: 17.19781494140625\n",
      "[step: 504] loss: 17.18998908996582\n",
      "[step: 505] loss: 17.181901931762695\n",
      "[step: 506] loss: 17.173538208007812\n",
      "[step: 507] loss: 17.164871215820312\n",
      "[step: 508] loss: 17.155902862548828\n",
      "[step: 509] loss: 17.146596908569336\n",
      "[step: 510] loss: 17.136940002441406\n",
      "[step: 511] loss: 17.126914978027344\n",
      "[step: 512] loss: 17.11648941040039\n",
      "[step: 513] loss: 17.10564422607422\n",
      "[step: 514] loss: 17.094348907470703\n",
      "[step: 515] loss: 17.08258628845215\n",
      "[step: 516] loss: 17.07032012939453\n",
      "[step: 517] loss: 17.05752944946289\n",
      "[step: 518] loss: 17.044178009033203\n",
      "[step: 519] loss: 17.030241012573242\n",
      "[step: 520] loss: 17.01569175720215\n",
      "[step: 521] loss: 17.000497817993164\n",
      "[step: 522] loss: 16.984634399414062\n",
      "[step: 523] loss: 16.96807289123535\n",
      "[step: 524] loss: 16.950788497924805\n",
      "[step: 525] loss: 16.93276596069336\n",
      "[step: 526] loss: 16.913978576660156\n",
      "[step: 527] loss: 16.8944034576416\n",
      "[step: 528] loss: 16.87403106689453\n",
      "[step: 529] loss: 16.85284996032715\n",
      "[step: 530] loss: 16.83084487915039\n",
      "[step: 531] loss: 16.808012008666992\n",
      "[step: 532] loss: 16.784351348876953\n",
      "[step: 533] loss: 16.759864807128906\n",
      "[step: 534] loss: 16.73454475402832\n",
      "[step: 535] loss: 16.708417892456055\n",
      "[step: 536] loss: 16.681503295898438\n",
      "[step: 537] loss: 16.653810501098633\n",
      "[step: 538] loss: 16.62538719177246\n",
      "[step: 539] loss: 16.596248626708984\n",
      "[step: 540] loss: 16.56644058227539\n",
      "[step: 541] loss: 16.535991668701172\n",
      "[step: 542] loss: 16.50493812561035\n",
      "[step: 543] loss: 16.473281860351562\n",
      "[step: 544] loss: 16.441041946411133\n",
      "[step: 545] loss: 16.408218383789062\n",
      "[step: 546] loss: 16.37481117248535\n",
      "[step: 547] loss: 16.340808868408203\n",
      "[step: 548] loss: 16.306259155273438\n",
      "[step: 549] loss: 16.271215438842773\n",
      "[step: 550] loss: 16.235803604125977\n",
      "[step: 551] loss: 16.200204849243164\n",
      "[step: 552] loss: 16.16466522216797\n",
      "[step: 553] loss: 16.12946319580078\n",
      "[step: 554] loss: 16.094867706298828\n",
      "[step: 555] loss: 16.061073303222656\n",
      "[step: 556] loss: 16.02815055847168\n",
      "[step: 557] loss: 15.996016502380371\n",
      "[step: 558] loss: 15.964420318603516\n",
      "[step: 559] loss: 15.933002471923828\n",
      "[step: 560] loss: 15.901378631591797\n",
      "[step: 561] loss: 15.869226455688477\n",
      "[step: 562] loss: 15.836392402648926\n",
      "[step: 563] loss: 15.802905082702637\n",
      "[step: 564] loss: 15.768941879272461\n",
      "[step: 565] loss: 15.73476505279541\n",
      "[step: 566] loss: 15.700637817382812\n",
      "[step: 567] loss: 15.666711807250977\n",
      "[step: 568] loss: 15.633003234863281\n",
      "[step: 569] loss: 15.599366188049316\n",
      "[step: 570] loss: 15.565550804138184\n",
      "[step: 571] loss: 15.531267166137695\n",
      "[step: 572] loss: 15.49626350402832\n",
      "[step: 573] loss: 15.460379600524902\n",
      "[step: 574] loss: 15.423568725585938\n",
      "[step: 575] loss: 15.385883331298828\n",
      "[step: 576] loss: 15.347373008728027\n",
      "[step: 577] loss: 15.308045387268066\n",
      "[step: 578] loss: 15.267793655395508\n",
      "[step: 579] loss: 15.226425170898438\n",
      "[step: 580] loss: 15.183677673339844\n",
      "[step: 581] loss: 15.13929557800293\n",
      "[step: 582] loss: 15.093106269836426\n",
      "[step: 583] loss: 15.04504108428955\n",
      "[step: 584] loss: 14.995070457458496\n",
      "[step: 585] loss: 14.943151473999023\n",
      "[step: 586] loss: 14.889154434204102\n",
      "[step: 587] loss: 14.83294677734375\n",
      "[step: 588] loss: 14.77447509765625\n",
      "[step: 589] loss: 14.713821411132812\n",
      "[step: 590] loss: 14.651190757751465\n",
      "[step: 591] loss: 14.586808204650879\n",
      "[step: 592] loss: 14.521068572998047\n",
      "[step: 593] loss: 14.454726219177246\n",
      "[step: 594] loss: 14.389168739318848\n",
      "[step: 595] loss: 14.327434539794922\n",
      "[step: 596] loss: 14.27985954284668\n",
      "[step: 597] loss: 14.29727554321289\n",
      "[step: 598] loss: 14.525818824768066\n",
      "[step: 599] loss: 15.17332935333252\n",
      "[step: 600] loss: 15.642773628234863\n",
      "[step: 601] loss: 16.02823257446289\n",
      "[step: 602] loss: 15.279869079589844\n",
      "[step: 603] loss: 14.544968605041504\n",
      "[step: 604] loss: 14.93643569946289\n",
      "[step: 605] loss: 14.485519409179688\n",
      "[step: 606] loss: 14.233857154846191\n",
      "[step: 607] loss: 14.713061332702637\n",
      "[step: 608] loss: 14.033955574035645\n",
      "[step: 609] loss: 14.284078598022461\n",
      "[step: 610] loss: 14.35861587524414\n",
      "[step: 611] loss: 13.930550575256348\n",
      "[step: 612] loss: 14.454834938049316\n",
      "[step: 613] loss: 13.818195343017578\n",
      "[step: 614] loss: 14.155641555786133\n",
      "[step: 615] loss: 13.876790046691895\n",
      "[step: 616] loss: 13.820822715759277\n",
      "[step: 617] loss: 13.971063613891602\n",
      "[step: 618] loss: 13.681282043457031\n",
      "[step: 619] loss: 13.916095733642578\n",
      "[step: 620] loss: 13.668896675109863\n",
      "[step: 621] loss: 13.700003623962402\n",
      "[step: 622] loss: 13.683262825012207\n",
      "[step: 623] loss: 13.560734748840332\n",
      "[step: 624] loss: 13.61400032043457\n",
      "[step: 625] loss: 13.490289688110352\n",
      "[step: 626] loss: 13.513686180114746\n",
      "[step: 627] loss: 13.485275268554688\n",
      "[step: 628] loss: 13.408967018127441\n",
      "[step: 629] loss: 13.412881851196289\n",
      "[step: 630] loss: 13.380024909973145\n",
      "[step: 631] loss: 13.337448120117188\n",
      "[step: 632] loss: 13.299297332763672\n",
      "[step: 633] loss: 13.29261589050293\n",
      "[step: 634] loss: 13.26106071472168\n",
      "[step: 635] loss: 13.185853004455566\n",
      "[step: 636] loss: 13.199408531188965\n",
      "[step: 637] loss: 13.197528839111328\n",
      "[step: 638] loss: 13.11015510559082\n",
      "[step: 639] loss: 13.087422370910645\n",
      "[step: 640] loss: 13.091346740722656\n",
      "[step: 641] loss: 13.053730010986328\n",
      "[step: 642] loss: 13.025548934936523\n",
      "[step: 643] loss: 12.985060691833496\n",
      "[step: 644] loss: 12.944679260253906\n",
      "[step: 645] loss: 12.943489074707031\n",
      "[step: 646] loss: 12.925606727600098\n",
      "[step: 647] loss: 12.887187957763672\n",
      "[step: 648] loss: 12.859566688537598\n",
      "[step: 649] loss: 12.81795883178711\n",
      "[step: 650] loss: 12.779972076416016\n",
      "[step: 651] loss: 12.762887001037598\n",
      "[step: 652] loss: 12.73929500579834\n",
      "[step: 653] loss: 12.71808910369873\n",
      "[step: 654] loss: 12.71229076385498\n",
      "[step: 655] loss: 12.707831382751465\n",
      "[step: 656] loss: 12.71784496307373\n",
      "[step: 657] loss: 12.76463794708252\n",
      "[step: 658] loss: 12.844642639160156\n",
      "[step: 659] loss: 12.951874732971191\n",
      "[step: 660] loss: 12.999269485473633\n",
      "[step: 661] loss: 12.859980583190918\n",
      "[step: 662] loss: 12.615677833557129\n",
      "[step: 663] loss: 12.453454971313477\n",
      "[step: 664] loss: 12.44593620300293\n",
      "[step: 665] loss: 12.554619789123535\n",
      "[step: 666] loss: 12.676575660705566\n",
      "[step: 667] loss: 12.697771072387695\n",
      "[step: 668] loss: 12.545968055725098\n",
      "[step: 669] loss: 12.35501766204834\n",
      "[step: 670] loss: 12.26564884185791\n",
      "[step: 671] loss: 12.308597564697266\n",
      "[step: 672] loss: 12.433248519897461\n",
      "[step: 673] loss: 12.550533294677734\n",
      "[step: 674] loss: 12.592366218566895\n",
      "[step: 675] loss: 12.434905052185059\n",
      "[step: 676] loss: 12.212145805358887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 677] loss: 12.08152961730957\n",
      "[step: 678] loss: 12.102556228637695\n",
      "[step: 679] loss: 12.22868537902832\n",
      "[step: 680] loss: 12.384658813476562\n",
      "[step: 681] loss: 12.494133949279785\n",
      "[step: 682] loss: 12.322273254394531\n",
      "[step: 683] loss: 12.039268493652344\n",
      "[step: 684] loss: 11.888249397277832\n",
      "[step: 685] loss: 11.95478343963623\n",
      "[step: 686] loss: 12.156081199645996\n",
      "[step: 687] loss: 12.309076309204102\n",
      "[step: 688] loss: 12.30502986907959\n",
      "[step: 689] loss: 11.984086990356445\n",
      "[step: 690] loss: 11.730917930603027\n",
      "[step: 691] loss: 11.75709342956543\n",
      "[step: 692] loss: 11.992030143737793\n",
      "[step: 693] loss: 12.252853393554688\n",
      "[step: 694] loss: 12.122190475463867\n",
      "[step: 695] loss: 11.793021202087402\n",
      "[step: 696] loss: 11.552664756774902\n",
      "[step: 697] loss: 11.65514850616455\n",
      "[step: 698] loss: 11.976968765258789\n",
      "[step: 699] loss: 12.015149116516113\n",
      "[step: 700] loss: 11.822999954223633\n",
      "[step: 701] loss: 11.47487735748291\n",
      "[step: 702] loss: 11.384262084960938\n",
      "[step: 703] loss: 11.56360149383545\n",
      "[step: 704] loss: 11.717402458190918\n",
      "[step: 705] loss: 11.72532844543457\n",
      "[step: 706] loss: 11.420417785644531\n",
      "[step: 707] loss: 11.19817066192627\n",
      "[step: 708] loss: 11.15717601776123\n",
      "[step: 709] loss: 11.28207778930664\n",
      "[step: 710] loss: 11.481061935424805\n",
      "[step: 711] loss: 11.432476043701172\n",
      "[step: 712] loss: 11.315524101257324\n",
      "[step: 713] loss: 11.038834571838379\n",
      "[step: 714] loss: 10.88778305053711\n",
      "[step: 715] loss: 10.97583293914795\n",
      "[step: 716] loss: 11.1776704788208\n",
      "[step: 717] loss: 11.481368064880371\n",
      "[step: 718] loss: 11.343006134033203\n",
      "[step: 719] loss: 10.986228942871094\n",
      "[step: 720] loss: 10.670635223388672\n",
      "[step: 721] loss: 10.816061973571777\n",
      "[step: 722] loss: 11.29096508026123\n",
      "[step: 723] loss: 11.456647872924805\n",
      "[step: 724] loss: 11.360162734985352\n",
      "[step: 725] loss: 10.672204971313477\n",
      "[step: 726] loss: 10.61131763458252\n",
      "[step: 727] loss: 11.372923851013184\n",
      "[step: 728] loss: 11.466262817382812\n",
      "[step: 729] loss: 11.321022987365723\n",
      "[step: 730] loss: 10.591748237609863\n",
      "[step: 731] loss: 10.650055885314941\n",
      "[step: 732] loss: 11.604905128479004\n",
      "[step: 733] loss: 11.086195945739746\n",
      "[step: 734] loss: 10.50275993347168\n",
      "[step: 735] loss: 10.354829788208008\n",
      "[step: 736] loss: 10.5363130569458\n",
      "[step: 737] loss: 10.796006202697754\n",
      "[step: 738] loss: 10.328301429748535\n",
      "[step: 739] loss: 10.068212509155273\n",
      "[step: 740] loss: 10.244897842407227\n",
      "[step: 741] loss: 10.216045379638672\n",
      "[step: 742] loss: 10.161349296569824\n",
      "[step: 743] loss: 9.994077682495117\n",
      "[step: 744] loss: 9.978883743286133\n",
      "[step: 745] loss: 9.972142219543457\n",
      "[step: 746] loss: 9.911772727966309\n",
      "[step: 747] loss: 9.969799995422363\n",
      "[step: 748] loss: 9.870501518249512\n",
      "[step: 749] loss: 9.809882164001465\n",
      "[step: 750] loss: 9.746774673461914\n",
      "[step: 751] loss: 9.783650398254395\n",
      "[step: 752] loss: 9.788846969604492\n",
      "[step: 753] loss: 9.773483276367188\n",
      "[step: 754] loss: 9.769596099853516\n",
      "[step: 755] loss: 9.71129322052002\n",
      "[step: 756] loss: 9.666661262512207\n",
      "[step: 757] loss: 9.598094940185547\n",
      "[step: 758] loss: 9.580574989318848\n",
      "[step: 759] loss: 9.551813125610352\n",
      "[step: 760] loss: 9.536840438842773\n",
      "[step: 761] loss: 9.501341819763184\n",
      "[step: 762] loss: 9.500120162963867\n",
      "[step: 763] loss: 9.498485565185547\n",
      "[step: 764] loss: 9.515057563781738\n",
      "[step: 765] loss: 9.560432434082031\n",
      "[step: 766] loss: 9.645641326904297\n",
      "[step: 767] loss: 9.897602081298828\n",
      "[step: 768] loss: 10.08153247833252\n",
      "[step: 769] loss: 10.794062614440918\n",
      "[step: 770] loss: 10.211746215820312\n",
      "[step: 771] loss: 9.837752342224121\n",
      "[step: 772] loss: 9.429537773132324\n",
      "[step: 773] loss: 9.316611289978027\n",
      "[step: 774] loss: 9.444908142089844\n",
      "[step: 775] loss: 9.629753112792969\n",
      "[step: 776] loss: 9.933167457580566\n",
      "[step: 777] loss: 9.744742393493652\n",
      "[step: 778] loss: 9.549220085144043\n",
      "[step: 779] loss: 9.290077209472656\n",
      "[step: 780] loss: 9.200337409973145\n",
      "[step: 781] loss: 9.277585983276367\n",
      "[step: 782] loss: 9.387676239013672\n",
      "[step: 783] loss: 9.514358520507812\n",
      "[step: 784] loss: 9.426109313964844\n",
      "[step: 785] loss: 9.32157039642334\n",
      "[step: 786] loss: 9.171814918518066\n",
      "[step: 787] loss: 9.096322059631348\n",
      "[step: 788] loss: 9.09762191772461\n",
      "[step: 789] loss: 9.14338207244873\n",
      "[step: 790] loss: 9.210288047790527\n",
      "[step: 791] loss: 9.227869033813477\n",
      "[step: 792] loss: 9.251913070678711\n",
      "[step: 793] loss: 9.186392784118652\n",
      "[step: 794] loss: 9.157210350036621\n",
      "[step: 795] loss: 9.081095695495605\n",
      "[step: 796] loss: 9.04071044921875\n",
      "[step: 797] loss: 8.993453979492188\n",
      "[step: 798] loss: 8.964970588684082\n",
      "[step: 799] loss: 8.938884735107422\n",
      "[step: 800] loss: 8.918639183044434\n",
      "[step: 801] loss: 8.900324821472168\n",
      "[step: 802] loss: 8.8828763961792\n",
      "[step: 803] loss: 8.866867065429688\n",
      "[step: 804] loss: 8.851130485534668\n",
      "[step: 805] loss: 8.836063385009766\n",
      "[step: 806] loss: 8.822016716003418\n",
      "[step: 807] loss: 8.80865478515625\n",
      "[step: 808] loss: 8.798294067382812\n",
      "[step: 809] loss: 8.794468879699707\n",
      "[step: 810] loss: 8.813065528869629\n",
      "[step: 811] loss: 8.883342742919922\n",
      "[step: 812] loss: 9.1806640625\n",
      "[step: 813] loss: 9.615225791931152\n",
      "[step: 814] loss: 11.05022144317627\n",
      "[step: 815] loss: 9.160731315612793\n",
      "[step: 816] loss: 8.740013122558594\n",
      "[step: 817] loss: 9.053631782531738\n",
      "[step: 818] loss: 9.602375984191895\n",
      "[step: 819] loss: 10.289658546447754\n",
      "[step: 820] loss: 9.123249053955078\n",
      "[step: 821] loss: 8.987203598022461\n",
      "[step: 822] loss: 9.86799430847168\n",
      "[step: 823] loss: 9.978692054748535\n",
      "[step: 824] loss: 10.816156387329102\n",
      "[step: 825] loss: 10.466909408569336\n",
      "[step: 826] loss: 10.554763793945312\n",
      "[step: 827] loss: 9.845751762390137\n",
      "[step: 828] loss: 8.853657722473145\n",
      "[step: 829] loss: 9.406312942504883\n",
      "[step: 830] loss: 10.221222877502441\n",
      "[step: 831] loss: 9.13093376159668\n",
      "[step: 832] loss: 9.060540199279785\n",
      "[step: 833] loss: 9.87987232208252\n",
      "[step: 834] loss: 8.981593132019043\n",
      "[step: 835] loss: 9.385329246520996\n",
      "[step: 836] loss: 10.306944847106934\n",
      "[step: 837] loss: 8.70663833618164\n",
      "[step: 838] loss: 10.678451538085938\n",
      "[step: 839] loss: 12.104047775268555\n",
      "[step: 840] loss: 12.226550102233887\n",
      "[step: 841] loss: 9.010127067565918\n",
      "[step: 842] loss: 10.817316055297852\n",
      "[step: 843] loss: 12.085372924804688\n",
      "[step: 844] loss: 13.541888236999512\n",
      "[step: 845] loss: 11.043785095214844\n",
      "[step: 846] loss: 12.420881271362305\n",
      "[step: 847] loss: 9.327208518981934\n",
      "[step: 848] loss: 10.49738883972168\n",
      "[step: 849] loss: 9.438430786132812\n",
      "[step: 850] loss: 10.423776626586914\n",
      "[step: 851] loss: 9.280058860778809\n",
      "[step: 852] loss: 9.72021770477295\n",
      "[step: 853] loss: 9.086222648620605\n",
      "[step: 854] loss: 9.419219017028809\n",
      "[step: 855] loss: 9.314048767089844\n",
      "[step: 856] loss: 9.142122268676758\n",
      "[step: 857] loss: 9.275321006774902\n",
      "[step: 858] loss: 8.82590389251709\n",
      "[step: 859] loss: 9.331344604492188\n",
      "[step: 860] loss: 8.716230392456055\n",
      "[step: 861] loss: 9.277851104736328\n",
      "[step: 862] loss: 8.59421157836914\n",
      "[step: 863] loss: 9.065230369567871\n",
      "[step: 864] loss: 8.56386661529541\n",
      "[step: 865] loss: 8.880644798278809\n",
      "[step: 866] loss: 8.64588737487793\n",
      "[step: 867] loss: 8.704235076904297\n",
      "[step: 868] loss: 8.668904304504395\n",
      "[step: 869] loss: 8.55186939239502\n",
      "[step: 870] loss: 8.649208068847656\n",
      "[step: 871] loss: 8.499122619628906\n",
      "[step: 872] loss: 8.586360931396484\n",
      "[step: 873] loss: 8.54086685180664\n",
      "[step: 874] loss: 8.47108268737793\n",
      "[step: 875] loss: 8.587188720703125\n",
      "[step: 876] loss: 8.409788131713867\n",
      "[step: 877] loss: 8.506592750549316\n",
      "[step: 878] loss: 8.433832168579102\n",
      "[step: 879] loss: 8.406338691711426\n",
      "[step: 880] loss: 8.438955307006836\n",
      "[step: 881] loss: 8.388581275939941\n",
      "[step: 882] loss: 8.381278991699219\n",
      "[step: 883] loss: 8.412096977233887\n",
      "[step: 884] loss: 8.342306137084961\n",
      "[step: 885] loss: 8.392111778259277\n",
      "[step: 886] loss: 8.355534553527832\n",
      "[step: 887] loss: 8.333548545837402\n",
      "[step: 888] loss: 8.355001449584961\n",
      "[step: 889] loss: 8.327722549438477\n",
      "[step: 890] loss: 8.302680015563965\n",
      "[step: 891] loss: 8.33269214630127\n",
      "[step: 892] loss: 8.297720909118652\n",
      "[step: 893] loss: 8.286884307861328\n",
      "[step: 894] loss: 8.300237655639648\n",
      "[step: 895] loss: 8.278332710266113\n",
      "[step: 896] loss: 8.263919830322266\n",
      "[step: 897] loss: 8.27444076538086\n",
      "[step: 898] loss: 8.25704574584961\n",
      "[step: 899] loss: 8.240772247314453\n",
      "[step: 900] loss: 8.251959800720215\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={\n",
    "                                X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    \n",
    "    test_predict = scaler2.inverse_transform(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "plt.title('2018-02-27 predict')\n",
    "plt.plot(dataY2, label='row data') # 데이터 있을때\n",
    "plt.plot(test_predict, label='predict')\n",
    "plt.xlabel(\"Time Period\")\n",
    "plt.ylabel(\"CCT\")\n",
    "# plt.ylim(0, 140000)\n",
    "plt.xticks(np.arange(0, 901, step = 56), ['5', '6', '7', '8', '9', '10', '11', '12', '1', '2', '3', '4', '5', '6', '7', '8']) # v2 모델\n",
    "plt.yticks(np.arange(0, 50000, step=5000))\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
