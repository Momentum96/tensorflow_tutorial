{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "font_name = font_manager.FontProperties(fname=\"/usr/share/fonts/truetype/nanum/NanumGothic_Coding_Bold.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "%matplotlib tk\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61.  -4.  53.  19.]\n",
      " [ 61.  -4.  53.  19.]\n",
      " [ 61.  -4.  53.  19.]\n",
      " ...\n",
      " [271. -21. 330. -11.]\n",
      " [271. -21. 330. -11.]\n",
      " [271. -21. 330. -11.]]\n",
      "[[-2.87e-05]\n",
      " [-2.03e-05]\n",
      " [-1.43e-05]\n",
      " ...\n",
      " [ 1.48e-06]\n",
      " [-2.14e-06]\n",
      " [ 3.16e-06]]\n",
      "[[ 86. -29. 321. -15.]\n",
      " [ 86. -29. 321. -15.]\n",
      " [ 87. -29. 321. -15.]\n",
      " ...\n",
      " [268. -23. 322. -14.]\n",
      " [268. -23. 322. -14.]\n",
      " [268. -23. 322. -14.]]\n",
      "8100\n",
      "892\n"
     ]
    }
   ],
   "source": [
    "# train Parameters\n",
    "seq_length = 9\n",
    "data_dim = 4\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 901\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler2 = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Open, High, Low, Volume, Close\n",
    "xy = np.loadtxt('./v2data/uvb/train_v2_data.csv', delimiter=',')\n",
    "x = scaler.fit_transform(xy[:, 0:-1])\n",
    "y = scaler2.fit_transform(xy[:, [-1]])  # Close as label\n",
    "\n",
    "print(xy[:, 0:-1])\n",
    "print(xy[:, [-1]])\n",
    "\n",
    "# build a dataset\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, len(x) - seq_length):\n",
    "    _x = x[i:i + seq_length]\n",
    "    _y = y[i + seq_length]  # Next close price\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "\n",
    "xy2 = np.loadtxt('./sun_data/2018-02-08.csv',delimiter=',')\n",
    "print(xy2)\n",
    "\n",
    "x2 = scaler.fit_transform(xy2) # 데이터 없을때\n",
    "# x2 = scaler.fit_transform(xy2[:, 0:-1]) # 데이터 있을때\n",
    "# y2 = scaler2.fit_transform(xy2[:, [-1]]) #데이터 있을때\n",
    "\n",
    "dataX2 = []\n",
    "# dataY2 = [] #데이터 있을때\n",
    "\n",
    "for i in range(0, len(x2) - seq_length):\n",
    "    _x2 = x2[i:i+seq_length]\n",
    "#     _y2 = y2[i+seq_length] #데이터 있을때\n",
    "    dataX2.append(_x2)\n",
    "#     dataY2.append(_y2) #데이터 있을때\n",
    "# train/test split\n",
    "\n",
    "train_size = len(dataX)\n",
    "test_size = len(dataX2)\n",
    "\n",
    "print(len(dataX))\n",
    "print(len(dataX2))\n",
    "\n",
    "trainX, testX = np.array(dataX[0:train_size]),np.array(dataX2[0:test_size])\n",
    "trainY = np.array(dataY[0:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gw/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 1930.05078125\n",
      "[step: 1] loss: 1303.6395263671875\n",
      "[step: 2] loss: 950.427734375\n",
      "[step: 3] loss: 810.900634765625\n",
      "[step: 4] loss: 778.6639404296875\n",
      "[step: 5] loss: 751.0137329101562\n",
      "[step: 6] loss: 687.8187255859375\n",
      "[step: 7] loss: 599.82275390625\n",
      "[step: 8] loss: 513.1419677734375\n",
      "[step: 9] loss: 448.2466125488281\n",
      "[step: 10] loss: 412.2690124511719\n",
      "[step: 11] loss: 397.8635559082031\n",
      "[step: 12] loss: 387.068359375\n",
      "[step: 13] loss: 362.582763671875\n",
      "[step: 14] loss: 320.3615417480469\n",
      "[step: 15] loss: 273.91351318359375\n",
      "[step: 16] loss: 248.537353515625\n",
      "[step: 17] loss: 259.078125\n",
      "[step: 18] loss: 278.83966064453125\n",
      "[step: 19] loss: 267.6593322753906\n",
      "[step: 20] loss: 228.976806640625\n",
      "[step: 21] loss: 194.00473022460938\n",
      "[step: 22] loss: 179.64405822753906\n",
      "[step: 23] loss: 176.44979858398438\n",
      "[step: 24] loss: 166.70677185058594\n",
      "[step: 25] loss: 144.7452392578125\n",
      "[step: 26] loss: 120.10762786865234\n",
      "[step: 27] loss: 107.55732727050781\n",
      "[step: 28] loss: 111.1275863647461\n",
      "[step: 29] loss: 114.80919647216797\n",
      "[step: 30] loss: 104.42671203613281\n",
      "[step: 31] loss: 91.03250122070312\n",
      "[step: 32] loss: 89.99005126953125\n",
      "[step: 33] loss: 94.79443359375\n",
      "[step: 34] loss: 89.97693634033203\n",
      "[step: 35] loss: 78.48138427734375\n",
      "[step: 36] loss: 75.87266540527344\n",
      "[step: 37] loss: 80.25033569335938\n",
      "[step: 38] loss: 76.0218734741211\n",
      "[step: 39] loss: 67.65457916259766\n",
      "[step: 40] loss: 66.23912811279297\n",
      "[step: 41] loss: 67.61480712890625\n",
      "[step: 42] loss: 64.57472229003906\n",
      "[step: 43] loss: 59.524478912353516\n",
      "[step: 44] loss: 58.11716079711914\n",
      "[step: 45] loss: 59.835941314697266\n",
      "[step: 46] loss: 60.0774040222168\n",
      "[step: 47] loss: 58.21003341674805\n",
      "[step: 48] loss: 57.2845344543457\n",
      "[step: 49] loss: 58.10846710205078\n",
      "[step: 50] loss: 58.414371490478516\n",
      "[step: 51] loss: 56.7633171081543\n",
      "[step: 52] loss: 54.4857063293457\n",
      "[step: 53] loss: 53.4095344543457\n",
      "[step: 54] loss: 53.047027587890625\n",
      "[step: 55] loss: 51.713844299316406\n",
      "[step: 56] loss: 49.70613479614258\n",
      "[step: 57] loss: 48.63980484008789\n",
      "[step: 58] loss: 48.41569137573242\n",
      "[step: 59] loss: 47.62531661987305\n",
      "[step: 60] loss: 46.23094177246094\n",
      "[step: 61] loss: 45.414939880371094\n",
      "[step: 62] loss: 45.1182975769043\n",
      "[step: 63] loss: 44.23112487792969\n",
      "[step: 64] loss: 42.961490631103516\n",
      "[step: 65] loss: 42.227115631103516\n",
      "[step: 66] loss: 41.7199592590332\n",
      "[step: 67] loss: 40.77267837524414\n",
      "[step: 68] loss: 39.73651123046875\n",
      "[step: 69] loss: 39.13120651245117\n",
      "[step: 70] loss: 38.6252555847168\n",
      "[step: 71] loss: 37.866695404052734\n",
      "[step: 72] loss: 37.18816375732422\n",
      "[step: 73] loss: 36.796478271484375\n",
      "[step: 74] loss: 36.367034912109375\n",
      "[step: 75] loss: 35.73883819580078\n",
      "[step: 76] loss: 35.160396575927734\n",
      "[step: 77] loss: 34.710262298583984\n",
      "[step: 78] loss: 34.15033721923828\n",
      "[step: 79] loss: 33.45973205566406\n",
      "[step: 80] loss: 32.86259841918945\n",
      "[step: 81] loss: 32.348209381103516\n",
      "[step: 82] loss: 31.756187438964844\n",
      "[step: 83] loss: 31.158720016479492\n",
      "[step: 84] loss: 30.678552627563477\n",
      "[step: 85] loss: 30.20230484008789\n",
      "[step: 86] loss: 29.656394958496094\n",
      "[step: 87] loss: 29.159574508666992\n",
      "[step: 88] loss: 28.71100616455078\n",
      "[step: 89] loss: 28.210391998291016\n",
      "[step: 90] loss: 27.7113037109375\n",
      "[step: 91] loss: 27.275928497314453\n",
      "[step: 92] loss: 26.829654693603516\n",
      "[step: 93] loss: 26.361188888549805\n",
      "[step: 94] loss: 25.934579849243164\n",
      "[step: 95] loss: 25.517316818237305\n",
      "[step: 96] loss: 25.07349967956543\n",
      "[step: 97] loss: 24.65211296081543\n",
      "[step: 98] loss: 24.255577087402344\n",
      "[step: 99] loss: 23.841480255126953\n",
      "[step: 100] loss: 23.433340072631836\n",
      "[step: 101] loss: 23.04668426513672\n",
      "[step: 102] loss: 22.648527145385742\n",
      "[step: 103] loss: 22.250585556030273\n",
      "[step: 104] loss: 21.87742042541504\n",
      "[step: 105] loss: 21.507949829101562\n",
      "[step: 106] loss: 21.14344024658203\n",
      "[step: 107] loss: 20.80120277404785\n",
      "[step: 108] loss: 20.46290397644043\n",
      "[step: 109] loss: 20.126609802246094\n",
      "[step: 110] loss: 19.809057235717773\n",
      "[step: 111] loss: 19.49920654296875\n",
      "[step: 112] loss: 19.19576644897461\n",
      "[step: 113] loss: 18.91013526916504\n",
      "[step: 114] loss: 18.631874084472656\n",
      "[step: 115] loss: 18.36159324645996\n",
      "[step: 116] loss: 18.10948371887207\n",
      "[step: 117] loss: 17.86860466003418\n",
      "[step: 118] loss: 17.640254974365234\n",
      "[step: 119] loss: 17.42879867553711\n",
      "[step: 120] loss: 17.226804733276367\n",
      "[step: 121] loss: 17.037025451660156\n",
      "[step: 122] loss: 16.86276626586914\n",
      "[step: 123] loss: 16.698875427246094\n",
      "[step: 124] loss: 16.54781723022461\n",
      "[step: 125] loss: 16.409255981445312\n",
      "[step: 126] loss: 16.279312133789062\n",
      "[step: 127] loss: 16.161176681518555\n",
      "[step: 128] loss: 16.05345344543457\n",
      "[step: 129] loss: 15.953535079956055\n",
      "[step: 130] loss: 15.86281681060791\n",
      "[step: 131] loss: 15.778583526611328\n",
      "[step: 132] loss: 15.700238227844238\n",
      "[step: 133] loss: 15.628671646118164\n",
      "[step: 134] loss: 15.561217308044434\n",
      "[step: 135] loss: 15.49795913696289\n",
      "[step: 136] loss: 15.438628196716309\n",
      "[step: 137] loss: 15.381697654724121\n",
      "[step: 138] loss: 15.327878952026367\n",
      "[step: 139] loss: 15.276092529296875\n",
      "[step: 140] loss: 15.225529670715332\n",
      "[step: 141] loss: 15.176752090454102\n",
      "[step: 142] loss: 15.128822326660156\n",
      "[step: 143] loss: 15.081880569458008\n",
      "[step: 144] loss: 15.035935401916504\n",
      "[step: 145] loss: 14.990377426147461\n",
      "[step: 146] loss: 14.945799827575684\n",
      "[step: 147] loss: 14.901993751525879\n",
      "[step: 148] loss: 14.858796119689941\n",
      "[step: 149] loss: 14.81653881072998\n",
      "[step: 150] loss: 14.774857521057129\n",
      "[step: 151] loss: 14.734025001525879\n",
      "[step: 152] loss: 14.694071769714355\n",
      "[step: 153] loss: 14.654757499694824\n",
      "[step: 154] loss: 14.616351127624512\n",
      "[step: 155] loss: 14.57871150970459\n",
      "[step: 156] loss: 14.541903495788574\n",
      "[step: 157] loss: 14.506017684936523\n",
      "[step: 158] loss: 14.470863342285156\n",
      "[step: 159] loss: 14.436591148376465\n",
      "[step: 160] loss: 14.403131484985352\n",
      "[step: 161] loss: 14.370455741882324\n",
      "[step: 162] loss: 14.338611602783203\n",
      "[step: 163] loss: 14.30748176574707\n",
      "[step: 164] loss: 14.277135848999023\n",
      "[step: 165] loss: 14.247544288635254\n",
      "[step: 166] loss: 14.218657493591309\n",
      "[step: 167] loss: 14.190516471862793\n",
      "[step: 168] loss: 14.163044929504395\n",
      "[step: 169] loss: 14.136274337768555\n",
      "[step: 170] loss: 14.110170364379883\n",
      "[step: 171] loss: 14.084693908691406\n",
      "[step: 172] loss: 14.059863090515137\n",
      "[step: 173] loss: 14.035636901855469\n",
      "[step: 174] loss: 14.012011528015137\n",
      "[step: 175] loss: 13.988973617553711\n",
      "[step: 176] loss: 13.966495513916016\n",
      "[step: 177] loss: 13.944581031799316\n",
      "[step: 178] loss: 13.923190116882324\n",
      "[step: 179] loss: 13.902328491210938\n",
      "[step: 180] loss: 13.881975173950195\n",
      "[step: 181] loss: 13.862107276916504\n",
      "[step: 182] loss: 13.842724800109863\n",
      "[step: 183] loss: 13.823802947998047\n",
      "[step: 184] loss: 13.805341720581055\n",
      "[step: 185] loss: 13.787317276000977\n",
      "[step: 186] loss: 13.769739151000977\n",
      "[step: 187] loss: 13.752579689025879\n",
      "[step: 188] loss: 13.735835075378418\n",
      "[step: 189] loss: 13.719497680664062\n",
      "[step: 190] loss: 13.703553199768066\n",
      "[step: 191] loss: 13.688003540039062\n",
      "[step: 192] loss: 13.672832489013672\n",
      "[step: 193] loss: 13.65803337097168\n",
      "[step: 194] loss: 13.643601417541504\n",
      "[step: 195] loss: 13.629528045654297\n",
      "[step: 196] loss: 13.61581039428711\n",
      "[step: 197] loss: 13.602436065673828\n",
      "[step: 198] loss: 13.589402198791504\n",
      "[step: 199] loss: 13.57669734954834\n",
      "[step: 200] loss: 13.564314842224121\n",
      "[step: 201] loss: 13.552253723144531\n",
      "[step: 202] loss: 13.540496826171875\n",
      "[step: 203] loss: 13.529050827026367\n",
      "[step: 204] loss: 13.517899513244629\n",
      "[step: 205] loss: 13.507035255432129\n",
      "[step: 206] loss: 13.49644947052002\n",
      "[step: 207] loss: 13.486141204833984\n",
      "[step: 208] loss: 13.476099967956543\n",
      "[step: 209] loss: 13.466318130493164\n",
      "[step: 210] loss: 13.456783294677734\n",
      "[step: 211] loss: 13.447488784790039\n",
      "[step: 212] loss: 13.438445091247559\n",
      "[step: 213] loss: 13.429622650146484\n",
      "[step: 214] loss: 13.421018600463867\n",
      "[step: 215] loss: 13.412643432617188\n",
      "[step: 216] loss: 13.404471397399902\n",
      "[step: 217] loss: 13.396501541137695\n",
      "[step: 218] loss: 13.388731002807617\n",
      "[step: 219] loss: 13.381150245666504\n",
      "[step: 220] loss: 13.373759269714355\n",
      "[step: 221] loss: 13.366539001464844\n",
      "[step: 222] loss: 13.359498977661133\n",
      "[step: 223] loss: 13.352622985839844\n",
      "[step: 224] loss: 13.345917701721191\n",
      "[step: 225] loss: 13.339363098144531\n",
      "[step: 226] loss: 13.332962036132812\n",
      "[step: 227] loss: 13.326708793640137\n",
      "[step: 228] loss: 13.320602416992188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 229] loss: 13.314630508422852\n",
      "[step: 230] loss: 13.30880069732666\n",
      "[step: 231] loss: 13.303089141845703\n",
      "[step: 232] loss: 13.297504425048828\n",
      "[step: 233] loss: 13.292037010192871\n",
      "[step: 234] loss: 13.286691665649414\n",
      "[step: 235] loss: 13.281458854675293\n",
      "[step: 236] loss: 13.276333808898926\n",
      "[step: 237] loss: 13.271307945251465\n",
      "[step: 238] loss: 13.266388893127441\n",
      "[step: 239] loss: 13.26156234741211\n",
      "[step: 240] loss: 13.256832122802734\n",
      "[step: 241] loss: 13.252193450927734\n",
      "[step: 242] loss: 13.247641563415527\n",
      "[step: 243] loss: 13.243170738220215\n",
      "[step: 244] loss: 13.238781929016113\n",
      "[step: 245] loss: 13.23446273803711\n",
      "[step: 246] loss: 13.23022747039795\n",
      "[step: 247] loss: 13.226064682006836\n",
      "[step: 248] loss: 13.221967697143555\n",
      "[step: 249] loss: 13.217934608459473\n",
      "[step: 250] loss: 13.213967323303223\n",
      "[step: 251] loss: 13.210064888000488\n",
      "[step: 252] loss: 13.206221580505371\n",
      "[step: 253] loss: 13.202425956726074\n",
      "[step: 254] loss: 13.198692321777344\n",
      "[step: 255] loss: 13.195013999938965\n",
      "[step: 256] loss: 13.191383361816406\n",
      "[step: 257] loss: 13.187797546386719\n",
      "[step: 258] loss: 13.18426513671875\n",
      "[step: 259] loss: 13.180771827697754\n",
      "[step: 260] loss: 13.177321434020996\n",
      "[step: 261] loss: 13.173914909362793\n",
      "[step: 262] loss: 13.170544624328613\n",
      "[step: 263] loss: 13.167220115661621\n",
      "[step: 264] loss: 13.163925170898438\n",
      "[step: 265] loss: 13.16066837310791\n",
      "[step: 266] loss: 13.157449722290039\n",
      "[step: 267] loss: 13.154256820678711\n",
      "[step: 268] loss: 13.151103019714355\n",
      "[step: 269] loss: 13.147971153259277\n",
      "[step: 270] loss: 13.144878387451172\n",
      "[step: 271] loss: 13.141800880432129\n",
      "[step: 272] loss: 13.138754844665527\n",
      "[step: 273] loss: 13.135740280151367\n",
      "[step: 274] loss: 13.132746696472168\n",
      "[step: 275] loss: 13.129773139953613\n",
      "[step: 276] loss: 13.126829147338867\n",
      "[step: 277] loss: 13.123903274536133\n",
      "[step: 278] loss: 13.120999336242676\n",
      "[step: 279] loss: 13.118119239807129\n",
      "[step: 280] loss: 13.11525821685791\n",
      "[step: 281] loss: 13.112415313720703\n",
      "[step: 282] loss: 13.109589576721191\n",
      "[step: 283] loss: 13.106779098510742\n",
      "[step: 284] loss: 13.103989601135254\n",
      "[step: 285] loss: 13.101219177246094\n",
      "[step: 286] loss: 13.098458290100098\n",
      "[step: 287] loss: 13.095719337463379\n",
      "[step: 288] loss: 13.092989921569824\n",
      "[step: 289] loss: 13.090282440185547\n",
      "[step: 290] loss: 13.087577819824219\n",
      "[step: 291] loss: 13.084890365600586\n",
      "[step: 292] loss: 13.082222938537598\n",
      "[step: 293] loss: 13.079558372497559\n",
      "[step: 294] loss: 13.076910972595215\n",
      "[step: 295] loss: 13.074275016784668\n",
      "[step: 296] loss: 13.071649551391602\n",
      "[step: 297] loss: 13.069034576416016\n",
      "[step: 298] loss: 13.066433906555176\n",
      "[step: 299] loss: 13.063837051391602\n",
      "[step: 300] loss: 13.061256408691406\n",
      "[step: 301] loss: 13.058683395385742\n",
      "[step: 302] loss: 13.056120872497559\n",
      "[step: 303] loss: 13.053560256958008\n",
      "[step: 304] loss: 13.05102252960205\n",
      "[step: 305] loss: 13.048478126525879\n",
      "[step: 306] loss: 13.045950889587402\n",
      "[step: 307] loss: 13.04343032836914\n",
      "[step: 308] loss: 13.040919303894043\n",
      "[step: 309] loss: 13.038415908813477\n",
      "[step: 310] loss: 13.035918235778809\n",
      "[step: 311] loss: 13.033429145812988\n",
      "[step: 312] loss: 13.030945777893066\n",
      "[step: 313] loss: 13.028470993041992\n",
      "[step: 314] loss: 13.026006698608398\n",
      "[step: 315] loss: 13.023536682128906\n",
      "[step: 316] loss: 13.02108097076416\n",
      "[step: 317] loss: 13.018632888793945\n",
      "[step: 318] loss: 13.016190528869629\n",
      "[step: 319] loss: 13.013751983642578\n",
      "[step: 320] loss: 13.011321067810059\n",
      "[step: 321] loss: 13.008893966674805\n",
      "[step: 322] loss: 13.006473541259766\n",
      "[step: 323] loss: 13.004058837890625\n",
      "[step: 324] loss: 13.001646995544434\n",
      "[step: 325] loss: 12.99924373626709\n",
      "[step: 326] loss: 12.996848106384277\n",
      "[step: 327] loss: 12.994457244873047\n",
      "[step: 328] loss: 12.99206256866455\n",
      "[step: 329] loss: 12.989681243896484\n",
      "[step: 330] loss: 12.987300872802734\n",
      "[step: 331] loss: 12.984925270080566\n",
      "[step: 332] loss: 12.982553482055664\n",
      "[step: 333] loss: 12.980194091796875\n",
      "[step: 334] loss: 12.977832794189453\n",
      "[step: 335] loss: 12.975470542907715\n",
      "[step: 336] loss: 12.973118782043457\n",
      "[step: 337] loss: 12.970765113830566\n",
      "[step: 338] loss: 12.968424797058105\n",
      "[step: 339] loss: 12.966079711914062\n",
      "[step: 340] loss: 12.963741302490234\n",
      "[step: 341] loss: 12.961405754089355\n",
      "[step: 342] loss: 12.959075927734375\n",
      "[step: 343] loss: 12.956751823425293\n",
      "[step: 344] loss: 12.954425811767578\n",
      "[step: 345] loss: 12.952105522155762\n",
      "[step: 346] loss: 12.949789047241211\n",
      "[step: 347] loss: 12.947474479675293\n",
      "[step: 348] loss: 12.945165634155273\n",
      "[step: 349] loss: 12.94286060333252\n",
      "[step: 350] loss: 12.940556526184082\n",
      "[step: 351] loss: 12.938257217407227\n",
      "[step: 352] loss: 12.935956954956055\n",
      "[step: 353] loss: 12.933662414550781\n",
      "[step: 354] loss: 12.931373596191406\n",
      "[step: 355] loss: 12.929081916809082\n",
      "[step: 356] loss: 12.926794052124023\n",
      "[step: 357] loss: 12.92451000213623\n",
      "[step: 358] loss: 12.922231674194336\n",
      "[step: 359] loss: 12.919950485229492\n",
      "[step: 360] loss: 12.917677879333496\n",
      "[step: 361] loss: 12.915400505065918\n",
      "[step: 362] loss: 12.91313362121582\n",
      "[step: 363] loss: 12.910866737365723\n",
      "[step: 364] loss: 12.908599853515625\n",
      "[step: 365] loss: 12.906336784362793\n",
      "[step: 366] loss: 12.904078483581543\n",
      "[step: 367] loss: 12.901816368103027\n",
      "[step: 368] loss: 12.899560928344727\n",
      "[step: 369] loss: 12.897306442260742\n",
      "[step: 370] loss: 12.895051956176758\n",
      "[step: 371] loss: 12.892802238464355\n",
      "[step: 372] loss: 12.890554428100586\n",
      "[step: 373] loss: 12.888306617736816\n",
      "[step: 374] loss: 12.886064529418945\n",
      "[step: 375] loss: 12.883824348449707\n",
      "[step: 376] loss: 12.881579399108887\n",
      "[step: 377] loss: 12.879344940185547\n",
      "[step: 378] loss: 12.877108573913574\n",
      "[step: 379] loss: 12.874868392944336\n",
      "[step: 380] loss: 12.872638702392578\n",
      "[step: 381] loss: 12.870405197143555\n",
      "[step: 382] loss: 12.868171691894531\n",
      "[step: 383] loss: 12.865944862365723\n",
      "[step: 384] loss: 12.863720893859863\n",
      "[step: 385] loss: 12.861494064331055\n",
      "[step: 386] loss: 12.859270095825195\n",
      "[step: 387] loss: 12.857048034667969\n",
      "[step: 388] loss: 12.854823112487793\n",
      "[step: 389] loss: 12.852609634399414\n",
      "[step: 390] loss: 12.850390434265137\n",
      "[step: 391] loss: 12.84817123413086\n",
      "[step: 392] loss: 12.845956802368164\n",
      "[step: 393] loss: 12.843741416931152\n",
      "[step: 394] loss: 12.841530799865723\n",
      "[step: 395] loss: 12.839319229125977\n",
      "[step: 396] loss: 12.837106704711914\n",
      "[step: 397] loss: 12.834898948669434\n",
      "[step: 398] loss: 12.832693099975586\n",
      "[step: 399] loss: 12.830480575561523\n",
      "[step: 400] loss: 12.828271865844727\n",
      "[step: 401] loss: 12.826075553894043\n",
      "[step: 402] loss: 12.823869705200195\n",
      "[step: 403] loss: 12.821669578552246\n",
      "[step: 404] loss: 12.819464683532715\n",
      "[step: 405] loss: 12.817259788513184\n",
      "[step: 406] loss: 12.815062522888184\n",
      "[step: 407] loss: 12.812859535217285\n",
      "[step: 408] loss: 12.810662269592285\n",
      "[step: 409] loss: 12.80846881866455\n",
      "[step: 410] loss: 12.806268692016602\n",
      "[step: 411] loss: 12.804076194763184\n",
      "[step: 412] loss: 12.8018798828125\n",
      "[step: 413] loss: 12.7996826171875\n",
      "[step: 414] loss: 12.797489166259766\n",
      "[step: 415] loss: 12.795300483703613\n",
      "[step: 416] loss: 12.793102264404297\n",
      "[step: 417] loss: 12.790913581848145\n",
      "[step: 418] loss: 12.788721084594727\n",
      "[step: 419] loss: 12.786530494689941\n",
      "[step: 420] loss: 12.784343719482422\n",
      "[step: 421] loss: 12.782149314880371\n",
      "[step: 422] loss: 12.779958724975586\n",
      "[step: 423] loss: 12.777775764465332\n",
      "[step: 424] loss: 12.77558422088623\n",
      "[step: 425] loss: 12.773396492004395\n",
      "[step: 426] loss: 12.771209716796875\n",
      "[step: 427] loss: 12.769024848937988\n",
      "[step: 428] loss: 12.766837120056152\n",
      "[step: 429] loss: 12.764653205871582\n",
      "[step: 430] loss: 12.762466430664062\n",
      "[step: 431] loss: 12.760281562805176\n",
      "[step: 432] loss: 12.758098602294922\n",
      "[step: 433] loss: 12.755908966064453\n",
      "[step: 434] loss: 12.753724098205566\n",
      "[step: 435] loss: 12.75153636932373\n",
      "[step: 436] loss: 12.749361038208008\n",
      "[step: 437] loss: 12.747174263000488\n",
      "[step: 438] loss: 12.744988441467285\n",
      "[step: 439] loss: 12.74280834197998\n",
      "[step: 440] loss: 12.740622520446777\n",
      "[step: 441] loss: 12.73843765258789\n",
      "[step: 442] loss: 12.73625659942627\n",
      "[step: 443] loss: 12.734076499938965\n",
      "[step: 444] loss: 12.731889724731445\n",
      "[step: 445] loss: 12.729707717895508\n",
      "[step: 446] loss: 12.72752571105957\n",
      "[step: 447] loss: 12.725343704223633\n",
      "[step: 448] loss: 12.723160743713379\n",
      "[step: 449] loss: 12.720975875854492\n",
      "[step: 450] loss: 12.718791961669922\n",
      "[step: 451] loss: 12.716611862182617\n",
      "[step: 452] loss: 12.714426040649414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 453] loss: 12.712242126464844\n",
      "[step: 454] loss: 12.710062980651855\n",
      "[step: 455] loss: 12.707876205444336\n",
      "[step: 456] loss: 12.705694198608398\n",
      "[step: 457] loss: 12.703511238098145\n",
      "[step: 458] loss: 12.701325416564941\n",
      "[step: 459] loss: 12.699139595031738\n",
      "[step: 460] loss: 12.696958541870117\n",
      "[step: 461] loss: 12.694770812988281\n",
      "[step: 462] loss: 12.69258975982666\n",
      "[step: 463] loss: 12.690402030944824\n",
      "[step: 464] loss: 12.688216209411621\n",
      "[step: 465] loss: 12.686037063598633\n",
      "[step: 466] loss: 12.683846473693848\n",
      "[step: 467] loss: 12.681666374206543\n",
      "[step: 468] loss: 12.679475784301758\n",
      "[step: 469] loss: 12.677291870117188\n",
      "[step: 470] loss: 12.675097465515137\n",
      "[step: 471] loss: 12.67291259765625\n",
      "[step: 472] loss: 12.670727729797363\n",
      "[step: 473] loss: 12.66853141784668\n",
      "[step: 474] loss: 12.666348457336426\n",
      "[step: 475] loss: 12.66416072845459\n",
      "[step: 476] loss: 12.661969184875488\n",
      "[step: 477] loss: 12.65977954864502\n",
      "[step: 478] loss: 12.657588958740234\n",
      "[step: 479] loss: 12.655396461486816\n",
      "[step: 480] loss: 12.653207778930664\n",
      "[step: 481] loss: 12.65101432800293\n",
      "[step: 482] loss: 12.648823738098145\n",
      "[step: 483] loss: 12.646625518798828\n",
      "[step: 484] loss: 12.64443302154541\n",
      "[step: 485] loss: 12.642240524291992\n",
      "[step: 486] loss: 12.640047073364258\n",
      "[step: 487] loss: 12.63785457611084\n",
      "[step: 488] loss: 12.635656356811523\n",
      "[step: 489] loss: 12.633463859558105\n",
      "[step: 490] loss: 12.631264686584473\n",
      "[step: 491] loss: 12.629061698913574\n",
      "[step: 492] loss: 12.626863479614258\n",
      "[step: 493] loss: 12.62466812133789\n",
      "[step: 494] loss: 12.622469902038574\n",
      "[step: 495] loss: 12.620266914367676\n",
      "[step: 496] loss: 12.618069648742676\n",
      "[step: 497] loss: 12.61586856842041\n",
      "[step: 498] loss: 12.613664627075195\n",
      "[step: 499] loss: 12.611458778381348\n",
      "[step: 500] loss: 12.609258651733398\n",
      "[step: 501] loss: 12.607049942016602\n",
      "[step: 502] loss: 12.604849815368652\n",
      "[step: 503] loss: 12.60263729095459\n",
      "[step: 504] loss: 12.600431442260742\n",
      "[step: 505] loss: 12.598228454589844\n",
      "[step: 506] loss: 12.596014976501465\n",
      "[step: 507] loss: 12.593806266784668\n",
      "[step: 508] loss: 12.591596603393555\n",
      "[step: 509] loss: 12.589391708374023\n",
      "[step: 510] loss: 12.587174415588379\n",
      "[step: 511] loss: 12.5849609375\n",
      "[step: 512] loss: 12.58275032043457\n",
      "[step: 513] loss: 12.580536842346191\n",
      "[step: 514] loss: 12.57832145690918\n",
      "[step: 515] loss: 12.576101303100586\n",
      "[step: 516] loss: 12.573891639709473\n",
      "[step: 517] loss: 12.571666717529297\n",
      "[step: 518] loss: 12.569450378417969\n",
      "[step: 519] loss: 12.567232131958008\n",
      "[step: 520] loss: 12.565016746520996\n",
      "[step: 521] loss: 12.56279468536377\n",
      "[step: 522] loss: 12.56057071685791\n",
      "[step: 523] loss: 12.558347702026367\n",
      "[step: 524] loss: 12.556123733520508\n",
      "[step: 525] loss: 12.553898811340332\n",
      "[step: 526] loss: 12.551671028137207\n",
      "[step: 527] loss: 12.549448013305664\n",
      "[step: 528] loss: 12.547219276428223\n",
      "[step: 529] loss: 12.544988632202148\n",
      "[step: 530] loss: 12.542756080627441\n",
      "[step: 531] loss: 12.540526390075684\n",
      "[step: 532] loss: 12.538293838500977\n",
      "[step: 533] loss: 12.536066055297852\n",
      "[step: 534] loss: 12.533829689025879\n",
      "[step: 535] loss: 12.531595230102539\n",
      "[step: 536] loss: 12.52935791015625\n",
      "[step: 537] loss: 12.527122497558594\n",
      "[step: 538] loss: 12.524879455566406\n",
      "[step: 539] loss: 12.522644996643066\n",
      "[step: 540] loss: 12.520401954650879\n",
      "[step: 541] loss: 12.51816463470459\n",
      "[step: 542] loss: 12.515924453735352\n",
      "[step: 543] loss: 12.513679504394531\n",
      "[step: 544] loss: 12.511431694030762\n",
      "[step: 545] loss: 12.509186744689941\n",
      "[step: 546] loss: 12.50694465637207\n",
      "[step: 547] loss: 12.504695892333984\n",
      "[step: 548] loss: 12.502449035644531\n",
      "[step: 549] loss: 12.500194549560547\n",
      "[step: 550] loss: 12.497947692871094\n",
      "[step: 551] loss: 12.495694160461426\n",
      "[step: 552] loss: 12.493437767028809\n",
      "[step: 553] loss: 12.49118709564209\n",
      "[step: 554] loss: 12.488930702209473\n",
      "[step: 555] loss: 12.486671447753906\n",
      "[step: 556] loss: 12.484419822692871\n",
      "[step: 557] loss: 12.482158660888672\n",
      "[step: 558] loss: 12.479898452758789\n",
      "[step: 559] loss: 12.477639198303223\n",
      "[step: 560] loss: 12.475375175476074\n",
      "[step: 561] loss: 12.473112106323242\n",
      "[step: 562] loss: 12.470847129821777\n",
      "[step: 563] loss: 12.468578338623047\n",
      "[step: 564] loss: 12.466316223144531\n",
      "[step: 565] loss: 12.464049339294434\n",
      "[step: 566] loss: 12.46178150177002\n",
      "[step: 567] loss: 12.459507942199707\n",
      "[step: 568] loss: 12.457234382629395\n",
      "[step: 569] loss: 12.454967498779297\n",
      "[step: 570] loss: 12.452692985534668\n",
      "[step: 571] loss: 12.450413703918457\n",
      "[step: 572] loss: 12.448140144348145\n",
      "[step: 573] loss: 12.445867538452148\n",
      "[step: 574] loss: 12.443584442138672\n",
      "[step: 575] loss: 12.441303253173828\n",
      "[step: 576] loss: 12.439020156860352\n",
      "[step: 577] loss: 12.43674087524414\n",
      "[step: 578] loss: 12.434453010559082\n",
      "[step: 579] loss: 12.432171821594238\n",
      "[step: 580] loss: 12.429886817932129\n",
      "[step: 581] loss: 12.427597999572754\n",
      "[step: 582] loss: 12.425309181213379\n",
      "[step: 583] loss: 12.423017501831055\n",
      "[step: 584] loss: 12.420727729797363\n",
      "[step: 585] loss: 12.418434143066406\n",
      "[step: 586] loss: 12.416147232055664\n",
      "[step: 587] loss: 12.41384506225586\n",
      "[step: 588] loss: 12.411550521850586\n",
      "[step: 589] loss: 12.40925407409668\n",
      "[step: 590] loss: 12.406953811645508\n",
      "[step: 591] loss: 12.404655456542969\n",
      "[step: 592] loss: 12.402351379394531\n",
      "[step: 593] loss: 12.400053024291992\n",
      "[step: 594] loss: 12.397747039794922\n",
      "[step: 595] loss: 12.3954439163208\n",
      "[step: 596] loss: 12.39313793182373\n",
      "[step: 597] loss: 12.390829086303711\n",
      "[step: 598] loss: 12.388521194458008\n",
      "[step: 599] loss: 12.386205673217773\n",
      "[step: 600] loss: 12.383899688720703\n",
      "[step: 601] loss: 12.381582260131836\n",
      "[step: 602] loss: 12.379266738891602\n",
      "[step: 603] loss: 12.376955032348633\n",
      "[step: 604] loss: 12.374635696411133\n",
      "[step: 605] loss: 12.372316360473633\n",
      "[step: 606] loss: 12.369998931884766\n",
      "[step: 607] loss: 12.367679595947266\n",
      "[step: 608] loss: 12.3653564453125\n",
      "[step: 609] loss: 12.363033294677734\n",
      "[step: 610] loss: 12.36070442199707\n",
      "[step: 611] loss: 12.358381271362305\n",
      "[step: 612] loss: 12.356054306030273\n",
      "[step: 613] loss: 12.353724479675293\n",
      "[step: 614] loss: 12.351391792297363\n",
      "[step: 615] loss: 12.349061012268066\n",
      "[step: 616] loss: 12.346731185913086\n",
      "[step: 617] loss: 12.344392776489258\n",
      "[step: 618] loss: 12.342057228088379\n",
      "[step: 619] loss: 12.339715957641602\n",
      "[step: 620] loss: 12.33737850189209\n",
      "[step: 621] loss: 12.335042953491211\n",
      "[step: 622] loss: 12.332697868347168\n",
      "[step: 623] loss: 12.33035659790039\n",
      "[step: 624] loss: 12.328009605407715\n",
      "[step: 625] loss: 12.325654983520508\n",
      "[step: 626] loss: 12.323311805725098\n",
      "[step: 627] loss: 12.320963859558105\n",
      "[step: 628] loss: 12.318615913391113\n",
      "[step: 629] loss: 12.316264152526855\n",
      "[step: 630] loss: 12.313909530639648\n",
      "[step: 631] loss: 12.311551094055176\n",
      "[step: 632] loss: 12.309197425842285\n",
      "[step: 633] loss: 12.306839942932129\n",
      "[step: 634] loss: 12.30447769165039\n",
      "[step: 635] loss: 12.302117347717285\n",
      "[step: 636] loss: 12.299755096435547\n",
      "[step: 637] loss: 12.297394752502441\n",
      "[step: 638] loss: 12.295025825500488\n",
      "[step: 639] loss: 12.292655944824219\n",
      "[step: 640] loss: 12.290287971496582\n",
      "[step: 641] loss: 12.287919998168945\n",
      "[step: 642] loss: 12.285541534423828\n",
      "[step: 643] loss: 12.283172607421875\n",
      "[step: 644] loss: 12.280795097351074\n",
      "[step: 645] loss: 12.278422355651855\n",
      "[step: 646] loss: 12.276041984558105\n",
      "[step: 647] loss: 12.273662567138672\n",
      "[step: 648] loss: 12.271275520324707\n",
      "[step: 649] loss: 12.268896102905273\n",
      "[step: 650] loss: 12.266509056091309\n",
      "[step: 651] loss: 12.264120101928711\n",
      "[step: 652] loss: 12.261738777160645\n",
      "[step: 653] loss: 12.2593412399292\n",
      "[step: 654] loss: 12.25694751739502\n",
      "[step: 655] loss: 12.254558563232422\n",
      "[step: 656] loss: 12.252164840698242\n",
      "[step: 657] loss: 12.249763488769531\n",
      "[step: 658] loss: 12.247366905212402\n",
      "[step: 659] loss: 12.244967460632324\n",
      "[step: 660] loss: 12.242568969726562\n",
      "[step: 661] loss: 12.240156173706055\n",
      "[step: 662] loss: 12.237751007080078\n",
      "[step: 663] loss: 12.235344886779785\n",
      "[step: 664] loss: 12.23293399810791\n",
      "[step: 665] loss: 12.230521202087402\n",
      "[step: 666] loss: 12.228109359741211\n",
      "[step: 667] loss: 12.225695610046387\n",
      "[step: 668] loss: 12.223278999328613\n",
      "[step: 669] loss: 12.220856666564941\n",
      "[step: 670] loss: 12.218436241149902\n",
      "[step: 671] loss: 12.216012954711914\n",
      "[step: 672] loss: 12.213591575622559\n",
      "[step: 673] loss: 12.211158752441406\n",
      "[step: 674] loss: 12.20872974395752\n",
      "[step: 675] loss: 12.20630168914795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 676] loss: 12.203866958618164\n",
      "[step: 677] loss: 12.201435089111328\n",
      "[step: 678] loss: 12.198994636535645\n",
      "[step: 679] loss: 12.196558952331543\n",
      "[step: 680] loss: 12.19411563873291\n",
      "[step: 681] loss: 12.19167423248291\n",
      "[step: 682] loss: 12.189229011535645\n",
      "[step: 683] loss: 12.186779022216797\n",
      "[step: 684] loss: 12.184328079223633\n",
      "[step: 685] loss: 12.181877136230469\n",
      "[step: 686] loss: 12.179421424865723\n",
      "[step: 687] loss: 12.17696475982666\n",
      "[step: 688] loss: 12.174510955810547\n",
      "[step: 689] loss: 12.172046661376953\n",
      "[step: 690] loss: 12.169587135314941\n",
      "[step: 691] loss: 12.167121887207031\n",
      "[step: 692] loss: 12.164651870727539\n",
      "[step: 693] loss: 12.162179946899414\n",
      "[step: 694] loss: 12.159708023071289\n",
      "[step: 695] loss: 12.157235145568848\n",
      "[step: 696] loss: 12.154760360717773\n",
      "[step: 697] loss: 12.152280807495117\n",
      "[step: 698] loss: 12.149797439575195\n",
      "[step: 699] loss: 12.14731502532959\n",
      "[step: 700] loss: 12.14482593536377\n",
      "[step: 701] loss: 12.142339706420898\n",
      "[step: 702] loss: 12.13984203338623\n",
      "[step: 703] loss: 12.137351989746094\n",
      "[step: 704] loss: 12.13485336303711\n",
      "[step: 705] loss: 12.132354736328125\n",
      "[step: 706] loss: 12.129851341247559\n",
      "[step: 707] loss: 12.127349853515625\n",
      "[step: 708] loss: 12.124844551086426\n",
      "[step: 709] loss: 12.122328758239746\n",
      "[step: 710] loss: 12.119817733764648\n",
      "[step: 711] loss: 12.117300987243652\n",
      "[step: 712] loss: 12.114786148071289\n",
      "[step: 713] loss: 12.112262725830078\n",
      "[step: 714] loss: 12.109739303588867\n",
      "[step: 715] loss: 12.107213020324707\n",
      "[step: 716] loss: 12.104683876037598\n",
      "[step: 717] loss: 12.102151870727539\n",
      "[step: 718] loss: 12.099617958068848\n",
      "[step: 719] loss: 12.097077369689941\n",
      "[step: 720] loss: 12.094535827636719\n",
      "[step: 721] loss: 12.09199333190918\n",
      "[step: 722] loss: 12.089452743530273\n",
      "[step: 723] loss: 12.08690071105957\n",
      "[step: 724] loss: 12.084344863891602\n",
      "[step: 725] loss: 12.081788063049316\n",
      "[step: 726] loss: 12.079229354858398\n",
      "[step: 727] loss: 12.076665878295898\n",
      "[step: 728] loss: 12.074105262756348\n",
      "[step: 729] loss: 12.071532249450684\n",
      "[step: 730] loss: 12.068964004516602\n",
      "[step: 731] loss: 12.066387176513672\n",
      "[step: 732] loss: 12.063811302185059\n",
      "[step: 733] loss: 12.061227798461914\n",
      "[step: 734] loss: 12.058646202087402\n",
      "[step: 735] loss: 12.056052207946777\n",
      "[step: 736] loss: 12.053464889526367\n",
      "[step: 737] loss: 12.050869941711426\n",
      "[step: 738] loss: 12.048271179199219\n",
      "[step: 739] loss: 12.045674324035645\n",
      "[step: 740] loss: 12.043068885803223\n",
      "[step: 741] loss: 12.040459632873535\n",
      "[step: 742] loss: 12.037842750549316\n",
      "[step: 743] loss: 12.03522777557373\n",
      "[step: 744] loss: 12.032611846923828\n",
      "[step: 745] loss: 12.029985427856445\n",
      "[step: 746] loss: 12.027360916137695\n",
      "[step: 747] loss: 12.024731636047363\n",
      "[step: 748] loss: 12.022089004516602\n",
      "[step: 749] loss: 12.019461631774902\n",
      "[step: 750] loss: 12.016817092895508\n",
      "[step: 751] loss: 12.014169692993164\n",
      "[step: 752] loss: 12.01152515411377\n",
      "[step: 753] loss: 12.008872032165527\n",
      "[step: 754] loss: 12.00621223449707\n",
      "[step: 755] loss: 12.003552436828613\n",
      "[step: 756] loss: 12.00088882446289\n",
      "[step: 757] loss: 11.998218536376953\n",
      "[step: 758] loss: 11.9955472946167\n",
      "[step: 759] loss: 11.992866516113281\n",
      "[step: 760] loss: 11.990187644958496\n",
      "[step: 761] loss: 11.987505912780762\n",
      "[step: 762] loss: 11.98481273651123\n",
      "[step: 763] loss: 11.982121467590332\n",
      "[step: 764] loss: 11.979422569274902\n",
      "[step: 765] loss: 11.976722717285156\n",
      "[step: 766] loss: 11.974014282226562\n",
      "[step: 767] loss: 11.971306800842285\n",
      "[step: 768] loss: 11.968591690063477\n",
      "[step: 769] loss: 11.965868949890137\n",
      "[step: 770] loss: 11.963146209716797\n",
      "[step: 771] loss: 11.960420608520508\n",
      "[step: 772] loss: 11.957688331604004\n",
      "[step: 773] loss: 11.954952239990234\n",
      "[step: 774] loss: 11.952211380004883\n",
      "[step: 775] loss: 11.949467658996582\n",
      "[step: 776] loss: 11.946714401245117\n",
      "[step: 777] loss: 11.943963050842285\n",
      "[step: 778] loss: 11.941200256347656\n",
      "[step: 779] loss: 11.938438415527344\n",
      "[step: 780] loss: 11.935664176940918\n",
      "[step: 781] loss: 11.932897567749023\n",
      "[step: 782] loss: 11.93011474609375\n",
      "[step: 783] loss: 11.927330017089844\n",
      "[step: 784] loss: 11.924549102783203\n",
      "[step: 785] loss: 11.92175579071045\n",
      "[step: 786] loss: 11.91895866394043\n",
      "[step: 787] loss: 11.916159629821777\n",
      "[step: 788] loss: 11.913359642028809\n",
      "[step: 789] loss: 11.910560607910156\n",
      "[step: 790] loss: 11.907771110534668\n",
      "[step: 791] loss: 11.905011177062988\n",
      "[step: 792] loss: 11.90231704711914\n",
      "[step: 793] loss: 11.899758338928223\n",
      "[step: 794] loss: 11.897522926330566\n",
      "[step: 795] loss: 11.896013259887695\n",
      "[step: 796] loss: 11.896100997924805\n",
      "[step: 797] loss: 11.899965286254883\n",
      "[step: 798] loss: 11.912060737609863\n",
      "[step: 799] loss: 11.944899559020996\n",
      "[step: 800] loss: 12.020487785339355\n",
      "[step: 801] loss: 12.214432716369629\n",
      "[step: 802] loss: 12.607015609741211\n",
      "[step: 803] loss: 13.631126403808594\n",
      "[step: 804] loss: 14.994306564331055\n",
      "[step: 805] loss: 18.162538528442383\n",
      "[step: 806] loss: 17.315956115722656\n",
      "[step: 807] loss: 16.123502731323242\n",
      "[step: 808] loss: 12.419984817504883\n",
      "[step: 809] loss: 12.785618782043457\n",
      "[step: 810] loss: 15.609644889831543\n",
      "[step: 811] loss: 14.515731811523438\n",
      "[step: 812] loss: 12.503607749938965\n",
      "[step: 813] loss: 12.062593460083008\n",
      "[step: 814] loss: 13.55370044708252\n",
      "[step: 815] loss: 14.348108291625977\n",
      "[step: 816] loss: 12.47661304473877\n",
      "[step: 817] loss: 12.022977828979492\n",
      "[step: 818] loss: 13.17768669128418\n",
      "[step: 819] loss: 12.875099182128906\n",
      "[step: 820] loss: 11.974853515625\n",
      "[step: 821] loss: 12.156437873840332\n",
      "[step: 822] loss: 12.686474800109863\n",
      "[step: 823] loss: 12.39980411529541\n",
      "[step: 824] loss: 11.859423637390137\n",
      "[step: 825] loss: 12.187219619750977\n",
      "[step: 826] loss: 12.561552047729492\n",
      "[step: 827] loss: 12.10518741607666\n",
      "[step: 828] loss: 11.844513893127441\n",
      "[step: 829] loss: 12.145188331604004\n",
      "[step: 830] loss: 12.207389831542969\n",
      "[step: 831] loss: 11.924540519714355\n",
      "[step: 832] loss: 11.849396705627441\n",
      "[step: 833] loss: 12.057052612304688\n",
      "[step: 834] loss: 12.068323135375977\n",
      "[step: 835] loss: 11.84760856628418\n",
      "[step: 836] loss: 11.847389221191406\n",
      "[step: 837] loss: 11.997137069702148\n",
      "[step: 838] loss: 11.947296142578125\n",
      "[step: 839] loss: 11.811503410339355\n",
      "[step: 840] loss: 11.826766014099121\n",
      "[step: 841] loss: 11.914265632629395\n",
      "[step: 842] loss: 11.883151054382324\n",
      "[step: 843] loss: 11.791051864624023\n",
      "[step: 844] loss: 11.809746742248535\n",
      "[step: 845] loss: 11.869880676269531\n",
      "[step: 846] loss: 11.836705207824707\n",
      "[step: 847] loss: 11.77801513671875\n",
      "[step: 848] loss: 11.78803539276123\n",
      "[step: 849] loss: 11.82411003112793\n",
      "[step: 850] loss: 11.809001922607422\n",
      "[step: 851] loss: 11.766831398010254\n",
      "[step: 852] loss: 11.768420219421387\n",
      "[step: 853] loss: 11.793722152709961\n",
      "[step: 854] loss: 11.78476333618164\n",
      "[step: 855] loss: 11.755870819091797\n",
      "[step: 856] loss: 11.750641822814941\n",
      "[step: 857] loss: 11.766215324401855\n",
      "[step: 858] loss: 11.766576766967773\n",
      "[step: 859] loss: 11.746424674987793\n",
      "[step: 860] loss: 11.736105918884277\n",
      "[step: 861] loss: 11.743581771850586\n",
      "[step: 862] loss: 11.747018814086914\n",
      "[step: 863] loss: 11.736040115356445\n",
      "[step: 864] loss: 11.724499702453613\n",
      "[step: 865] loss: 11.725069046020508\n",
      "[step: 866] loss: 11.728984832763672\n",
      "[step: 867] loss: 11.723880767822266\n",
      "[step: 868] loss: 11.714207649230957\n",
      "[step: 869] loss: 11.710184097290039\n",
      "[step: 870] loss: 11.711736679077148\n",
      "[step: 871] loss: 11.71057415008545\n",
      "[step: 872] loss: 11.703916549682617\n",
      "[step: 873] loss: 11.6979341506958\n",
      "[step: 874] loss: 11.69650650024414\n",
      "[step: 875] loss: 11.696173667907715\n",
      "[step: 876] loss: 11.6926851272583\n",
      "[step: 877] loss: 11.687032699584961\n",
      "[step: 878] loss: 11.68319320678711\n",
      "[step: 879] loss: 11.681800842285156\n",
      "[step: 880] loss: 11.679990768432617\n",
      "[step: 881] loss: 11.676103591918945\n",
      "[step: 882] loss: 11.671548843383789\n",
      "[step: 883] loss: 11.668402671813965\n",
      "[step: 884] loss: 11.666481018066406\n",
      "[step: 885] loss: 11.664026260375977\n",
      "[step: 886] loss: 11.660357475280762\n",
      "[step: 887] loss: 11.656440734863281\n",
      "[step: 888] loss: 11.653376579284668\n",
      "[step: 889] loss: 11.651007652282715\n",
      "[step: 890] loss: 11.648337364196777\n",
      "[step: 891] loss: 11.644963264465332\n",
      "[step: 892] loss: 11.641373634338379\n",
      "[step: 893] loss: 11.63821029663086\n",
      "[step: 894] loss: 11.635504722595215\n",
      "[step: 895] loss: 11.632752418518066\n",
      "[step: 896] loss: 11.629610061645508\n",
      "[step: 897] loss: 11.626230239868164\n",
      "[step: 898] loss: 11.622980117797852\n",
      "[step: 899] loss: 11.620019912719727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 900] loss: 11.617170333862305\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={\n",
    "                                X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    \n",
    "    test_predict = scaler2.inverse_transform(test_predict)\n",
    "#     dataY2 = scaler2.inverse_transform(dataY2) # 데이터 있을때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "plt.title('2018-02-08 predict')\n",
    "# plt.plot(dataY2, label='row data') # 데이터 있을때\n",
    "plt.plot(test_predict, label='predict')\n",
    "plt.xlabel(\"Time Period\")\n",
    "plt.ylabel(\"uvb\")\n",
    "# plt.ylim(0, 140000)\n",
    "plt.xticks(np.arange(0, 901, step = 56), ['5', '6', '7', '8', '9', '10', '11', '12', '1', '2', '3', '4', '5', '6', '7', '8']) # v2 모델\n",
    "plt.yticks(np.arange(-0.25,2.25, step=0.25))\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
