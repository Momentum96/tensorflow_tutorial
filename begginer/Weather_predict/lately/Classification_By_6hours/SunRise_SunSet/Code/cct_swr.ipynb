{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-13 6:00:00 6:00:00\n",
      "2017-04-13 19:00:00 19:00:00\n",
      "2017-04-13 am saved.\n",
      "2017-04-13 pm saved.\n",
      "----------------------------------------------\n",
      "2017-04-19 5:52:00 5:52:00\n",
      "2017-04-19 19:52:00 19:52:00\n",
      "2017-04-19 am saved.\n",
      "2017-04-19 pm saved.\n",
      "----------------------------------------------\n",
      "2017-04-21 5:49:00 5:49:00\n",
      "2017-04-21 19:49:00 19:49:00\n",
      "2017-04-21 am saved.\n",
      "2017-04-21 pm saved.\n",
      "----------------------------------------------\n",
      "2017-04-22 5:48:00 5:48:00\n",
      "2017-04-22 19:48:00 19:48:00\n",
      "2017-04-22 am saved.\n",
      "2017-04-22 pm saved.\n",
      "----------------------------------------------\n",
      "2017-04-24 5:45:00 5:45:00\n",
      "2017-04-24 19:54:00 19:45:00\n",
      "2017-04-24 am saved.\n",
      "2017-04-24 pm saved.\n",
      "----------------------------------------------\n",
      "2017-05-01 5:37:00 5:37:00\n",
      "2017-05-01 19:37:00 19:37:00\n",
      "2017-05-01 am saved.\n",
      "2017-05-01 pm saved.\n",
      "----------------------------------------------\n",
      "2017-05-02 5:36:00 5:36:00\n",
      "2017-05-02 19:36:00 19:36:00\n",
      "2017-05-02 am saved.\n",
      "2017-05-02 pm saved.\n",
      "----------------------------------------------\n",
      "2017-05-15 5:23:00 5:23:00\n",
      "2017-05-15 19:23:00 19:23:00\n",
      "2017-05-15 am saved.\n",
      "2017-05-15 pm saved.\n",
      "----------------------------------------------\n",
      "2017-05-17 5:22:00 5:22:00\n",
      "2017-05-17 19:22:00 19:22:00\n",
      "2017-05-17 am saved.\n",
      "2017-05-17 pm saved.\n",
      "----------------------------------------------\n",
      "2017-05-18 5:21:00 5:21:00\n",
      "2017-05-18 19:21:00 19:21:00\n",
      "2017-05-18 am saved.\n",
      "2017-05-18 pm saved.\n",
      "----------------------------------------------\n",
      "2017-05-22 5:18:00 5:18:00\n",
      "2017-05-22 19:18:00 19:18:00\n",
      "2017-05-22 am saved.\n",
      "2017-05-22 pm saved.\n",
      "----------------------------------------------\n",
      "2017-05-23 5:18:00 5:18:00\n",
      "2017-05-23 19:18:00 19:18:00\n",
      "2017-05-23 am saved.\n",
      "2017-05-23 pm saved.\n",
      "----------------------------------------------\n",
      "2017-05-25 5:16:00 5:16:00\n",
      "2017-05-25 19:16:00 19:16:00\n",
      "2017-05-25 am saved.\n",
      "2017-05-25 pm saved.\n",
      "----------------------------------------------\n",
      "2017-05-30 5:15:00 5:14:00\n",
      "2017-05-30 19:14:00 19:14:00\n",
      "2017-05-30 am saved.\n",
      "2017-05-30 pm saved.\n",
      "----------------------------------------------\n",
      "2017-06-02 5:13:00 5:13:00\n",
      "2017-06-02 19:13:00 19:13:00\n",
      "2017-06-02 am saved.\n",
      "2017-06-02 pm saved.\n",
      "----------------------------------------------\n",
      "2017-06-05 5:12:00 5:12:00\n",
      "2017-06-05 19:12:00 19:12:00\n",
      "2017-06-05 am saved.\n",
      "2017-06-05 pm saved.\n",
      "----------------------------------------------\n",
      "2017-06-09 5:11:00 5:11:00\n",
      "2017-06-09 19:11:00 19:11:00\n",
      "2017-06-09 am saved.\n",
      "2017-06-09 pm saved.\n",
      "----------------------------------------------\n",
      "2017-06-10 5:11:00 5:11:00\n",
      "2017-06-10 19:11:00 19:11:00\n",
      "2017-06-10 am saved.\n",
      "2017-06-10 pm saved.\n",
      "----------------------------------------------\n",
      "2017-06-14 5:11:00 5:11:00\n",
      "2017-06-14 19:11:00 19:11:00\n",
      "2017-06-14 am saved.\n",
      "2017-06-14 pm saved.\n",
      "----------------------------------------------\n",
      "2017-06-15 5:11:00 5:11:00\n",
      "2017-06-15 19:11:00 19:11:00\n",
      "2017-06-15 am saved.\n",
      "2017-06-15 pm saved.\n",
      "----------------------------------------------\n",
      "2017-06-19 5:12:00 5:12:00\n",
      "2017-06-19 19:12:00 19:12:00\n",
      "2017-06-19 am saved.\n",
      "2017-06-19 pm saved.\n",
      "----------------------------------------------\n",
      "2017-06-20 5:12:00 5:12:00\n",
      "2017-06-20 19:12:00 19:12:00\n",
      "2017-06-20 am saved.\n",
      "2017-06-20 pm saved.\n",
      "----------------------------------------------\n",
      "2017-06-21 5:12:00 5:12:00\n",
      "2017-06-21 19:12:00 19:12:00\n",
      "2017-06-21 am saved.\n",
      "2017-06-21 pm saved.\n",
      "----------------------------------------------\n",
      "2017-06-22 5:12:00 5:12:00\n",
      "2017-06-22 19:12:00 19:12:00\n",
      "2017-06-22 am saved.\n",
      "2017-06-22 pm saved.\n",
      "----------------------------------------------\n",
      "2017-07-12 5:22:00 5:21:00\n",
      "2017-07-12 19:21:00 19:21:00\n",
      "2017-07-12 am saved.\n",
      "2017-07-12 pm saved.\n",
      "----------------------------------------------\n",
      "2017-07-13 5:22:00 5:22:00\n",
      "2017-07-13 19:22:00 19:22:00\n",
      "2017-07-13 am saved.\n",
      "2017-07-13 pm saved.\n",
      "----------------------------------------------\n",
      "2017-07-20 5:27:00 5:27:00\n",
      "2017-07-20 19:27:00 19:27:00\n",
      "2017-07-20 am saved.\n",
      "2017-07-20 pm saved.\n",
      "----------------------------------------------\n",
      "2017-07-26 5:32:00 5:32:00\n",
      "2017-07-26 19:32:00 19:32:00\n",
      "2017-07-26 am saved.\n",
      "2017-07-26 pm saved.\n",
      "----------------------------------------------\n",
      "2017-07-27 5:32:00 5:32:00\n",
      "2017-07-27 19:32:00 19:32:00\n",
      "2017-07-27 am saved.\n",
      "2017-07-27 pm saved.\n",
      "----------------------------------------------\n",
      "2017-08-02 5:38:00 5:37:00\n",
      "2017-08-02 19:37:00 19:37:00\n",
      "2017-08-02 am saved.\n",
      "2017-08-02 pm saved.\n",
      "----------------------------------------------\n",
      "2017-08-22 5:54:00 5:54:00\n",
      "2017-08-22 19:54:00 19:54:00\n",
      "2017-08-22 am saved.\n",
      "2017-08-22 pm saved.\n",
      "----------------------------------------------\n",
      "2017-08-26 5:57:00 5:57:00\n",
      "2017-08-26 19:57:00 19:57:00\n",
      "2017-08-26 am saved.\n",
      "2017-08-26 pm saved.\n",
      "----------------------------------------------\n",
      "2017-08-30 6:00:00 6:00:00\n",
      "2017-08-30 19:00:00 19:00:00\n",
      "2017-08-30 am saved.\n",
      "2017-08-30 pm saved.\n",
      "----------------------------------------------\n",
      "2017-08-31 6:01:00 6:01:00\n",
      "2017-08-31 19:01:00 19:01:00\n",
      "2017-08-31 am saved.\n",
      "2017-08-31 pm saved.\n",
      "----------------------------------------------\n",
      "2017-09-01 6:02:00 6:02:00\n",
      "2017-09-01 19:02:00 19:02:00\n",
      "2017-09-01 am saved.\n",
      "2017-09-01 pm saved.\n",
      "----------------------------------------------\n",
      "2017-09-08 6:07:00 6:07:00\n",
      "2017-09-08 18:07:00 18:07:00\n",
      "2017-09-08 am saved.\n",
      "2017-09-08 pm saved.\n",
      "----------------------------------------------\n",
      "2017-09-09 6:09:00 6:08:00\n",
      "2017-09-09 18:08:00 18:08:00\n",
      "2017-09-09 am saved.\n",
      "2017-09-09 pm saved.\n",
      "----------------------------------------------\n",
      "2017-09-10 6:09:00 6:09:00\n",
      "2017-09-10 18:09:00 18:09:00\n",
      "2017-09-10 am saved.\n",
      "2017-09-10 pm saved.\n",
      "----------------------------------------------\n",
      "2017-09-12 6:11:00 6:11:00\n",
      "2017-09-12 18:11:00 18:11:00\n",
      "2017-09-12 am saved.\n",
      "2017-09-12 pm saved.\n",
      "----------------------------------------------\n",
      "2017-09-13 6:11:00 6:11:00\n",
      "2017-09-13 18:11:00 18:11:00\n",
      "2017-09-13 am saved.\n",
      "2017-09-13 pm saved.\n",
      "----------------------------------------------\n",
      "2017-09-14 6:12:00 6:12:00\n",
      "2017-09-14 18:12:00 18:12:00\n",
      "2017-09-14 am saved.\n",
      "2017-09-14 pm saved.\n",
      "----------------------------------------------\n",
      "2017-09-18 6:15:00 6:15:00\n",
      "2017-09-18 18:15:00 18:15:00\n",
      "2017-09-18 am saved.\n",
      "2017-09-18 pm saved.\n",
      "----------------------------------------------\n",
      "2017-09-21 6:18:00 6:18:00\n",
      "2017-09-21 18:18:00 18:18:00\n",
      "2017-09-21 am saved.\n",
      "2017-09-21 pm saved.\n",
      "----------------------------------------------\n",
      "2017-09-22 6:19:00 6:19:00\n",
      "2017-09-22 18:19:00 18:19:00\n",
      "2017-09-22 am saved.\n",
      "2017-09-22 pm saved.\n",
      "----------------------------------------------\n",
      "2017-09-24 6:20:00 6:20:00\n",
      "2017-09-24 18:20:00 18:20:00\n",
      "2017-09-24 am saved.\n",
      "2017-09-24 pm saved.\n",
      "----------------------------------------------\n",
      "2017-09-28 6:24:00 6:24:00\n",
      "2017-09-28 18:24:00 18:24:00\n",
      "2017-09-28 am saved.\n",
      "2017-09-28 pm saved.\n",
      "----------------------------------------------\n",
      "2017-10-08 6:32:00 6:32:00\n",
      "2017-10-08 18:32:00 18:32:00\n",
      "2017-10-08 am saved.\n",
      "2017-10-08 pm saved.\n",
      "----------------------------------------------\n",
      "2017-10-09 6:33:00 6:33:00\n",
      "2017-10-09 18:33:00 18:33:00\n",
      "2017-10-09 am saved.\n",
      "2017-10-09 pm saved.\n",
      "----------------------------------------------\n",
      "2017-10-14 6:37:00 6:37:00\n",
      "2017-10-14 17:37:00 17:37:00\n",
      "2017-10-14 am saved.\n",
      "2017-10-14 pm saved.\n",
      "----------------------------------------------\n",
      "2017-10-19 6:42:00 6:42:00\n",
      "2017-10-19 17:42:00 17:42:00\n",
      "2017-10-19 am saved.\n",
      "2017-10-19 pm saved.\n",
      "----------------------------------------------\n",
      "2017-10-21 6:44:00 6:44:00\n",
      "2017-10-21 17:44:00 17:44:00\n",
      "2017-10-21 am saved.\n",
      "2017-10-21 pm saved.\n",
      "----------------------------------------------\n",
      "2017-10-24 6:47:00 6:47:00\n",
      "2017-10-24 17:47:00 17:47:00\n",
      "2017-10-24 am saved.\n",
      "2017-10-24 pm saved.\n",
      "----------------------------------------------\n",
      "2017-10-30 6:53:00 6:53:00\n",
      "2017-10-30 17:53:00 17:53:00\n",
      "2017-10-30 am saved.\n",
      "2017-10-30 pm saved.\n",
      "----------------------------------------------\n",
      "2017-10-31 6:54:00 6:54:00\n",
      "2017-10-31 17:54:00 17:54:00\n",
      "2017-10-31 am saved.\n",
      "2017-10-31 pm saved.\n",
      "----------------------------------------------\n",
      "2017-11-11 7:05:00 7:05:00\n",
      "2017-11-11 17:05:00 17:05:00\n",
      "2017-11-11 am saved.\n",
      "2017-11-11 pm saved.\n",
      "----------------------------------------------\n",
      "2017-11-12 7:06:00 7:06:00\n",
      "2017-11-12 17:06:00 17:06:00\n",
      "2017-11-12 am saved.\n",
      "2017-11-12 pm saved.\n",
      "----------------------------------------------\n",
      "2017-11-15 7:09:00 7:09:00\n",
      "2017-11-15 17:09:00 17:09:00\n",
      "2017-11-15 am saved.\n",
      "2017-11-15 pm saved.\n",
      "----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-16 7:10:00 7:10:00\n",
      "2017-11-16 17:10:00 17:10:00\n",
      "2017-11-16 am saved.\n",
      "2017-11-16 pm saved.\n",
      "----------------------------------------------\n",
      "2017-12-13 7:35:00 7:35:00\n",
      "2017-12-13 17:35:00 17:35:00\n",
      "2017-12-13 am saved.\n",
      "2017-12-13 pm saved.\n",
      "----------------------------------------------\n",
      "2017-12-27 7:42:00 7:42:00\n",
      "2017-12-27 17:42:00 17:42:00\n",
      "2017-12-27 am saved.\n",
      "2017-12-27 pm saved.\n",
      "----------------------------------------------\n",
      "2017-12-28 7:43:00 7:43:00\n",
      "2017-12-28 17:43:00 17:43:00\n",
      "2017-12-28 am saved.\n",
      "2017-12-28 pm saved.\n",
      "----------------------------------------------\n",
      "2018-01-03 7:44:00 7:44:00\n",
      "2018-01-03 17:44:00 17:44:00\n",
      "2018-01-03 am saved.\n",
      "2018-01-03 pm saved.\n",
      "----------------------------------------------\n",
      "2018-01-05 7:45:00 7:44:00\n",
      "2018-01-05 17:44:00 17:44:00\n",
      "2018-01-05 am saved.\n",
      "2018-01-05 pm saved.\n",
      "----------------------------------------------\n",
      "2018-01-06 7:44:00 7:44:00\n",
      "2018-01-06 17:44:00 17:44:00\n",
      "2018-01-06 am saved.\n",
      "2018-01-06 pm saved.\n",
      "----------------------------------------------\n",
      "2018-01-07 7:44:00 7:44:00\n",
      "2018-01-07 17:44:00 17:44:00\n",
      "2018-01-07 am saved.\n",
      "2018-01-07 pm saved.\n",
      "----------------------------------------------\n",
      "2018-02-16 7:19:00 7:19:00\n",
      "2018-02-16 18:19:00 18:19:00\n",
      "2018-02-16 am saved.\n",
      "2018-02-16 pm saved.\n",
      "----------------------------------------------\n",
      "2018-02-17 7:18:00 7:18:00\n",
      "2018-02-17 18:18:00 18:18:00\n",
      "2018-02-17 am saved.\n",
      "2018-02-17 pm saved.\n",
      "----------------------------------------------\n",
      "2018-02-24 7:09:00 7:09:00\n",
      "2018-02-24 18:09:00 18:09:00\n",
      "2018-02-24 am saved.\n",
      "2018-02-24 pm saved.\n",
      "----------------------------------------------\n",
      "2018-02-25 7:08:00 7:08:00\n",
      "2018-02-25 18:08:00 18:08:00\n",
      "2018-02-25 am saved.\n",
      "2018-02-25 pm saved.\n",
      "----------------------------------------------\n",
      "2018-02-27 7:05:00 7:05:00\n",
      "2018-02-27 18:05:00 18:05:00\n",
      "2018-02-27 am saved.\n",
      "2018-02-27 pm saved.\n",
      "----------------------------------------------\n",
      "2018-03-03 7:00:00 7:00:00\n",
      "2018-03-03 18:00:00 18:00:00\n",
      "2018-03-03 am saved.\n",
      "2018-03-03 pm saved.\n",
      "----------------------------------------------\n",
      "2018-03-07 6:54:00 6:54:00\n",
      "2018-03-07 18:54:00 18:54:00\n",
      "2018-03-07 am saved.\n",
      "2018-03-07 pm saved.\n",
      "----------------------------------------------\n",
      "2018-03-10 6:50:00 6:50:00\n",
      "2018-03-10 18:50:00 18:50:00\n",
      "2018-03-10 am saved.\n",
      "2018-03-10 pm saved.\n",
      "----------------------------------------------\n",
      "2018-03-24 6:29:00 6:29:00\n",
      "2018-03-24 18:29:00 18:29:00\n",
      "2018-03-24 am saved.\n",
      "2018-03-24 pm saved.\n",
      "----------------------------------------------\n",
      "2018-03-26 6:26:00 6:26:00\n",
      "2018-03-26 18:26:00 18:26:00\n",
      "2018-03-26 am saved.\n",
      "2018-03-26 pm saved.\n",
      "----------------------------------------------\n",
      "2018-03-27 6:25:00 6:25:00\n",
      "2018-03-27 18:25:00 18:25:00\n",
      "2018-03-27 am saved.\n",
      "2018-03-27 pm saved.\n",
      "----------------------------------------------\n",
      "2018-03-31 6:19:00 6:19:00\n",
      "2018-03-31 18:19:00 18:19:00\n",
      "2018-03-31 am saved.\n",
      "2018-03-31 pm saved.\n",
      "----------------------------------------------\n",
      "2018-04-07 6:09:00 6:09:00\n",
      "2018-04-07 18:09:00 18:09:00\n",
      "2018-04-07 am saved.\n",
      "2018-04-07 pm saved.\n",
      "----------------------------------------------\n",
      "2018-04-09 6:06:00 6:06:00\n",
      "2018-04-09 18:06:00 18:06:00\n",
      "2018-04-09 am saved.\n",
      "2018-04-09 pm saved.\n",
      "----------------------------------------------\n",
      "2018-04-10 6:04:00 6:04:00\n",
      "2018-04-10 19:27:00 19:04:00\n",
      "2018-04-10 am saved.\n",
      "2018-04-10 pm saved.\n",
      "----------------------------------------------\n",
      "2018-04-12 6:03:00 6:02:00\n",
      "2018-04-12 19:02:00 19:02:00\n",
      "2018-04-12 am saved.\n",
      "2018-04-12 pm saved.\n",
      "----------------------------------------------\n",
      "2018-04-13 6:00:00 6:00:00\n",
      "2018-04-13 19:00:00 19:00:00\n",
      "2018-04-13 am saved.\n",
      "2018-04-13 pm saved.\n",
      "----------------------------------------------\n",
      "2018-04-16 5:56:00 5:56:00\n",
      "2018-04-16 19:56:00 19:56:00\n",
      "2018-04-16 am saved.\n",
      "2018-04-16 pm saved.\n",
      "----------------------------------------------\n",
      "2018-04-17 5:55:00 5:55:00\n",
      "2018-04-17 19:55:00 19:55:00\n",
      "2018-04-17 am saved.\n",
      "2018-04-17 pm saved.\n",
      "----------------------------------------------\n",
      "2018-04-18 5:53:00 5:53:00\n",
      "2018-04-18 19:53:00 19:53:00\n",
      "2018-04-18 am saved.\n",
      "2018-04-18 pm saved.\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 분별 날짜 파일에 있는 날짜 기준으로 DB에서 데이터 받아와서 저장하기\n",
    "\n",
    "import MySQLdb\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "# MySQL DB 연결\n",
    "db = MySQLdb.connect('210.102.142.13',\"root\", \"witlab8*\", \"cas_db\")\n",
    "c = db.cursor()\n",
    "\n",
    "# 분별 날짜\n",
    "date = np.genfromtxt('../data/date_sunrise_sunset.csv', delimiter=',', dtype='str')\n",
    "# date = np.genfromtxt('../data/test_date_sunrise_sunset.csv', delimiter=',', dtype='str')\n",
    "\n",
    "# csv 파일로 내보내기\n",
    "w = open('../data/db_connect_data_am.csv', 'w', encoding='utf-8')\n",
    "# w = open('../data/db_connect_test_data_am.csv', 'w', encoding='utf-8')\n",
    "wr = csv.writer(w)\n",
    "\n",
    "# csv 파일로 내보내기\n",
    "w2 = open('../data/db_connect_data_pm.csv', 'w', encoding='utf-8')\n",
    "# w2 = open('../data/db_connect_test_data_pm.csv', 'w', encoding='utf-8')\n",
    "wr2 = csv.writer(w2)\n",
    "\n",
    "# 각 날짜별 데이터들 DB에서 가져오고 csv 파일로 저장\n",
    "# 데이터는 cct, swr, uvb, uvi 순\n",
    "for j in range(len(date)):\n",
    "    sql = \"select time(date), date(date), cct, cas_swr from natural_tracker left outer join cas_wave_ratio using(date) where date(date) = '\"+ str(date[j][0]) + \"' order by time(date)\"\n",
    "    c.execute(sql)\n",
    "    rows = c.fetchall()\n",
    "    \n",
    "    # 일출 후 6시간과 일몰 후 6시간 데이터 사용, 일출 혹은 일몰 당시 데이터가 없을 경우 가장 가까운 다른 데이터로 변환\n",
    "    sunrise = datetime.datetime.strptime(date[j][1], '%H:%M:%S')\n",
    "    sunset = datetime.datetime.strptime(date[j][2], '%H:%M:%S')\n",
    "    \n",
    "    standard = datetime.datetime.strptime('1900-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    sunrise = sunrise - standard\n",
    "    sunset = sunset - standard\n",
    "    \n",
    "    start = 0\n",
    "    end = 0\n",
    "\n",
    "    for i in range(len(rows)):\n",
    "        if(rows[i][0] >= sunrise):\n",
    "            start = i\n",
    "            print(date[j][0], rows[start][0], sunrise)\n",
    "            break;\n",
    "    \n",
    "    for i in range(len(rows)):\n",
    "        if(rows[i][0] >= sunset):\n",
    "            end = i\n",
    "            print(date[j][0], rows[end][0], sunset)\n",
    "            break;\n",
    "    \n",
    "#     # start에 저장된 index부터 772개 데이터 가져와서 저장\n",
    "    for l in range(start, start+360):\n",
    "        wr.writerow([rows[l][2], rows[l][3]])\n",
    "    print(date[j][0] + \" am saved.\")\n",
    "    \n",
    "    for l in range(end-360, end):\n",
    "        wr2.writerow([rows[l][2], rows[l][3]])\n",
    "    print(date[j][0] + \" pm saved.\")\n",
    "    \n",
    "    print('----------------------------------------------')\n",
    "        \n",
    "w.close()\n",
    "w2.close()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = np.genfromtxt('../data/date_sunrise_sunset.csv', delimiter=',', dtype='str')\n",
    "# date = np.genfromtxt('../data/test_date_sunrise_sunset.csv', delimiter=',', dtype='str')\n",
    "\n",
    "# 데이터 받아오고 편차 계산히기\n",
    "import_data = np.loadtxt('../data/db_connect_data_am.csv', delimiter=',')\n",
    "# import_data = np.loadtxt('../data/db_connect_test_data_am.csv', delimiter=',')\n",
    "\n",
    "one = import_data[:,0] # cct\n",
    "two = import_data[:,1] # cas_swr\n",
    "\n",
    "# # 날짜별 772개의 데이터 모두 저장, 2차원 배열\n",
    "cct_am = []\n",
    "swr_am = []\n",
    "\n",
    "# 날짜별 데이터의 편차 (데이터 수 - 1)개의 데이터 모두 저장, 2차원 배열\n",
    "delta_cct_am = []\n",
    "delta_swr_am = []\n",
    "\n",
    "# 각 날짜별 raw data 총 합 저장, 2차원 배열\n",
    "cct_hap_am = []\n",
    "swr_hap_am = []\n",
    "\n",
    "# raw data들 저장\n",
    "for cnt in range(len(date)): # 전체 데이터 날짜 수 (행)\n",
    "    temp = []\n",
    "    temp2 = []\n",
    "    for i in range(360): # 한 날짜의 데이터들 한 행에 저장 (열)\n",
    "        temp.append(one[i+360*cnt])\n",
    "        temp2.append(two[i+360*cnt])\n",
    "    cct_am.append(temp)\n",
    "    swr_am.append(temp2)\n",
    "\n",
    "# 편차 데이터 저장\n",
    "for cnt in range(len(date)): \n",
    "    temp = []\n",
    "    temp2 = []\n",
    "    for i in range(359):\n",
    "        temp.append(cct_am[cnt][i+1] - cct_am[cnt][i])\n",
    "        temp2.append(swr_am[cnt][i+1] - swr_am[cnt][i])\n",
    "    delta_cct_am.append(temp)\n",
    "    delta_swr_am.append(temp2)\n",
    "\n",
    "# raw data 합 저장\n",
    "for i in range(len(date)):\n",
    "    temp1 = 0\n",
    "    temp2 = 0\n",
    "    for j in range(len(cct_am[0])):\n",
    "        temp1 += cct_am[i][j]\n",
    "        temp2 += swr_am[i][j]\n",
    "    cct_hap_am.append(temp1)\n",
    "    swr_hap_am.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 받아오고 편차 계산히기\n",
    "import_data = np.loadtxt('../data/db_connect_data_pm.csv', delimiter=',')\n",
    "# import_data = np.loadtxt('../data/db_connect_test_data_pm.csv', delimiter=',')\n",
    "\n",
    "one = import_data[:,0] # cct\n",
    "two = import_data[:,1] # cas_swr\n",
    "\n",
    "# # 날짜별 772개의 데이터 모두 저장, 2차원 배열\n",
    "cct_pm = []\n",
    "swr_pm = []\n",
    "\n",
    "# 날짜별 데이터의 편차 (데이터 수 - 1)개의 데이터 모두 저장, 2차원 배열\n",
    "delta_cct_pm = []\n",
    "delta_swr_pm = []\n",
    "\n",
    "# 각 날짜별 raw data 총 합 저장, 2차원 배열\n",
    "cct_hap_pm = []\n",
    "swr_hap_pm = []\n",
    "\n",
    "# raw data들 저장\n",
    "for cnt in range(len(date)): # 전체 데이터 날짜 수 (행)\n",
    "    temp = []\n",
    "    temp2 = []\n",
    "    for i in range(360): # 한 날짜의 데이터들 한 행에 저장 (열)\n",
    "        temp.append(one[i+360*cnt])\n",
    "        temp2.append(two[i+360*cnt])\n",
    "    cct_pm.append(temp)\n",
    "    swr_pm.append(temp2)\n",
    "\n",
    "# 편차 데이터 저장\n",
    "for cnt in range(len(date)): \n",
    "    temp = []\n",
    "    temp2 = []\n",
    "    for i in range(359):\n",
    "        temp.append(cct_pm[cnt][i+1] - cct_pm[cnt][i])\n",
    "        temp2.append(swr_pm[cnt][i+1] - swr_pm[cnt][i])\n",
    "    delta_cct_pm.append(temp)\n",
    "    delta_swr_pm.append(temp2)\n",
    "\n",
    "# raw data 합 저장\n",
    "for i in range(len(date)):\n",
    "    temp1 = 0\n",
    "    temp2 = 0\n",
    "    for j in range(len(cct_pm[0])):\n",
    "        temp1 += cct_pm[i][j]\n",
    "        temp2 += swr_pm[i][j]\n",
    "    cct_hap_pm.append(temp1)\n",
    "    swr_hap_pm.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오전 6시 ~ 12시 데이터들의 편차 비율 계산\n",
    "\n",
    "w = open('../data/cct_swr_calculation_am.csv', 'w', encoding='utf-8')\n",
    "# w = open('../data/cct_swr_test_calculation_am.csv', 'w', encoding='utf-8')\n",
    "wr = csv.writer(w)\n",
    "\n",
    "for i in range(len(delta_cct_am)):\n",
    "    up_dot_25 = 0\n",
    "    up_dot_50 = 0\n",
    "    up_dot_100 = 0\n",
    "    up_dot_150 = 0\n",
    "    up_dot_200 = 0\n",
    "    up_dot_250 = 0\n",
    "    up_dot_300 = 0\n",
    "    up_dot_350 = 0\n",
    "    up_dot_400 = 0\n",
    "    up_dot_450 = 0\n",
    "    up_dot_500 = 0\n",
    "\n",
    "    # 위 변수들을 비율로 계산\n",
    "    percent_1 = 0\n",
    "    percent_2 = 0\n",
    "    percent_3 = 0\n",
    "    percent_4 = 0\n",
    "    percent_5 = 0\n",
    "    percent_6 = 0\n",
    "    percent_7 = 0\n",
    "    percent_8 = 0\n",
    "    percent_9 = 0\n",
    "    percent_10 = 0\n",
    "    percent_11 = 0\n",
    "\n",
    "    # 편차의 합, 평균, 등급 저장\n",
    "    hap = 0\n",
    "    avg = 0\n",
    "\n",
    "    # 편차 절댓값을 기준으로 갯수 저장\n",
    "    for j in range(len(delta_cct_am[i])):\n",
    "        if(abs(delta_cct_am[i][j]) >= 25):\n",
    "            up_dot_25 += 1\n",
    "        if(abs(delta_cct_am[i][j]) >= 50):\n",
    "            up_dot_50 += 1\n",
    "        if(abs(delta_cct_am[i][j]) >= 100):\n",
    "            up_dot_100 += 1\n",
    "        if(abs(delta_cct_am[i][j]) >= 150):\n",
    "            up_dot_150 += 1\n",
    "        if(abs(delta_cct_am[i][j]) >= 200):\n",
    "            up_dot_200 += 1\n",
    "        if(abs(delta_cct_am[i][j]) >= 250):\n",
    "            up_dot_250 += 1\n",
    "        if(abs(delta_cct_am[i][j]) >= 300):\n",
    "            up_dot_300 += 1\n",
    "        if(abs(delta_cct_am[i][j]) >= 350):\n",
    "            up_dot_350 += 1\n",
    "        if(abs(delta_cct_am[i][j]) >= 400):\n",
    "            up_dot_400 += 1\n",
    "        if(abs(delta_cct_am[i][j]) >= 450):\n",
    "            up_dot_450 += 1\n",
    "        if(abs(delta_cct_am[i][j]) >= 500):\n",
    "            up_dot_500 += 1\n",
    "        \n",
    "        # 편차 합 및 평균 저장\n",
    "        hap += abs(delta_cct_am[i][j])\n",
    "        avg = hap / 359\n",
    "\n",
    "        # 편차 갯수들이 차지하는 비율 소수점 둘째짜리까지 계산\n",
    "        percent_1 = round(up_dot_25 / 360 * 100, 2)\n",
    "        percent_2 = round(up_dot_50 / 360 * 100, 2)\n",
    "        percent_3 = round(up_dot_100 / 360 * 100, 2)\n",
    "        percent_4 = round(up_dot_150 / 360 * 100, 2)\n",
    "        percent_5 = round(up_dot_200 / 360 * 100, 2)\n",
    "        percent_6 = round(up_dot_250 / 360 * 100, 2)\n",
    "        percent_7 = round(up_dot_300 / 360 * 100, 2)\n",
    "        percent_8 = round(up_dot_350 / 360 * 100, 2)\n",
    "        percent_9 = round(up_dot_400 / 360 * 100, 2)\n",
    "        percent_10 = round(up_dot_450 / 360 * 100, 2)\n",
    "        percent_11 = round(up_dot_500 / 360 * 100, 2)\n",
    "        \n",
    "    # 편차의 절대값이 0.1 ~ 5이상인 경우의 갯수 저장\n",
    "    up_dot_1 = 0\n",
    "    up_dot_2 = 0\n",
    "    up_dot_3 = 0\n",
    "    up_dot_4 = 0\n",
    "    up_dot_5 = 0\n",
    "    up_dot_6 = 0\n",
    "    up_dot_7 = 0\n",
    "    up_dot_8 = 0\n",
    "    up_dot_9 = 0\n",
    "    up_dot_10 = 0\n",
    "    up_dot_20 = 0\n",
    "    up_dot_30 = 0\n",
    "    up_dot_40 = 0\n",
    "    up_dot_50_2 = 0\n",
    "\n",
    "    # 위의 갯수가 하루 전체 데이터에서 차지하는 비율 저장\n",
    "    percent_1 = 0\n",
    "    percent_2 = 0\n",
    "    percent_3 = 0\n",
    "    percent_4 = 0\n",
    "    percent_5 = 0\n",
    "    percent_6 = 0\n",
    "    percent_7 = 0\n",
    "    percent_8 = 0\n",
    "    percent_9 = 0\n",
    "    percent_10 = 0\n",
    "    percent_20 = 0\n",
    "    percent_30 = 0\n",
    "    percent_40 = 0\n",
    "    percent_50_2 = 0\n",
    "\n",
    "    # 편차의 합, 평균, 등급 저장\n",
    "    hap_2 = 0\n",
    "    avg_2 = 0\n",
    "\n",
    "    for j in range(len(delta_swr_am[i])):\n",
    "        if(abs(delta_swr_am[i][j]) >= 0.1):\n",
    "            up_dot_1 += 1\n",
    "        if(abs(delta_swr_am[i][j]) >= 0.2):\n",
    "            up_dot_2 += 1\n",
    "        if(abs(delta_swr_am[i][j]) >= 0.3):\n",
    "            up_dot_3 += 1\n",
    "        if(abs(delta_swr_am[i][j]) >= 0.4):\n",
    "            up_dot_4 += 1\n",
    "        if(abs(delta_swr_am[i][j]) >= 0.5):\n",
    "            up_dot_5 += 1\n",
    "        if(abs(delta_swr_am[i][j]) >= 0.6):\n",
    "            up_dot_6 += 1\n",
    "        if(abs(delta_swr_am[i][j]) >= 0.7):\n",
    "            up_dot_7 += 1\n",
    "        if(abs(delta_swr_am[i][j]) >= 0.8):\n",
    "            up_dot_8 += 1\n",
    "        if(abs(delta_swr_am[i][j]) >= 0.9):\n",
    "            up_dot_9 += 1\n",
    "        if(abs(delta_swr_am[i][j]) >= 1):\n",
    "            up_dot_10 += 1\n",
    "        if(abs(delta_swr_am[i][j] >= 2)):\n",
    "            up_dot_20 += 1\n",
    "        if(abs(delta_swr_am[i][j] >= 3)):\n",
    "            up_dot_30 += 1\n",
    "        if(abs(delta_swr_am[i][j] >= 4)):\n",
    "            up_dot_40 += 1\n",
    "        if(abs(delta_swr_am[i][j] >= 5)):\n",
    "            up_dot_50_2 += 1\n",
    "\n",
    "        hap_2 += abs(delta_swr_am[i][j])\n",
    "        avg_2 = hap / 359\n",
    "\n",
    "        percent_1 = round(up_dot_1 / 360 * 100, 2);\n",
    "        percent_2 = round(up_dot_2 / 360 * 100, 2);\n",
    "        percent_3 = round(up_dot_3 / 360 * 100, 2);\n",
    "        percent_4 = round(up_dot_4 / 360 * 100, 2);\n",
    "        percent_5 = round(up_dot_5 / 360 * 100, 2);\n",
    "        percent_6 = round(up_dot_6 / 360 * 100, 2);\n",
    "        percent_7 = round(up_dot_7 / 360 * 100, 2);\n",
    "        percent_8 = round(up_dot_8 / 360 * 100, 2);\n",
    "        percent_9 = round(up_dot_9 / 360 * 100, 2);\n",
    "        percent_10 = round(up_dot_10 / 360 * 100, 2);\n",
    "        percent_20 = round(up_dot_20 / 360 * 100, 2);\n",
    "        percent_30 = round(up_dot_30 / 360 * 100, 2);\n",
    "        percent_40 = round(up_dot_40 / 360 * 100, 2);\n",
    "        percent_50 = round(up_dot_50 / 360 * 100, 2);\n",
    "        \n",
    "    wr.writerow([up_dot_25, up_dot_50, up_dot_100, up_dot_150, up_dot_200, up_dot_250, up_dot_300, up_dot_350, up_dot_400, up_dot_450, up_dot_500, up_dot_1, up_dot_2, up_dot_3, up_dot_4, up_dot_5, up_dot_6, up_dot_7, up_dot_8, up_dot_9, up_dot_10, up_dot_20, up_dot_30, up_dot_40, up_dot_50_2])\n",
    "w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 색온도, 단파장의 편차 분포 저장\n",
    "# 색온도 기준 적용\n",
    "\n",
    "w = open('../data/cct_swr_calculation_pm.csv', 'w', encoding='utf-8')\n",
    "# w = open('../data/cct_swr_test_calculation_pm.csv', 'w', encoding='utf-8')\n",
    "wr = csv.writer(w)\n",
    "\n",
    "for i in range(len(delta_cct_pm)):\n",
    "    up_dot_25 = 0\n",
    "    up_dot_50 = 0\n",
    "    up_dot_100 = 0\n",
    "    up_dot_150 = 0\n",
    "    up_dot_200 = 0\n",
    "    up_dot_250 = 0\n",
    "    up_dot_300 = 0\n",
    "    up_dot_350 = 0\n",
    "    up_dot_400 = 0\n",
    "    up_dot_450 = 0\n",
    "    up_dot_500 = 0\n",
    "\n",
    "    # 위 변수들을 비율로 계산\n",
    "    percent_1 = 0\n",
    "    percent_2 = 0\n",
    "    percent_3 = 0\n",
    "    percent_4 = 0\n",
    "    percent_5 = 0\n",
    "    percent_6 = 0\n",
    "    percent_7 = 0\n",
    "    percent_8 = 0\n",
    "    percent_9 = 0\n",
    "    percent_10 = 0\n",
    "    percent_11 = 0\n",
    "\n",
    "    # 편차의 합, 평균, 등급 저장\n",
    "    hap = 0\n",
    "    avg = 0\n",
    "    grade = 0\n",
    "\n",
    "    # 편차 절댓값을 기준으로 갯수 저장\n",
    "    for j in range(len(delta_cct_pm[i])):\n",
    "        if(abs(delta_cct_pm[i][j]) >= 25):\n",
    "            up_dot_25 += 1\n",
    "        if(abs(delta_cct_pm[i][j]) >= 50):\n",
    "            up_dot_50 += 1\n",
    "        if(abs(delta_cct_pm[i][j]) >= 100):\n",
    "            up_dot_100 += 1\n",
    "        if(abs(delta_cct_pm[i][j]) >= 150):\n",
    "            up_dot_150 += 1\n",
    "        if(abs(delta_cct_pm[i][j]) >= 200):\n",
    "            up_dot_200 += 1\n",
    "        if(abs(delta_cct_pm[i][j]) >= 250):\n",
    "            up_dot_250 += 1\n",
    "        if(abs(delta_cct_pm[i][j]) >= 300):\n",
    "            up_dot_300 += 1\n",
    "        if(abs(delta_cct_pm[i][j]) >= 350):\n",
    "            up_dot_350 += 1\n",
    "        if(abs(delta_cct_pm[i][j]) >= 400):\n",
    "            up_dot_400 += 1\n",
    "        if(abs(delta_cct_pm[i][j]) >= 450):\n",
    "            up_dot_450 += 1\n",
    "        if(abs(delta_cct_pm[i][j]) >= 500):\n",
    "            up_dot_500 += 1\n",
    "        \n",
    "        # 편차 합 및 평균 저장\n",
    "        hap += abs(delta_cct_pm[i][j])\n",
    "        avg = hap / 359\n",
    "\n",
    "        # 편차 갯수들이 차지하는 비율 소수점 둘째짜리까지 계산\n",
    "        percent_1 = round(up_dot_25 / 360 * 100, 2)\n",
    "        percent_2 = round(up_dot_50 / 360 * 100, 2)\n",
    "        percent_3 = round(up_dot_100 / 360 * 100, 2)\n",
    "        percent_4 = round(up_dot_150 / 360 * 100, 2)\n",
    "        percent_5 = round(up_dot_200 / 360 * 100, 2)\n",
    "        percent_6 = round(up_dot_250 / 360 * 100, 2)\n",
    "        percent_7 = round(up_dot_300 / 360 * 100, 2)\n",
    "        percent_8 = round(up_dot_350 / 360 * 100, 2)\n",
    "        percent_9 = round(up_dot_400 / 360 * 100, 2)\n",
    "        percent_10 = round(up_dot_450 / 360 * 100, 2)\n",
    "        percent_11 = round(up_dot_500 / 360 * 100, 2)\n",
    "\n",
    "    # 등급 선정 기준\n",
    "    if(percent_1 > 30.0):\n",
    "        grade += 1\n",
    "    \n",
    "    if(percent_2 > 20.0):\n",
    "        grade += 1\n",
    "    \n",
    "    if(cct_hap_pm[i] < 1900000.0 or cct_hap_pm[i] > 2100000.0):\n",
    "        grade += 1\n",
    "        \n",
    "    if(hap > 15000.0):\n",
    "        grade += 1\n",
    "        \n",
    "        # 편차의 절대값이 0.1 ~ 5이상인 경우의 갯수 저장\n",
    "    up_dot_1 = 0\n",
    "    up_dot_2 = 0\n",
    "    up_dot_3 = 0\n",
    "    up_dot_4 = 0\n",
    "    up_dot_5 = 0\n",
    "    up_dot_6 = 0\n",
    "    up_dot_7 = 0\n",
    "    up_dot_8 = 0\n",
    "    up_dot_9 = 0\n",
    "    up_dot_10 = 0\n",
    "    up_dot_20 = 0\n",
    "    up_dot_30 = 0\n",
    "    up_dot_40 = 0\n",
    "    up_dot_50_2 = 0\n",
    "\n",
    "    # 위의 갯수가 하루 전체 데이터에서 차지하는 비율 저장\n",
    "    percent_1 = 0\n",
    "    percent_2 = 0\n",
    "    percent_3 = 0\n",
    "    percent_4 = 0\n",
    "    percent_5 = 0\n",
    "    percent_6 = 0\n",
    "    percent_7 = 0\n",
    "    percent_8 = 0\n",
    "    percent_9 = 0\n",
    "    percent_10 = 0\n",
    "    percent_20 = 0\n",
    "    percent_30 = 0\n",
    "    percent_40 = 0\n",
    "    percent_50_2 = 0\n",
    "\n",
    "    # 편차의 합, 평균, 등급 저장\n",
    "    hap_2 = 0\n",
    "    avg_2 = 0\n",
    "#     grade = 0\n",
    "\n",
    "    for j in range(len(delta_swr_pm[i])):\n",
    "        if(abs(delta_swr_pm[i][j]) >= 0.1):\n",
    "            up_dot_1 += 1\n",
    "        if(abs(delta_swr_pm[i][j]) >= 0.2):\n",
    "            up_dot_2 += 1\n",
    "        if(abs(delta_swr_pm[i][j]) >= 0.3):\n",
    "            up_dot_3 += 1\n",
    "        if(abs(delta_swr_pm[i][j]) >= 0.4):\n",
    "            up_dot_4 += 1\n",
    "        if(abs(delta_swr_pm[i][j]) >= 0.5):\n",
    "            up_dot_5 += 1\n",
    "        if(abs(delta_swr_pm[i][j]) >= 0.6):\n",
    "            up_dot_6 += 1\n",
    "        if(abs(delta_swr_pm[i][j]) >= 0.7):\n",
    "            up_dot_7 += 1\n",
    "        if(abs(delta_swr_pm[i][j]) >= 0.8):\n",
    "            up_dot_8 += 1\n",
    "        if(abs(delta_swr_pm[i][j]) >= 0.9):\n",
    "            up_dot_9 += 1\n",
    "        if(abs(delta_swr_pm[i][j]) >= 1):\n",
    "            up_dot_10 += 1\n",
    "        if(abs(delta_swr_pm[i][j] >= 2)):\n",
    "            up_dot_20 += 1\n",
    "        if(abs(delta_swr_pm[i][j] >= 3)):\n",
    "            up_dot_30 += 1\n",
    "        if(abs(delta_swr_pm[i][j] >= 4)):\n",
    "            up_dot_40 += 1\n",
    "        if(abs(delta_swr_pm[i][j] >= 5)):\n",
    "            up_dot_50_2 += 1\n",
    "\n",
    "        hap_2 += abs(delta_swr_pm[i][j])\n",
    "        avg_2 = hap / 359\n",
    "\n",
    "        percent_1 = round(up_dot_1 / 360 * 100, 2);\n",
    "        percent_2 = round(up_dot_2 / 360 * 100, 2);\n",
    "        percent_3 = round(up_dot_3 / 360 * 100, 2);\n",
    "        percent_4 = round(up_dot_4 / 360 * 100, 2);\n",
    "        percent_5 = round(up_dot_5 / 360 * 100, 2);\n",
    "        percent_6 = round(up_dot_6 / 360 * 100, 2);\n",
    "        percent_7 = round(up_dot_7 / 360 * 100, 2);\n",
    "        percent_8 = round(up_dot_8 / 360 * 100, 2);\n",
    "        percent_9 = round(up_dot_9 / 360 * 100, 2);\n",
    "        percent_10 = round(up_dot_10 / 360 * 100, 2);\n",
    "        percent_20 = round(up_dot_20 / 360 * 100, 2);\n",
    "        percent_30 = round(up_dot_30 / 360 * 100, 2);\n",
    "        percent_40 = round(up_dot_40 / 360 * 100, 2);\n",
    "        percent_50 = round(up_dot_50 / 360 * 100, 2);\n",
    "\n",
    "        # 등급 기준 1\n",
    "#     if(swr_hap[i] <= 15500.0 or swr_hap[i] >= 16500.0):\n",
    "#         grade += 1\n",
    "\n",
    "#     # 등급 기준 2\n",
    "#     # 편차의 절대값이 0.1 이상인 경우가 전체 데이터의 25% + 1, 30%를 넘을 경우 +1, 40%를 넘을 경우 +2, 50%를 넘길 경우 +3\n",
    "#     if(percent_1 >= 25.0):\n",
    "#         grade += 1\n",
    "\n",
    "#     if(percent_1 >= 40.0):\n",
    "#         grade += 1\n",
    "\n",
    "#     # 등급 기준 3\n",
    "#     # 편차의 절대값이 1 이상인 경우가 전체 데이터의 5%를 넘을 경우 +1, 10%를 넘을 경우 +2\n",
    "#     if(percent_10 >= 10.0):\n",
    "#         grade += 1\n",
    "        \n",
    "    wr.writerow([up_dot_25, up_dot_50, up_dot_100, up_dot_150, up_dot_200, up_dot_250, up_dot_300, up_dot_350, up_dot_400, up_dot_450, up_dot_500, up_dot_1, up_dot_2, up_dot_3, up_dot_4, up_dot_5, up_dot_6, up_dot_7, up_dot_8, up_dot_9, up_dot_10, up_dot_20, up_dot_30, up_dot_40, up_dot_50_2, grade])\n",
    "w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 색온도, 단파장 학습, 테스트 데이터 두 파일 합치기 (같은 상태의 minmaxScaler 적용을 위해)\n",
    "import_data = np.genfromtxt('../data/cct_swr_calculation_am.csv', delimiter=',', dtype='float')\n",
    "import_data2 = np.genfromtxt('../data/cct_swr_test_calculation_am.csv', delimiter=',', dtype='float')\n",
    "w = open('../data/cct_swr_classification_am_input.csv', 'w', encoding='utf-8')\n",
    "wr = csv.writer(w)\n",
    "\n",
    "for i in range (len(import_data)):\n",
    "    wr.writerow(import_data[i][:])\n",
    "for i in range(len(import_data2)):\n",
    "    wr.writerow(import_data2[i][:])\n",
    "w.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.6593384\n",
      "100 1.4683053\n",
      "200 1.4264661\n",
      "300 1.3956591\n",
      "400 1.3699558\n",
      "500 1.3478478\n",
      "-----------------------------\n",
      "train_data =  85 test_data =  37\n",
      "train accuracy =  45.88235294117647\n",
      "test accuracy =  43.24324324324324\n"
     ]
    }
   ],
   "source": [
    "# 텐서플로우 모델 생성 위한 import\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "# 초기값 선정 xavier 알고리즘\n",
    "def xavier_init(n_inputs, n_outputs, uniform=True):\n",
    "    \n",
    "    if uniform:\n",
    "        init_range = tf.sqrt(6.0 / (n_inputs + n_outputs))\n",
    "        return tf.random_uniform_initializer(-init_range, init_range)\n",
    "    else:\n",
    "        stddev = tf.sqrt(3.0 / (n_inputs + n_outputs))\n",
    "        return tf.truncated_normal_initializer(stddev=stddev)\n",
    "\n",
    "# 색온도 학습 및 테스트 (편차 분포)\n",
    "\n",
    "import_data = np.genfromtxt('../data/cct_swr_classification_am_input.csv', delimiter=',', dtype='float')\n",
    "\n",
    "x_data = import_data[:, :25]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_data = scaler.fit_transform(x_data)\n",
    "test_x = x_data[85:, :25]\n",
    "x_data = x_data[:85, :25]\n",
    "\n",
    "y_data = []\n",
    "test_y = []\n",
    "\n",
    "for i in range(85):\n",
    "    temp = []\n",
    "    temp.append(import_data[i][25])\n",
    "    y_data.append(temp)\n",
    "\n",
    "raw_y = y_data\n",
    "    \n",
    "for i in range(85, len(import_data)):\n",
    "    temp = []\n",
    "    temp.append(import_data[i][25])\n",
    "    test_y.append(temp)\n",
    "    \n",
    "\n",
    "y_data = ohe.fit_transform(y_data)\n",
    "y_data = y_data.toarray();\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 25])\n",
    "Y = tf.placeholder(tf.float32, [None, 5])\n",
    "\n",
    "nb_classes = 5\n",
    "\n",
    "#기존 코드\n",
    "# W = tf.Variable(tf.random_normal([25, nb_classes]), name='weight')\n",
    "# b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# Xavier Initializer 추가 코드\n",
    "W = tf.get_variable(\"W\", shape=[25, nb_classes], initializer=xavier_init(25, nb_classes))\n",
    "b = tf.Variable(tf.zeros([nb_classes]))\n",
    "\n",
    "H = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# W1 = tf.get_variable(\"W1\", shape=[25, 30], initializer=tf.contrib.layers.xavier_initializer())\n",
    "# b1 = tf.Variable(tf.zeros([30]))\n",
    "# H1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# W2 = tf.get_variable(\"W2\", shape=[30, 30], initializer=tf.contrib.layers.xavier_initializer())\n",
    "# b2 = tf.Variable(tf.zeros([30]))\n",
    "# H2 = tf.nn.relu(tf.matmul(H1, W2) + b2)\n",
    "\n",
    "# W3 = tf.get_variable(\"W3\", shape=[30, nb_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "# b3 = tf.Variable(tf.zeros([nb_classes]))\n",
    "# H3 = tf.matmul(H2, W3) + b3\n",
    "\n",
    "# H3 = tf.nn.softmax(H3)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(H), axis=1))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(501):\n",
    "        sess.run(optimizer, feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}))\n",
    "\n",
    "    print('-----------------------------')\n",
    "    print('train_data = ', len(x_data), 'test_data = ', len(test_x))\n",
    "    \n",
    "    for i in range(len(x_data)):\n",
    "        a = sess.run(H, feed_dict={X:[x_data[i]]})\n",
    "        if(sess.run(tf.argmax(a, 1)) == (raw_y[i][0])):\n",
    "            train_accuracy += 1\n",
    "    print(\"train accuracy = \", float(train_accuracy / len(x_data) * 100))\n",
    "\n",
    "    for i in range(len(test_x)):\n",
    "        a = sess.run(H, feed_dict={X:[test_x[i]]})\n",
    "        if(sess.run(tf.argmax(a, 1)) == (test_y[i][0])):\n",
    "            test_accuracy += 1\n",
    "    print(\"test accuracy = \", float(test_accuracy / len(test_x) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20000\n",
    "77\n",
    "35\n",
    "\n",
    "15000\n",
    "69\n",
    "32\n",
    "\n",
    "8000\n",
    "56\n",
    "35\n",
    "\n",
    "1000\n",
    "47\n",
    "43\n",
    "\n",
    "500\n",
    "45\n",
    "43"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
