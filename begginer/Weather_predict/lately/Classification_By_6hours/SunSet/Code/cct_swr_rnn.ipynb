{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-19 5:52:00 5:52:00\n",
      "2018-04-19 19:52:00 19:52:00\n",
      "2018-04-19 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-04-21 5:49:00 5:49:00\n",
      "2018-04-21 19:49:00 19:49:00\n",
      "2018-04-21 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-04-26 5:43:00 5:43:00\n",
      "2018-04-26 19:43:00 19:43:00\n",
      "2018-04-26 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-04-27 5:42:00 5:42:00\n",
      "2018-04-27 19:42:00 19:42:00\n",
      "2018-04-27 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-04-28 5:41:00 5:41:00\n",
      "2018-04-28 19:41:00 19:41:00\n",
      "2018-04-28 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-04-29 5:40:00 5:40:00\n",
      "2018-04-29 19:40:00 19:40:00\n",
      "2018-04-29 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-04-30 5:38:00 5:38:00\n",
      "2018-04-30 19:38:00 19:38:00\n",
      "2018-04-30 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-05-04 5:34:00 5:34:00\n",
      "2018-05-04 19:34:00 19:34:00\n",
      "2018-05-04 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-05-05 5:33:00 5:33:00\n",
      "2018-05-05 19:33:00 19:33:00\n",
      "2018-05-05 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-05-08 5:30:00 5:30:00\n",
      "2018-05-08 19:30:00 19:30:00\n",
      "2018-05-08 828 data saved.\n",
      "----------------------------------------------\n",
      "2018-05-10 5:28:00 5:28:00\n",
      "2018-05-10 19:28:00 19:28:00\n",
      "2018-05-10 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-05-14 5:24:00 5:24:00\n",
      "2018-05-14 19:24:00 19:24:00\n",
      "2018-05-14 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-05-19 5:20:00 5:20:00\n",
      "2018-05-19 19:20:00 19:20:00\n",
      "2018-05-19 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-05-21 5:19:00 5:19:00\n",
      "2018-05-21 19:19:00 19:19:00\n",
      "2018-05-21 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-05-24 5:17:00 5:17:00\n",
      "2018-05-24 19:17:00 19:17:00\n",
      "2018-05-24 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-05-25 5:17:00 5:17:00\n",
      "2018-05-25 19:17:00 19:17:00\n",
      "2018-05-25 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-05-26 5:16:00 5:16:00\n",
      "2018-05-26 19:16:00 19:16:00\n",
      "2018-05-26 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-05-27 5:16:00 5:16:00\n",
      "2018-05-27 19:16:00 19:16:00\n",
      "2018-05-27 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-05-28 5:15:00 5:15:00\n",
      "2018-05-28 19:15:00 19:15:00\n",
      "2018-05-28 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-05-29 5:15:00 5:15:00\n",
      "2018-05-29 19:15:00 19:15:00\n",
      "2018-05-29 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-05-30 5:14:00 5:14:00\n",
      "2018-05-30 19:14:00 19:14:00\n",
      "2018-05-30 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-05-31 5:14:00 5:14:00\n",
      "2018-05-31 19:14:00 19:14:00\n",
      "2018-05-31 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-06-01 5:13:00 5:13:00\n",
      "2018-06-01 19:13:00 19:13:00\n",
      "2018-06-01 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-06-02 5:13:00 5:13:00\n",
      "2018-06-02 19:13:00 19:13:00\n",
      "2018-06-02 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-06-03 5:13:00 5:13:00\n",
      "2018-06-03 19:13:00 19:13:00\n",
      "2018-06-03 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-06-05 5:12:00 5:12:00\n",
      "2018-06-05 19:12:00 19:12:00\n",
      "2018-06-05 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-06-06 5:12:00 5:12:00\n",
      "2018-06-06 19:12:00 19:12:00\n",
      "2018-06-06 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-06-07 5:12:00 5:12:00\n",
      "2018-06-07 19:12:00 19:12:00\n",
      "2018-06-07 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-06-08 5:12:00 5:12:00\n",
      "2018-06-08 19:12:00 19:12:00\n",
      "2018-06-08 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-06-09 5:11:00 5:11:00\n",
      "2018-06-09 19:11:00 19:11:00\n",
      "2018-06-09 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-06-11 5:11:00 5:11:00\n",
      "2018-06-11 19:11:00 19:11:00\n",
      "2018-06-11 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-06-13 5:11:00 5:11:00\n",
      "2018-06-13 19:11:00 19:11:00\n",
      "2018-06-13 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-06-16 5:11:00 5:11:00\n",
      "2018-06-16 19:11:00 19:11:00\n",
      "2018-06-16 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-06-17 5:11:00 5:11:00\n",
      "2018-06-17 19:11:00 19:11:00\n",
      "2018-06-17 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-06-18 5:11:00 5:11:00\n",
      "2018-06-18 19:11:00 19:11:00\n",
      "2018-06-18 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-06-24 5:13:00 5:13:00\n",
      "2018-06-24 19:13:00 19:13:00\n",
      "2018-06-24 832 data saved.\n",
      "----------------------------------------------\n",
      "2018-06-25 5:13:00 5:13:00\n",
      "2018-06-25 19:13:00 19:13:00\n",
      "2018-06-25 828 data saved.\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 분별 날짜 파일에 있는 날짜 기준으로 DB에서 데이터 받아와서 저장하기\n",
    "\n",
    "import MySQLdb\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "# MySQL DB 연결\n",
    "db = MySQLdb.connect('210.102.142.13',\"root\", \"witlab8*\", \"cas_db\")\n",
    "c = db.cursor()\n",
    "\n",
    "# 분별 날짜\n",
    "# date = np.genfromtxt('../data/date_sunrise_sunset.csv', delimiter=',', dtype='str')\n",
    "date = np.genfromtxt('../data/test_date_sunrise_sunset.csv', delimiter=',', dtype='str')\n",
    "\n",
    "# csv 파일로 내보내기\n",
    "# w = open('../data/db_connect_data_rnn.csv', 'w', encoding='utf-8')\n",
    "w = open('../data/db_connect_test_data_rnn.csv', 'w', encoding='utf-8')\n",
    "wr = csv.writer(w)\n",
    "\n",
    "data_length = []\n",
    "\n",
    "# 각 날짜별 데이터들 DB에서 가져오고 csv 파일로 저장\n",
    "# 데이터는 cct, swr, uvb, uvi 순\n",
    "for j in range(len(date)):\n",
    "    sql = \"select time(date), date(date), cct, cas_swr from natural_tracker left outer join cas_wave_ratio using(date) where date(date) = '\"+ str(date[j][0]) + \"' order by time(date)\"\n",
    "    c.execute(sql)\n",
    "    rows = c.fetchall()\n",
    "    \n",
    "    # 일출 후 6시간과 일몰 후 6시간 데이터 사용, 일출 혹은 일몰 당시 데이터가 없을 경우 가장 가까운 다른 데이터로 변환\n",
    "    sunrise = datetime.datetime.strptime(date[j][1], '%H:%M:%S')\n",
    "    sunset = datetime.datetime.strptime(date[j][2], '%H:%M:%S')\n",
    "    \n",
    "    standard = datetime.datetime.strptime('1900-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    sunrise = sunrise - standard\n",
    "    sunset = sunset - standard\n",
    "    \n",
    "    start = 0\n",
    "    end = 0\n",
    "\n",
    "    for i in range(len(rows)):\n",
    "        if(rows[i][0] >= sunrise):\n",
    "            start = i\n",
    "            print(date[j][0], rows[start][0], sunrise)\n",
    "            break;\n",
    "    \n",
    "    for i in range(len(rows)):\n",
    "        if(rows[i][0] >= sunset):\n",
    "            end = i\n",
    "            print(date[j][0], rows[end][0], sunset)\n",
    "            break;\n",
    "    \n",
    "    last = int((end - start) / 4)\n",
    "    last = last * 4\n",
    "    \n",
    "#     # start에 저장된 index부터 772개 데이터 가져와서 저장\n",
    "    for l in range(start, start + last):\n",
    "        wr.writerow([rows[l][2], rows[l][3]])\n",
    "    print(date[j][0] + \" \" + str(last) + \" data saved.\")\n",
    "    for l in range(4):\n",
    "        data_length.append(int(last/4))\n",
    "    print('----------------------------------------------')\n",
    "        \n",
    "w.close()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148,)\n"
     ]
    }
   ],
   "source": [
    "# date = np.genfromtxt('../data/date_sunrise_sunset.csv', delimiter=',', dtype='str')\n",
    "date = np.genfromtxt('../data/test_date_sunrise_sunset.csv', delimiter=',', dtype='str')\n",
    "\n",
    "# 데이터 받아오고 편차 계산히기\n",
    "# import_data = np.loadtxt('../data/db_connect_data_rnn.csv', delimiter=',')\n",
    "import_data = np.loadtxt('../data/db_connect_test_data_rnn.csv', delimiter=',')\n",
    "\n",
    "one = import_data[:,0] # cct\n",
    "two = import_data[:,1] # cas_swr\n",
    "\n",
    "data_index = 0\n",
    "\n",
    "cct = []\n",
    "swr = []\n",
    "\n",
    "delta_cct = []\n",
    "delta_swr = []\n",
    "\n",
    "cct_hap = []\n",
    "swr_hap = []\n",
    "\n",
    "for i in range(len(data_length)):\n",
    "    temp = []\n",
    "    temp2 = []\n",
    "    for j in range(data_length[i]):\n",
    "        temp.append(one[j + data_index])\n",
    "        temp2.append(two[j + data_index])\n",
    "    cct.append(temp)\n",
    "    swr.append(temp2)\n",
    "    data_index += data_length[i]\n",
    "\n",
    "for i in range(len(data_length)):\n",
    "    temp = []\n",
    "    temp2 = []\n",
    "    for j in range(data_length[i] - 1):\n",
    "        temp.append(cct[i][j+1] - cct[i][j])\n",
    "        temp2.append(swr[i][j+1] - swr[i][j])\n",
    "    delta_cct.append(temp)\n",
    "    delta_swr.append(temp2)\n",
    "    \n",
    "for i in range(len(data_length)):\n",
    "    temp1 = 0\n",
    "    temp2 = 0\n",
    "    for j in range(data_length[i]):\n",
    "        temp1 += cct[i][j]\n",
    "        temp2 += swr[i][j]\n",
    "    cct_hap.append(temp1)\n",
    "    swr_hap.append(temp2)\n",
    "\n",
    "print(np.shape(cct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = open('../data/cct_swr_calculation_rnn.csv', 'w', encoding='utf-8')\n",
    "w = open('../data/cct_swr_calculation_test_rnn.csv', 'w', encoding='utf-8')\n",
    "\n",
    "wr = csv.writer(w)\n",
    "\n",
    "for i in range(len(delta_cct)):\n",
    "    up_dot_25 = 0\n",
    "    up_dot_50 = 0\n",
    "    up_dot_100 = 0\n",
    "    up_dot_150 = 0\n",
    "    up_dot_200 = 0\n",
    "    up_dot_250 = 0\n",
    "    up_dot_300 = 0\n",
    "    up_dot_350 = 0\n",
    "    up_dot_400 = 0\n",
    "    up_dot_450 = 0\n",
    "    up_dot_500 = 0\n",
    "\n",
    "    # 위 변수들을 비율로 계산\n",
    "    percent_1 = 0\n",
    "    percent_2 = 0\n",
    "    percent_3 = 0\n",
    "    percent_4 = 0\n",
    "    percent_5 = 0\n",
    "    percent_6 = 0\n",
    "    percent_7 = 0\n",
    "    percent_8 = 0\n",
    "    percent_9 = 0\n",
    "    percent_10 = 0\n",
    "    percent_11 = 0\n",
    "\n",
    "    # 편차의 합, 평균, 등급 저장\n",
    "    hap = 0\n",
    "    avg = 0\n",
    "    grade = 0\n",
    "\n",
    "    # 편차 절댓값을 기준으로 갯수 저장\n",
    "    for j in range(len(delta_cct[i])):\n",
    "        if(abs(delta_cct[i][j]) >= 25):\n",
    "            up_dot_25 += 1\n",
    "        if(abs(delta_cct[i][j]) >= 50):\n",
    "            up_dot_50 += 1\n",
    "        if(abs(delta_cct[i][j]) >= 100):\n",
    "            up_dot_100 += 1\n",
    "        if(abs(delta_cct[i][j]) >= 150):\n",
    "            up_dot_150 += 1\n",
    "        if(abs(delta_cct[i][j]) >= 200):\n",
    "            up_dot_200 += 1\n",
    "        if(abs(delta_cct[i][j]) >= 250):\n",
    "            up_dot_250 += 1\n",
    "        if(abs(delta_cct[i][j]) >= 300):\n",
    "            up_dot_300 += 1\n",
    "        if(abs(delta_cct[i][j]) >= 350):\n",
    "            up_dot_350 += 1\n",
    "        if(abs(delta_cct[i][j]) >= 400):\n",
    "            up_dot_400 += 1\n",
    "        if(abs(delta_cct[i][j]) >= 450):\n",
    "            up_dot_450 += 1\n",
    "        if(abs(delta_cct[i][j]) >= 500):\n",
    "            up_dot_500 += 1\n",
    "        \n",
    "        # 편차 합 및 평균 저장\n",
    "        hap += abs(delta_cct[i][j])\n",
    "        avg = hap / len(delta_cct[i])\n",
    "\n",
    "        # 편차 갯수들이 차지하는 비율 소수점 둘째짜리까지 계산\n",
    "        percent_1 = round(up_dot_25 / len(cct[i]) * 100, 2)\n",
    "        percent_2 = round(up_dot_50 / len(cct[i]) * 100, 2)\n",
    "        percent_3 = round(up_dot_100 / len(cct[i]) * 100, 2)\n",
    "        percent_4 = round(up_dot_150 / len(cct[i]) * 100, 2)\n",
    "        percent_5 = round(up_dot_200 / len(cct[i]) * 100, 2)\n",
    "        percent_6 = round(up_dot_250 / len(cct[i]) * 100, 2)\n",
    "        percent_7 = round(up_dot_300 / len(cct[i]) * 100, 2)\n",
    "        percent_8 = round(up_dot_350 / len(cct[i]) * 100, 2)\n",
    "        percent_9 = round(up_dot_400 / len(cct[i]) * 100, 2)\n",
    "        percent_10 = round(up_dot_450 / len(cct[i]) * 100, 2)\n",
    "        percent_11 = round(up_dot_500 / len(cct[i]) * 100, 2)\n",
    "\n",
    "    # 등급 선정 기준\n",
    "    if(percent_1 > 30.0):\n",
    "        grade += 1\n",
    "    \n",
    "    if(percent_2 > 20.0):\n",
    "        grade += 1\n",
    "    \n",
    "    if(cct_hap[i] < 950000.0 or cct_hap[i] > 1050000.0):\n",
    "        grade += 1\n",
    "        \n",
    "    if(hap > 15000.0):\n",
    "        grade += 1\n",
    "        \n",
    "        # 편차의 절대값이 0.1 ~ 5이상인 경우의 갯수 저장\n",
    "    up_dot_1 = 0\n",
    "    up_dot_2 = 0\n",
    "    up_dot_3 = 0\n",
    "    up_dot_4 = 0\n",
    "    up_dot_5 = 0\n",
    "    up_dot_6 = 0\n",
    "    up_dot_7 = 0\n",
    "    up_dot_8 = 0\n",
    "    up_dot_9 = 0\n",
    "    up_dot_10 = 0\n",
    "    up_dot_20 = 0\n",
    "    up_dot_30 = 0\n",
    "    up_dot_40 = 0\n",
    "    up_dot_50_2 = 0\n",
    "\n",
    "    # 위의 갯수가 하루 전체 데이터에서 차지하는 비율 저장\n",
    "    percent_1 = 0\n",
    "    percent_2 = 0\n",
    "    percent_3 = 0\n",
    "    percent_4 = 0\n",
    "    percent_5 = 0\n",
    "    percent_6 = 0\n",
    "    percent_7 = 0\n",
    "    percent_8 = 0\n",
    "    percent_9 = 0\n",
    "    percent_10 = 0\n",
    "    percent_20 = 0\n",
    "    percent_30 = 0\n",
    "    percent_40 = 0\n",
    "    percent_50_2 = 0\n",
    "\n",
    "    # 편차의 합, 평균, 등급 저장\n",
    "    hap_2 = 0\n",
    "    avg_2 = 0\n",
    "#     grade = 0\n",
    "\n",
    "    for j in range(len(delta_swr[i])):\n",
    "        if(abs(delta_swr[i][j]) >= 0.1):\n",
    "            up_dot_1 += 1\n",
    "        if(abs(delta_swr[i][j]) >= 0.2):\n",
    "            up_dot_2 += 1\n",
    "        if(abs(delta_swr[i][j]) >= 0.3):\n",
    "            up_dot_3 += 1\n",
    "        if(abs(delta_swr[i][j]) >= 0.4):\n",
    "            up_dot_4 += 1\n",
    "        if(abs(delta_swr[i][j]) >= 0.5):\n",
    "            up_dot_5 += 1\n",
    "        if(abs(delta_swr[i][j]) >= 0.6):\n",
    "            up_dot_6 += 1\n",
    "        if(abs(delta_swr[i][j]) >= 0.7):\n",
    "            up_dot_7 += 1\n",
    "        if(abs(delta_swr[i][j]) >= 0.8):\n",
    "            up_dot_8 += 1\n",
    "        if(abs(delta_swr[i][j]) >= 0.9):\n",
    "            up_dot_9 += 1\n",
    "        if(abs(delta_swr[i][j]) >= 1):\n",
    "            up_dot_10 += 1\n",
    "        if(abs(delta_swr[i][j] >= 2)):\n",
    "            up_dot_20 += 1\n",
    "        if(abs(delta_swr[i][j] >= 3)):\n",
    "            up_dot_30 += 1\n",
    "        if(abs(delta_swr[i][j] >= 4)):\n",
    "            up_dot_40 += 1\n",
    "        if(abs(delta_swr[i][j] >= 5)):\n",
    "            up_dot_50_2 += 1\n",
    "\n",
    "        hap_2 += abs(delta_swr[i][j])\n",
    "        avg_2 = hap / len(delta_swr[i])\n",
    "\n",
    "        percent_1 = round(up_dot_1 / len(swr[i]) * 100, 2);\n",
    "        percent_2 = round(up_dot_2 / len(swr[i]) * 100, 2);\n",
    "        percent_3 = round(up_dot_3 / len(swr[i]) * 100, 2);\n",
    "        percent_4 = round(up_dot_4 / len(swr[i]) * 100, 2);\n",
    "        percent_5 = round(up_dot_5 / len(swr[i]) * 100, 2);\n",
    "        percent_6 = round(up_dot_6 / len(swr[i]) * 100, 2);\n",
    "        percent_7 = round(up_dot_7 / len(swr[i]) * 100, 2);\n",
    "        percent_8 = round(up_dot_8 / len(swr[i]) * 100, 2);\n",
    "        percent_9 = round(up_dot_9 / len(swr[i]) * 100, 2);\n",
    "        percent_10 = round(up_dot_10 / len(swr[i]) * 100, 2);\n",
    "        percent_20 = round(up_dot_20 / len(swr[i]) * 100, 2);\n",
    "        percent_30 = round(up_dot_30 / len(swr[i]) * 100, 2);\n",
    "        percent_40 = round(up_dot_40 / len(swr[i]) * 100, 2);\n",
    "        percent_50 = round(up_dot_50 / len(swr[i]) * 100, 2);\n",
    "\n",
    "        # 등급 기준 1\n",
    "#     if(swr_hap[i] <= 15500.0 or swr_hap[i] >= 16500.0):\n",
    "#         grade += 1\n",
    "\n",
    "#     # 등급 기준 2\n",
    "#     # 편차의 절대값이 0.1 이상인 경우가 전체 데이터의 25% + 1, 30%를 넘을 경우 +1, 40%를 넘을 경우 +2, 50%를 넘길 경우 +3\n",
    "#     if(percent_1 >= 25.0):\n",
    "#         grade += 1\n",
    "\n",
    "#     if(percent_1 >= 40.0):\n",
    "#         grade += 1\n",
    "\n",
    "#     # 등급 기준 3\n",
    "#     # 편차의 절대값이 1 이상인 경우가 전체 데이터의 5%를 넘을 경우 +1, 10%를 넘을 경우 +2\n",
    "#     if(percent_10 >= 10.0):\n",
    "#         grade += 1\n",
    "        \n",
    "    wr.writerow([up_dot_25, up_dot_50, up_dot_100, up_dot_150, up_dot_200, up_dot_250, up_dot_300, up_dot_350, up_dot_400, up_dot_450, up_dot_500, up_dot_1, up_dot_2, up_dot_3, up_dot_4, up_dot_5, up_dot_6, up_dot_7, up_dot_8, up_dot_9, up_dot_10, up_dot_20, up_dot_30, up_dot_40, up_dot_50_2, grade])\n",
    "w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] cost: 1.8057533502578735\n",
      "학습 정확도: 0.16780046\n",
      "테스트 정확도: 0.16842106\n",
      "[step: 1] cost: 1.740919828414917\n",
      "[step: 2] cost: 1.6917649507522583\n",
      "[step: 3] cost: 1.656561017036438\n",
      "[step: 4] cost: 1.633130669593811\n",
      "[step: 5] cost: 1.6187456846237183\n",
      "[step: 6] cost: 1.6101666688919067\n",
      "[step: 7] cost: 1.6043492555618286\n",
      "[step: 8] cost: 1.5993471145629883\n",
      "[step: 9] cost: 1.5944291353225708\n",
      "[step: 10] cost: 1.5895594358444214\n",
      "학습 정확도: 0.34920636\n",
      "테스트 정확도: 0.3105263\n",
      "[step: 11] cost: 1.5848288536071777\n",
      "[step: 12] cost: 1.5801514387130737\n",
      "[step: 13] cost: 1.575265884399414\n",
      "[step: 14] cost: 1.5699036121368408\n",
      "[step: 15] cost: 1.5639374256134033\n",
      "[step: 16] cost: 1.5574254989624023\n",
      "[step: 17] cost: 1.5505715608596802\n",
      "[step: 18] cost: 1.543650507926941\n",
      "[step: 19] cost: 1.5369367599487305\n",
      "[step: 20] cost: 1.530651569366455\n",
      "학습 정확도: 0.35827664\n",
      "테스트 정확도: 0.32105264\n",
      "[step: 21] cost: 1.5249301195144653\n",
      "[step: 22] cost: 1.5198203325271606\n",
      "[step: 23] cost: 1.5152982473373413\n",
      "[step: 24] cost: 1.5112965106964111\n",
      "[step: 25] cost: 1.5077323913574219\n",
      "[step: 26] cost: 1.5045199394226074\n",
      "[step: 27] cost: 1.5015687942504883\n",
      "[step: 28] cost: 1.4987815618515015\n",
      "[step: 29] cost: 1.4960566759109497\n",
      "[step: 30] cost: 1.493302345275879\n",
      "학습 정확도: 0.37641722\n",
      "테스트 정확도: 0.3263158\n",
      "[step: 31] cost: 1.4904547929763794\n",
      "[step: 32] cost: 1.4874926805496216\n",
      "[step: 33] cost: 1.484440565109253\n",
      "[step: 34] cost: 1.4813636541366577\n",
      "[step: 35] cost: 1.4783505201339722\n",
      "[step: 36] cost: 1.4754910469055176\n",
      "[step: 37] cost: 1.472858190536499\n",
      "[step: 38] cost: 1.4704898595809937\n",
      "[step: 39] cost: 1.4683837890625\n",
      "[step: 40] cost: 1.466504693031311\n",
      "학습 정확도: 0.38548753\n",
      "테스트 정확도: 0.3263158\n",
      "[step: 41] cost: 1.464796781539917\n",
      "[step: 42] cost: 1.4632023572921753\n",
      "[step: 43] cost: 1.4616730213165283\n",
      "[step: 44] cost: 1.460175633430481\n",
      "[step: 45] cost: 1.4586929082870483\n",
      "[step: 46] cost: 1.4572227001190186\n",
      "[step: 47] cost: 1.455778956413269\n",
      "[step: 48] cost: 1.4543867111206055\n",
      "[step: 49] cost: 1.4530737400054932\n",
      "[step: 50] cost: 1.451857089996338\n",
      "학습 정확도: 0.39455783\n",
      "테스트 정확도: 0.33157894\n",
      "[step: 51] cost: 1.4507315158843994\n",
      "[step: 52] cost: 1.4496692419052124\n",
      "[step: 53] cost: 1.4486314058303833\n",
      "[step: 54] cost: 1.4475871324539185\n",
      "[step: 55] cost: 1.4465222358703613\n",
      "[step: 56] cost: 1.4454407691955566\n",
      "[step: 57] cost: 1.44435453414917\n",
      "[step: 58] cost: 1.4432761669158936\n",
      "[step: 59] cost: 1.442214846611023\n",
      "[step: 60] cost: 1.4411766529083252\n",
      "학습 정확도: 0.40136054\n",
      "테스트 정확도: 0.34210527\n",
      "[step: 61] cost: 1.4401642084121704\n",
      "[step: 62] cost: 1.4391753673553467\n",
      "[step: 63] cost: 1.4381996393203735\n",
      "[step: 64] cost: 1.437224268913269\n",
      "[step: 65] cost: 1.4362399578094482\n",
      "[step: 66] cost: 1.4352450370788574\n",
      "[step: 67] cost: 1.4342447519302368\n",
      "[step: 68] cost: 1.4332464933395386\n",
      "[step: 69] cost: 1.4322534799575806\n",
      "[step: 70] cost: 1.4312667846679688\n",
      "학습 정확도: 0.4036281\n",
      "테스트 정확도: 0.34210527\n",
      "[step: 71] cost: 1.430284857749939\n",
      "[step: 72] cost: 1.4293051958084106\n",
      "[step: 73] cost: 1.4283238649368286\n",
      "[step: 74] cost: 1.4273353815078735\n",
      "[step: 75] cost: 1.4263352155685425\n",
      "[step: 76] cost: 1.425323247909546\n",
      "[step: 77] cost: 1.4243015050888062\n",
      "[step: 78] cost: 1.4232743978500366\n",
      "[step: 79] cost: 1.4222415685653687\n",
      "[step: 80] cost: 1.4212027788162231\n",
      "학습 정확도: 0.41269842\n",
      "테스트 정확도: 0.34210527\n",
      "[step: 81] cost: 1.4201548099517822\n",
      "[step: 82] cost: 1.4190951585769653\n",
      "[step: 83] cost: 1.4180201292037964\n",
      "[step: 84] cost: 1.416927695274353\n",
      "[step: 85] cost: 1.4158169031143188\n",
      "[step: 86] cost: 1.414689064025879\n",
      "[step: 87] cost: 1.4135452508926392\n",
      "[step: 88] cost: 1.4123854637145996\n",
      "[step: 89] cost: 1.4112075567245483\n",
      "[step: 90] cost: 1.4100085496902466\n",
      "학습 정확도: 0.41723356\n",
      "테스트 정확도: 0.33157894\n",
      "[step: 91] cost: 1.4087867736816406\n",
      "[step: 92] cost: 1.4075409173965454\n",
      "[step: 93] cost: 1.4062705039978027\n",
      "[step: 94] cost: 1.4049758911132812\n",
      "[step: 95] cost: 1.4036574363708496\n",
      "[step: 96] cost: 1.40231454372406\n",
      "[step: 97] cost: 1.400946855545044\n",
      "[step: 98] cost: 1.399552583694458\n",
      "[step: 99] cost: 1.3981317281723022\n",
      "[step: 100] cost: 1.3966845273971558\n",
      "학습 정확도: 0.41950113\n",
      "테스트 정확도: 0.33684212\n",
      "[step: 101] cost: 1.39521062374115\n",
      "[step: 102] cost: 1.393710970878601\n",
      "[step: 103] cost: 1.3921858072280884\n",
      "[step: 104] cost: 1.3906352519989014\n",
      "[step: 105] cost: 1.389059066772461\n",
      "[step: 106] cost: 1.3874568939208984\n",
      "[step: 107] cost: 1.3858288526535034\n",
      "[step: 108] cost: 1.3841749429702759\n",
      "[step: 109] cost: 1.382495641708374\n",
      "[step: 110] cost: 1.3807915449142456\n",
      "학습 정확도: 0.4240363\n",
      "테스트 정확도: 0.3263158\n",
      "[step: 111] cost: 1.3790619373321533\n",
      "[step: 112] cost: 1.3773070573806763\n",
      "[step: 113] cost: 1.3755266666412354\n",
      "[step: 114] cost: 1.3737214803695679\n",
      "[step: 115] cost: 1.3718922138214111\n",
      "[step: 116] cost: 1.3700395822525024\n",
      "[step: 117] cost: 1.3681633472442627\n",
      "[step: 118] cost: 1.3662642240524292\n",
      "[step: 119] cost: 1.364342212677002\n",
      "[step: 120] cost: 1.3623981475830078\n",
      "학습 정확도: 0.43537414\n",
      "테스트 정확도: 0.33684212\n",
      "[step: 121] cost: 1.360432505607605\n",
      "[step: 122] cost: 1.358445405960083\n",
      "[step: 123] cost: 1.3564367294311523\n",
      "[step: 124] cost: 1.354406714439392\n",
      "[step: 125] cost: 1.3523553609848022\n",
      "[step: 126] cost: 1.3502833843231201\n",
      "[step: 127] cost: 1.3481903076171875\n",
      "[step: 128] cost: 1.3460761308670044\n",
      "[step: 129] cost: 1.3439401388168335\n",
      "[step: 130] cost: 1.3417829275131226\n",
      "학습 정확도: 0.44217688\n",
      "테스트 정확도: 0.34210527\n",
      "[step: 131] cost: 1.3396035432815552\n",
      "[step: 132] cost: 1.3374022245407104\n",
      "[step: 133] cost: 1.3351781368255615\n",
      "[step: 134] cost: 1.3329308032989502\n",
      "[step: 135] cost: 1.330660104751587\n",
      "[step: 136] cost: 1.3283650875091553\n",
      "[step: 137] cost: 1.3260456323623657\n",
      "[step: 138] cost: 1.323701024055481\n",
      "[step: 139] cost: 1.3213305473327637\n",
      "[step: 140] cost: 1.3189339637756348\n",
      "학습 정확도: 0.45578232\n",
      "테스트 정확도: 0.33684212\n",
      "[step: 141] cost: 1.3165113925933838\n",
      "[step: 142] cost: 1.3140614032745361\n",
      "[step: 143] cost: 1.3115848302841187\n",
      "[step: 144] cost: 1.3090810775756836\n",
      "[step: 145] cost: 1.3065502643585205\n",
      "[step: 146] cost: 1.3039922714233398\n",
      "[step: 147] cost: 1.3014073371887207\n",
      "[step: 148] cost: 1.2987961769104004\n",
      "[step: 149] cost: 1.2961586713790894\n",
      "[step: 150] cost: 1.2934958934783936\n",
      "학습 정확도: 0.4739229\n",
      "테스트 정확도: 0.30526316\n",
      "[step: 151] cost: 1.2908084392547607\n",
      "[step: 152] cost: 1.288097620010376\n",
      "[step: 153] cost: 1.2853648662567139\n",
      "[step: 154] cost: 1.2826123237609863\n",
      "[step: 155] cost: 1.2798420190811157\n",
      "[step: 156] cost: 1.277057409286499\n",
      "[step: 157] cost: 1.2742654085159302\n",
      "[step: 158] cost: 1.2715040445327759\n",
      "[step: 159] cost: 1.2690057754516602\n",
      "[step: 160] cost: 1.2680623531341553\n",
      "학습 정확도: 0.48526078\n",
      "테스트 정확도: 0.2736842\n",
      "[step: 161] cost: 1.2679047584533691\n",
      "[step: 162] cost: 1.26328706741333\n",
      "[step: 163] cost: 1.2604917287826538\n",
      "[step: 164] cost: 1.2592885494232178\n",
      "[step: 165] cost: 1.2561898231506348\n",
      "[step: 166] cost: 1.2539939880371094\n",
      "[step: 167] cost: 1.2519761323928833\n",
      "[step: 168] cost: 1.2494103908538818\n",
      "[step: 169] cost: 1.2474628686904907\n",
      "[step: 170] cost: 1.2455638647079468\n",
      "학습 정확도: 0.4920635\n",
      "테스트 정확도: 0.25789472\n",
      "[step: 171] cost: 1.2430229187011719\n",
      "[step: 172] cost: 1.2418549060821533\n",
      "[step: 173] cost: 1.2387816905975342\n",
      "[step: 174] cost: 1.2379150390625\n",
      "[step: 175] cost: 1.235086441040039\n",
      "[step: 176] cost: 1.2337474822998047\n",
      "[step: 177] cost: 1.2315232753753662\n",
      "[step: 178] cost: 1.2297656536102295\n",
      "[step: 179] cost: 1.2276043891906738\n",
      "[step: 180] cost: 1.226141333580017\n",
      "학습 정확도: 0.4988662\n",
      "테스트 정확도: 0.25789472\n",
      "[step: 181] cost: 1.2236741781234741\n",
      "[step: 182] cost: 1.2223432064056396\n",
      "[step: 183] cost: 1.2200425863265991\n",
      "[step: 184] cost: 1.2184146642684937\n",
      "[step: 185] cost: 1.216270923614502\n",
      "[step: 186] cost: 1.2147446870803833\n",
      "[step: 187] cost: 1.212450623512268\n",
      "[step: 188] cost: 1.2108911275863647\n",
      "[step: 189] cost: 1.2088254690170288\n",
      "[step: 190] cost: 1.2070937156677246\n",
      "학습 정확도: 0.5147392\n",
      "테스트 정확도: 0.24210526\n",
      "[step: 191] cost: 1.2049885988235474\n",
      "[step: 192] cost: 1.2032434940338135\n",
      "[step: 193] cost: 1.2013274431228638\n",
      "[step: 194] cost: 1.1993898153305054\n",
      "[step: 195] cost: 1.1974740028381348\n",
      "[step: 196] cost: 1.1954721212387085\n",
      "[step: 197] cost: 1.1936819553375244\n",
      "[step: 198] cost: 1.1916606426239014\n",
      "[step: 199] cost: 1.1898088455200195\n",
      "[step: 200] cost: 1.1877542734146118\n",
      "학습 정확도: 0.52380955\n",
      "테스트 정확도: 0.23157895\n",
      "[step: 201] cost: 1.1858084201812744\n",
      "[step: 202] cost: 1.1838181018829346\n",
      "[step: 203] cost: 1.1818010807037354\n",
      "[step: 204] cost: 1.1798652410507202\n",
      "[step: 205] cost: 1.177817940711975\n",
      "[step: 206] cost: 1.1758784055709839\n",
      "[step: 207] cost: 1.173900842666626\n",
      "[step: 208] cost: 1.1720225811004639\n",
      "[step: 209] cost: 1.1704463958740234\n",
      "[step: 210] cost: 1.1693202257156372\n",
      "학습 정확도: 0.5147392\n",
      "테스트 정확도: 0.23684211\n",
      "[step: 211] cost: 1.1693115234375\n",
      "[step: 212] cost: 1.167734980583191\n",
      "[step: 213] cost: 1.1642495393753052\n",
      "[step: 214] cost: 1.1599509716033936\n",
      "[step: 215] cost: 1.1593098640441895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 216] cost: 1.1592100858688354\n",
      "[step: 217] cost: 1.1551833152770996\n",
      "[step: 218] cost: 1.1524972915649414\n",
      "[step: 219] cost: 1.1522608995437622\n",
      "[step: 220] cost: 1.1498545408248901\n",
      "학습 정확도: 0.5419501\n",
      "테스트 정확도: 0.22105263\n",
      "[step: 221] cost: 1.1467243432998657\n",
      "[step: 222] cost: 1.1455084085464478\n",
      "[step: 223] cost: 1.1440818309783936\n",
      "[step: 224] cost: 1.1412978172302246\n",
      "[step: 225] cost: 1.1392238140106201\n",
      "[step: 226] cost: 1.1380139589309692\n",
      "[step: 227] cost: 1.1359022855758667\n",
      "[step: 228] cost: 1.1333342790603638\n",
      "[step: 229] cost: 1.1317341327667236\n",
      "[step: 230] cost: 1.1301796436309814\n",
      "학습 정확도: 0.5442177\n",
      "테스트 정확도: 0.22105263\n",
      "[step: 231] cost: 1.1277167797088623\n",
      "[step: 232] cost: 1.1255062818527222\n",
      "[step: 233] cost: 1.123914122581482\n",
      "[step: 234] cost: 1.1220029592514038\n",
      "[step: 235] cost: 1.119668960571289\n",
      "[step: 236] cost: 1.1175129413604736\n",
      "[step: 237] cost: 1.1157411336898804\n",
      "[step: 238] cost: 1.113855004310608\n",
      "[step: 239] cost: 1.1115901470184326\n",
      "[step: 240] cost: 1.1093460321426392\n",
      "학습 정확도: 0.5555556\n",
      "테스트 정확도: 0.22105263\n",
      "[step: 241] cost: 1.1073462963104248\n",
      "[step: 242] cost: 1.1054408550262451\n",
      "[step: 243] cost: 1.1033990383148193\n",
      "[step: 244] cost: 1.1011568307876587\n",
      "[step: 245] cost: 1.0989080667495728\n",
      "[step: 246] cost: 1.096755862236023\n",
      "[step: 247] cost: 1.0946992635726929\n",
      "[step: 248] cost: 1.092665672302246\n",
      "[step: 249] cost: 1.0905739068984985\n",
      "[step: 250] cost: 1.0884345769882202\n",
      "학습 정확도: 0.57369614\n",
      "테스트 정확도: 0.21052632\n",
      "[step: 251] cost: 1.0862302780151367\n",
      "[step: 252] cost: 1.0840071439743042\n",
      "[step: 253] cost: 1.0817694664001465\n",
      "[step: 254] cost: 1.0795457363128662\n",
      "[step: 255] cost: 1.0773261785507202\n",
      "[step: 256] cost: 1.0751386880874634\n",
      "[step: 257] cost: 1.0729900598526\n",
      "[step: 258] cost: 1.0709614753723145\n",
      "[step: 259] cost: 1.0691471099853516\n",
      "[step: 260] cost: 1.0679465532302856\n",
      "학습 정확도: 0.5873016\n",
      "테스트 정확도: 0.22105263\n",
      "[step: 261] cost: 1.0674102306365967\n",
      "[step: 262] cost: 1.0681105852127075\n",
      "[step: 263] cost: 1.065476417541504\n",
      "[step: 264] cost: 1.0604500770568848\n",
      "[step: 265] cost: 1.0554643869400024\n",
      "[step: 266] cost: 1.0551642179489136\n",
      "[step: 267] cost: 1.0557568073272705\n",
      "[step: 268] cost: 1.051487684249878\n",
      "[step: 269] cost: 1.0473403930664062\n",
      "[step: 270] cost: 1.0467482805252075\n",
      "학습 정확도: 0.5963719\n",
      "테스트 정확도: 0.2263158\n",
      "[step: 271] cost: 1.0458418130874634\n",
      "[step: 272] cost: 1.0424189567565918\n",
      "[step: 273] cost: 1.0393295288085938\n",
      "[step: 274] cost: 1.0385993719100952\n",
      "[step: 275] cost: 1.0372394323349\n",
      "[step: 276] cost: 1.0337445735931396\n",
      "[step: 277] cost: 1.0314581394195557\n",
      "[step: 278] cost: 1.030543565750122\n",
      "[step: 279] cost: 1.0284183025360107\n",
      "[step: 280] cost: 1.0254582166671753\n",
      "학습 정확도: 0.6099773\n",
      "테스트 정확도: 0.23157895\n",
      "[step: 281] cost: 1.0234463214874268\n",
      "[step: 282] cost: 1.0221328735351562\n",
      "[step: 283] cost: 1.020108699798584\n",
      "[step: 284] cost: 1.0174052715301514\n",
      "[step: 285] cost: 1.0152877569198608\n",
      "[step: 286] cost: 1.0137579441070557\n",
      "[step: 287] cost: 1.0118411779403687\n",
      "[step: 288] cost: 1.0094300508499146\n",
      "[step: 289] cost: 1.0071041584014893\n",
      "[step: 290] cost: 1.0052381753921509\n",
      "학습 정확도: 0.6235828\n",
      "테스트 정확도: 0.23157895\n",
      "[step: 291] cost: 1.0035065412521362\n",
      "[step: 292] cost: 1.0014519691467285\n",
      "[step: 293] cost: 0.9991481304168701\n",
      "[step: 294] cost: 0.9968851208686829\n",
      "[step: 295] cost: 0.9948585033416748\n",
      "[step: 296] cost: 0.9929805994033813\n",
      "[step: 297] cost: 0.9910537600517273\n",
      "[step: 298] cost: 0.9890016317367554\n",
      "[step: 299] cost: 0.9867965579032898\n",
      "[step: 300] cost: 0.9845678210258484\n",
      "학습 정확도: 0.6371882\n",
      "테스트 정확도: 0.23157895\n",
      "[step: 301] cost: 0.9823776483535767\n",
      "[step: 302] cost: 0.9802567958831787\n",
      "[step: 303] cost: 0.9781966805458069\n",
      "[step: 304] cost: 0.9761726260185242\n",
      "[step: 305] cost: 0.9741830229759216\n",
      "[step: 306] cost: 0.9722298383712769\n",
      "[step: 307] cost: 0.9703856110572815\n",
      "[step: 308] cost: 0.9686437249183655\n",
      "[step: 309] cost: 0.9672232866287231\n",
      "[step: 310] cost: 0.9658840298652649\n",
      "학습 정확도: 0.64625853\n",
      "테스트 정확도: 0.23157895\n",
      "[step: 311] cost: 0.9649217128753662\n",
      "[step: 312] cost: 0.9628894329071045\n",
      "[step: 313] cost: 0.9601970314979553\n",
      "[step: 314] cost: 0.9562904834747314\n",
      "[step: 315] cost: 0.9531257748603821\n",
      "[step: 316] cost: 0.9513622522354126\n",
      "[step: 317] cost: 0.95037442445755\n",
      "[step: 318] cost: 0.9491109848022461\n",
      "[step: 319] cost: 0.9465361833572388\n",
      "[step: 320] cost: 0.9434789419174194\n",
      "학습 정확도: 0.6507937\n",
      "테스트 정확도: 0.23684211\n",
      "[step: 321] cost: 0.940832257270813\n",
      "[step: 322] cost: 0.9391010999679565\n",
      "[step: 323] cost: 0.9377783536911011\n",
      "[step: 324] cost: 0.9359759092330933\n",
      "[step: 325] cost: 0.9335991740226746\n",
      "[step: 326] cost: 0.9308907389640808\n",
      "[step: 327] cost: 0.928529679775238\n",
      "[step: 328] cost: 0.9266672730445862\n",
      "[step: 329] cost: 0.9250119924545288\n",
      "[step: 330] cost: 0.923215925693512\n",
      "학습 정확도: 0.6553288\n",
      "테스트 정확도: 0.23684211\n",
      "[step: 331] cost: 0.9210294485092163\n",
      "[step: 332] cost: 0.9186500310897827\n",
      "[step: 333] cost: 0.916250467300415\n",
      "[step: 334] cost: 0.9140444993972778\n",
      "[step: 335] cost: 0.9120481610298157\n",
      "[step: 336] cost: 0.9101642370223999\n",
      "[step: 337] cost: 0.9082966446876526\n",
      "[step: 338] cost: 0.9063376188278198\n",
      "[step: 339] cost: 0.9043087363243103\n",
      "[step: 340] cost: 0.9021468758583069\n",
      "학습 정확도: 0.67573696\n",
      "테스트 정확도: 0.23157895\n",
      "[step: 341] cost: 0.8999525904655457\n",
      "[step: 342] cost: 0.8977038264274597\n",
      "[step: 343] cost: 0.8954750299453735\n",
      "[step: 344] cost: 0.8932575583457947\n",
      "[step: 345] cost: 0.8910712003707886\n",
      "[step: 346] cost: 0.8889074325561523\n",
      "[step: 347] cost: 0.8867592811584473\n",
      "[step: 348] cost: 0.8846190571784973\n",
      "[step: 349] cost: 0.8824805617332458\n",
      "[step: 350] cost: 0.8803406357765198\n",
      "학습 정확도: 0.6825397\n",
      "테스트 정확도: 0.23157895\n",
      "[step: 351] cost: 0.878196656703949\n",
      "[step: 352] cost: 0.87604820728302\n",
      "[step: 353] cost: 0.8738952875137329\n",
      "[step: 354] cost: 0.8717386722564697\n",
      "[step: 355] cost: 0.8695817589759827\n",
      "[step: 356] cost: 0.8674350380897522\n",
      "[step: 357] cost: 0.865328848361969\n",
      "[step: 358] cost: 0.8633583188056946\n",
      "[step: 359] cost: 0.8618481755256653\n",
      "[step: 360] cost: 0.8616917133331299\n",
      "학습 정확도: 0.68480724\n",
      "테스트 정확도: 0.2263158\n",
      "[step: 361] cost: 0.8657189011573792\n",
      "[step: 362] cost: 0.8737828135490417\n",
      "[step: 363] cost: 0.8804004192352295\n",
      "[step: 364] cost: 0.860687255859375\n",
      "[step: 365] cost: 0.8524063229560852\n",
      "[step: 366] cost: 0.8632256984710693\n",
      "[step: 367] cost: 0.854847252368927\n",
      "[step: 368] cost: 0.8465014696121216\n",
      "[step: 369] cost: 0.8542276620864868\n",
      "[step: 370] cost: 0.8437285423278809\n",
      "학습 정확도: 0.70068026\n",
      "테스트 정확도: 0.23157895\n",
      "[step: 371] cost: 0.8439584374427795\n",
      "[step: 372] cost: 0.844291627407074\n",
      "[step: 373] cost: 0.8367740511894226\n",
      "[step: 374] cost: 0.8398145437240601\n",
      "[step: 375] cost: 0.8341787457466125\n",
      "[step: 376] cost: 0.8336603045463562\n",
      "[step: 377] cost: 0.8322651982307434\n",
      "[step: 378] cost: 0.8288151621818542\n",
      "[step: 379] cost: 0.8280606865882874\n",
      "[step: 380] cost: 0.8254863619804382\n",
      "학습 정확도: 0.7097506\n",
      "테스트 정확도: 0.2263158\n",
      "[step: 381] cost: 0.8233482837677002\n",
      "[step: 382] cost: 0.8225829005241394\n",
      "[step: 383] cost: 0.819339394569397\n",
      "[step: 384] cost: 0.8191819787025452\n",
      "[step: 385] cost: 0.8161015510559082\n",
      "[step: 386] cost: 0.8149650692939758\n",
      "[step: 387] cost: 0.8130587935447693\n",
      "[step: 388] cost: 0.810999870300293\n",
      "[step: 389] cost: 0.8097937703132629\n",
      "[step: 390] cost: 0.8075105547904968\n",
      "학습 정확도: 0.71882087\n",
      "테스트 정확도: 0.2263158\n",
      "[step: 391] cost: 0.8061193823814392\n",
      "[step: 392] cost: 0.8042758107185364\n",
      "[step: 393] cost: 0.8024206161499023\n",
      "[step: 394] cost: 0.8009057641029358\n",
      "[step: 395] cost: 0.7989842891693115\n",
      "[step: 396] cost: 0.7972943782806396\n",
      "[step: 397] cost: 0.7957219481468201\n",
      "[step: 398] cost: 0.7936623096466064\n",
      "[step: 399] cost: 0.792275071144104\n",
      "[step: 400] cost: 0.7902493476867676\n",
      "학습 정확도: 0.73015875\n",
      "테스트 정확도: 0.22105263\n",
      "[step: 401] cost: 0.7886413335800171\n",
      "[step: 402] cost: 0.7869033217430115\n",
      "[step: 403] cost: 0.7850897908210754\n",
      "[step: 404] cost: 0.7833582758903503\n",
      "[step: 405] cost: 0.7816973328590393\n",
      "[step: 406] cost: 0.7797357439994812\n",
      "[step: 407] cost: 0.7781869173049927\n",
      "[step: 408] cost: 0.7762807011604309\n",
      "[step: 409] cost: 0.7745211720466614\n",
      "[step: 410] cost: 0.772797167301178\n",
      "학습 정확도: 0.73015875\n",
      "테스트 정확도: 0.22105263\n",
      "[step: 411] cost: 0.770983874797821\n",
      "[step: 412] cost: 0.7691308259963989\n",
      "[step: 413] cost: 0.7674675583839417\n",
      "[step: 414] cost: 0.7655518054962158\n",
      "[step: 415] cost: 0.7637700438499451\n",
      "[step: 416] cost: 0.7620008587837219\n",
      "[step: 417] cost: 0.7601463794708252\n",
      "[step: 418] cost: 0.7582892179489136\n",
      "[step: 419] cost: 0.7565337419509888\n",
      "[step: 420] cost: 0.7546448707580566\n",
      "학습 정확도: 0.73015875\n",
      "테스트 정확도: 0.21578947\n",
      "[step: 421] cost: 0.7527971267700195\n",
      "[step: 422] cost: 0.7509718537330627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 423] cost: 0.7491216063499451\n",
      "[step: 424] cost: 0.7472283840179443\n",
      "[step: 425] cost: 0.7453820109367371\n",
      "[step: 426] cost: 0.7435087561607361\n",
      "[step: 427] cost: 0.7416317462921143\n",
      "[step: 428] cost: 0.7397322058677673\n",
      "[step: 429] cost: 0.737845778465271\n",
      "[step: 430] cost: 0.7359555959701538\n",
      "학습 정확도: 0.7346939\n",
      "테스트 정확도: 0.21578947\n",
      "[step: 431] cost: 0.7340583801269531\n",
      "[step: 432] cost: 0.7321305871009827\n",
      "[step: 433] cost: 0.7302212715148926\n",
      "[step: 434] cost: 0.7283118963241577\n",
      "[step: 435] cost: 0.7263898253440857\n",
      "[step: 436] cost: 0.7244478464126587\n",
      "[step: 437] cost: 0.7225141525268555\n",
      "[step: 438] cost: 0.7205742001533508\n",
      "[step: 439] cost: 0.7186343669891357\n",
      "[step: 440] cost: 0.7166869640350342\n",
      "학습 정확도: 0.7414966\n",
      "테스트 정확도: 0.2263158\n",
      "[step: 441] cost: 0.7147334218025208\n",
      "[step: 442] cost: 0.7127670049667358\n",
      "[step: 443] cost: 0.7108035087585449\n",
      "[step: 444] cost: 0.70883709192276\n",
      "[step: 445] cost: 0.7068691849708557\n",
      "[step: 446] cost: 0.7048950791358948\n",
      "[step: 447] cost: 0.7029216289520264\n",
      "[step: 448] cost: 0.7009413838386536\n",
      "[step: 449] cost: 0.6989582777023315\n",
      "[step: 450] cost: 0.696972131729126\n",
      "학습 정확도: 0.7505669\n",
      "테스트 정확도: 0.21052632\n",
      "[step: 451] cost: 0.6949877142906189\n",
      "[step: 452] cost: 0.6930025219917297\n",
      "[step: 453] cost: 0.6910263299942017\n",
      "[step: 454] cost: 0.6890705823898315\n",
      "[step: 455] cost: 0.6871685981750488\n",
      "[step: 456] cost: 0.6853943467140198\n",
      "[step: 457] cost: 0.6839337944984436\n",
      "[step: 458] cost: 0.6832067966461182\n",
      "[step: 459] cost: 0.6836884021759033\n",
      "[step: 460] cost: 0.6851863861083984\n",
      "학습 정확도: 0.7641723\n",
      "테스트 정확도: 0.20526315\n",
      "[step: 461] cost: 0.6837424635887146\n",
      "[step: 462] cost: 0.6777098178863525\n",
      "[step: 463] cost: 0.6717645525932312\n",
      "[step: 464] cost: 0.671582818031311\n",
      "[step: 465] cost: 0.6728721261024475\n",
      "[step: 466] cost: 0.669354259967804\n",
      "[step: 467] cost: 0.6645713448524475\n",
      "[step: 468] cost: 0.6633078455924988\n",
      "[step: 469] cost: 0.6635151505470276\n",
      "[step: 470] cost: 0.6610338687896729\n",
      "학습 정확도: 0.76870745\n",
      "테스트 정확도: 0.21052632\n",
      "[step: 471] cost: 0.6570516228675842\n",
      "[step: 472] cost: 0.6559679508209229\n",
      "[step: 473] cost: 0.655718982219696\n",
      "[step: 474] cost: 0.6526248455047607\n",
      "[step: 475] cost: 0.6496158242225647\n",
      "[step: 476] cost: 0.6487777829170227\n",
      "[step: 477] cost: 0.6475479006767273\n",
      "[step: 478] cost: 0.6447829604148865\n",
      "[step: 479] cost: 0.6424643993377686\n",
      "[step: 480] cost: 0.6414064764976501\n",
      "학습 정확도: 0.77324265\n",
      "테스트 정확도: 0.21578947\n",
      "[step: 481] cost: 0.6398847103118896\n",
      "[step: 482] cost: 0.6373708248138428\n",
      "[step: 483] cost: 0.6353685855865479\n",
      "[step: 484] cost: 0.6340049505233765\n",
      "[step: 485] cost: 0.6323078870773315\n",
      "[step: 486] cost: 0.6301975250244141\n",
      "[step: 487] cost: 0.6281964778900146\n",
      "[step: 488] cost: 0.6265431642532349\n",
      "[step: 489] cost: 0.6249569058418274\n",
      "[step: 490] cost: 0.6230954527854919\n",
      "학습 정확도: 0.78458047\n",
      "테스트 정확도: 0.20526315\n",
      "[step: 491] cost: 0.6210597157478333\n",
      "[step: 492] cost: 0.6192156672477722\n",
      "[step: 493] cost: 0.6176272630691528\n",
      "[step: 494] cost: 0.6159502863883972\n",
      "[step: 495] cost: 0.6140055656433105\n",
      "[step: 496] cost: 0.6120583415031433\n",
      "[step: 497] cost: 0.6103296875953674\n",
      "[step: 498] cost: 0.6086897850036621\n",
      "[step: 499] cost: 0.606935441493988\n",
      "[step: 500] cost: 0.6050712466239929\n",
      "학습 정확도: 0.78684807\n",
      "테스트 정확도: 0.20526315\n",
      "[step: 501] cost: 0.6032184958457947\n",
      "[step: 502] cost: 0.6014339923858643\n",
      "[step: 503] cost: 0.5997047424316406\n",
      "[step: 504] cost: 0.5979914665222168\n",
      "[step: 505] cost: 0.5962421894073486\n",
      "[step: 506] cost: 0.5944368839263916\n",
      "[step: 507] cost: 0.5926063656806946\n",
      "[step: 508] cost: 0.5908138155937195\n",
      "[step: 509] cost: 0.5890747308731079\n",
      "[step: 510] cost: 0.587354302406311\n",
      "학습 정확도: 0.8072562\n",
      "테스트 정확도: 0.20526315\n",
      "[step: 511] cost: 0.5856211185455322\n",
      "[step: 512] cost: 0.5838692784309387\n",
      "[step: 513] cost: 0.5821095705032349\n",
      "[step: 514] cost: 0.5803430080413818\n",
      "[step: 515] cost: 0.5785704851150513\n",
      "[step: 516] cost: 0.5768008828163147\n",
      "[step: 517] cost: 0.5750453472137451\n",
      "[step: 518] cost: 0.5733059048652649\n",
      "[step: 519] cost: 0.5715760588645935\n",
      "[step: 520] cost: 0.5698489546775818\n",
      "학습 정확도: 0.8095238\n",
      "테스트 정확도: 0.20526315\n",
      "[step: 521] cost: 0.5681247711181641\n",
      "[step: 522] cost: 0.5664070844650269\n",
      "[step: 523] cost: 0.5646978616714478\n",
      "[step: 524] cost: 0.5629978179931641\n",
      "[step: 525] cost: 0.5613078474998474\n",
      "[step: 526] cost: 0.5596369504928589\n",
      "[step: 527] cost: 0.5579959154129028\n",
      "[step: 528] cost: 0.5564082264900208\n",
      "[step: 529] cost: 0.5548999309539795\n",
      "[step: 530] cost: 0.5535167455673218\n",
      "학습 정확도: 0.8208617\n",
      "테스트 정확도: 0.2\n",
      "[step: 531] cost: 0.5522952675819397\n",
      "[step: 532] cost: 0.5512762069702148\n",
      "[step: 533] cost: 0.5503409504890442\n",
      "[step: 534] cost: 0.5492992401123047\n",
      "[step: 535] cost: 0.5475555062294006\n",
      "[step: 536] cost: 0.5450760126113892\n",
      "[step: 537] cost: 0.5420293211936951\n",
      "[step: 538] cost: 0.5395240783691406\n",
      "[step: 539] cost: 0.5379911065101624\n",
      "[step: 540] cost: 0.5370630025863647\n",
      "학습 정확도: 0.8276644\n",
      "테스트 정확도: 0.20526315\n",
      "[step: 541] cost: 0.5359556078910828\n",
      "[step: 542] cost: 0.5341455936431885\n",
      "[step: 543] cost: 0.5319355130195618\n",
      "[step: 544] cost: 0.5298553705215454\n",
      "[step: 545] cost: 0.5283209085464478\n",
      "[step: 546] cost: 0.5270810127258301\n",
      "[step: 547] cost: 0.5256586670875549\n",
      "[step: 548] cost: 0.5238952040672302\n",
      "[step: 549] cost: 0.5219900012016296\n",
      "[step: 550] cost: 0.52030348777771\n",
      "학습 정확도: 0.8344671\n",
      "테스트 정확도: 0.2\n",
      "[step: 551] cost: 0.5188676714897156\n",
      "[step: 552] cost: 0.5174407362937927\n",
      "[step: 553] cost: 0.5158281326293945\n",
      "[step: 554] cost: 0.5140864849090576\n",
      "[step: 555] cost: 0.512424111366272\n",
      "[step: 556] cost: 0.5109273791313171\n",
      "[step: 557] cost: 0.5094911456108093\n",
      "[step: 558] cost: 0.5079603791236877\n",
      "[step: 559] cost: 0.5062999725341797\n",
      "[step: 560] cost: 0.5046301484107971\n",
      "학습 정확도: 0.84126985\n",
      "테스트 정확도: 0.2\n",
      "[step: 561] cost: 0.5030724406242371\n",
      "[step: 562] cost: 0.5016191005706787\n",
      "[step: 563] cost: 0.5001651048660278\n",
      "[step: 564] cost: 0.49862560629844666\n",
      "[step: 565] cost: 0.4970191717147827\n",
      "[step: 566] cost: 0.49542298913002014\n",
      "[step: 567] cost: 0.49389052391052246\n",
      "[step: 568] cost: 0.4924013018608093\n",
      "[step: 569] cost: 0.4909020662307739\n",
      "[step: 570] cost: 0.48936963081359863\n",
      "학습 정확도: 0.83900225\n",
      "테스트 정확도: 0.19473684\n",
      "[step: 571] cost: 0.48782625794410706\n",
      "[step: 572] cost: 0.486306369304657\n",
      "[step: 573] cost: 0.4848168194293976\n",
      "[step: 574] cost: 0.48333269357681274\n",
      "[step: 575] cost: 0.4818311333656311\n",
      "[step: 576] cost: 0.480307012796402\n",
      "[step: 577] cost: 0.47878405451774597\n",
      "[step: 578] cost: 0.47727838158607483\n",
      "[step: 579] cost: 0.47579023241996765\n",
      "[step: 580] cost: 0.47430530190467834\n",
      "학습 정확도: 0.8435374\n",
      "테스트 정확도: 0.19473684\n",
      "[step: 581] cost: 0.47281140089035034\n",
      "[step: 582] cost: 0.4713122546672821\n",
      "[step: 583] cost: 0.46981969475746155\n",
      "[step: 584] cost: 0.468344122171402\n",
      "[step: 585] cost: 0.4668857157230377\n",
      "[step: 586] cost: 0.4654393494129181\n",
      "[step: 587] cost: 0.46401283144950867\n",
      "[step: 588] cost: 0.4626191258430481\n",
      "[step: 589] cost: 0.4612933397293091\n",
      "[step: 590] cost: 0.4600791335105896\n",
      "학습 정확도: 0.8435374\n",
      "테스트 정확도: 0.18947369\n",
      "[step: 591] cost: 0.45900389552116394\n",
      "[step: 592] cost: 0.45814886689186096\n",
      "[step: 593] cost: 0.45739978551864624\n",
      "[step: 594] cost: 0.45670285820961\n",
      "[step: 595] cost: 0.45555850863456726\n",
      "[step: 596] cost: 0.45356371998786926\n",
      "[step: 597] cost: 0.45091110467910767\n",
      "[step: 598] cost: 0.4482075870037079\n",
      "[step: 599] cost: 0.4466288089752197\n",
      "[step: 600] cost: 0.4459537863731384\n",
      "학습 정확도: 0.845805\n",
      "테스트 정확도: 0.20526315\n",
      "[step: 601] cost: 0.4450700581073761\n",
      "[step: 602] cost: 0.4433971643447876\n",
      "[step: 603] cost: 0.44117096066474915\n",
      "[step: 604] cost: 0.4394521117210388\n",
      "[step: 605] cost: 0.4384370446205139\n",
      "[step: 606] cost: 0.43742215633392334\n",
      "[step: 607] cost: 0.4358960688114166\n",
      "[step: 608] cost: 0.4339894950389862\n",
      "[step: 609] cost: 0.43241703510284424\n",
      "[step: 610] cost: 0.4312994182109833\n",
      "학습 정확도: 0.8480726\n",
      "테스트 정확도: 0.2\n",
      "[step: 611] cost: 0.4301302433013916\n",
      "[step: 612] cost: 0.4286063015460968\n",
      "[step: 613] cost: 0.4268966019153595\n",
      "[step: 614] cost: 0.42544877529144287\n",
      "[step: 615] cost: 0.4242613613605499\n",
      "[step: 616] cost: 0.422978013753891\n",
      "[step: 617] cost: 0.4214692711830139\n",
      "[step: 618] cost: 0.4199043810367584\n",
      "[step: 619] cost: 0.41851329803466797\n",
      "[step: 620] cost: 0.4172437787055969\n",
      "학습 정확도: 0.861678\n",
      "테스트 정확도: 0.19473684\n",
      "[step: 621] cost: 0.4158991575241089\n",
      "[step: 622] cost: 0.41444292664527893\n",
      "[step: 623] cost: 0.41297370195388794\n",
      "[step: 624] cost: 0.4115811288356781\n",
      "[step: 625] cost: 0.41023892164230347\n",
      "[step: 626] cost: 0.40887683629989624\n",
      "[step: 627] cost: 0.4074777066707611\n",
      "[step: 628] cost: 0.40606042742729187\n",
      "[step: 629] cost: 0.4046502113342285\n",
      "[step: 630] cost: 0.4032628536224365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 정확도: 0.8707483\n",
      "테스트 정확도: 0.19473684\n",
      "[step: 631] cost: 0.4018973112106323\n",
      "[step: 632] cost: 0.40053197741508484\n",
      "[step: 633] cost: 0.39914172887802124\n",
      "[step: 634] cost: 0.3977278769016266\n",
      "[step: 635] cost: 0.39632248878479004\n",
      "[step: 636] cost: 0.39494654536247253\n",
      "[step: 637] cost: 0.3935849666595459\n",
      "[step: 638] cost: 0.39220893383026123\n",
      "[step: 639] cost: 0.39081159234046936\n",
      "[step: 640] cost: 0.38941097259521484\n",
      "학습 정확도: 0.877551\n",
      "테스트 정확도: 0.19473684\n",
      "[step: 641] cost: 0.3880223333835602\n",
      "[step: 642] cost: 0.38664379715919495\n",
      "[step: 643] cost: 0.38526639342308044\n",
      "[step: 644] cost: 0.38388654589653015\n",
      "[step: 645] cost: 0.38250401616096497\n",
      "[step: 646] cost: 0.3811180591583252\n",
      "[step: 647] cost: 0.37972867488861084\n",
      "[step: 648] cost: 0.3783408999443054\n",
      "[step: 649] cost: 0.3769593834877014\n",
      "[step: 650] cost: 0.37558314204216003\n",
      "학습 정확도: 0.8798186\n",
      "테스트 정확도: 0.19473684\n",
      "[step: 651] cost: 0.3742063045501709\n",
      "[step: 652] cost: 0.3728254437446594\n",
      "[step: 653] cost: 0.3714424669742584\n",
      "[step: 654] cost: 0.3700597882270813\n",
      "[step: 655] cost: 0.36867842078208923\n",
      "[step: 656] cost: 0.3672977685928345\n",
      "[step: 657] cost: 0.36591866612434387\n",
      "[step: 658] cost: 0.36454200744628906\n",
      "[step: 659] cost: 0.3631671667098999\n",
      "[step: 660] cost: 0.3617924153804779\n",
      "학습 정확도: 0.8866213\n",
      "테스트 정확도: 0.19473684\n",
      "[step: 661] cost: 0.36041688919067383\n",
      "[step: 662] cost: 0.35904139280319214\n",
      "[step: 663] cost: 0.3576667308807373\n",
      "[step: 664] cost: 0.3562929928302765\n",
      "[step: 665] cost: 0.3549198508262634\n",
      "[step: 666] cost: 0.3535476326942444\n",
      "[step: 667] cost: 0.3521770238876343\n",
      "[step: 668] cost: 0.35080811381340027\n",
      "[step: 669] cost: 0.34944066405296326\n",
      "[step: 670] cost: 0.348074346780777\n",
      "학습 정확도: 0.8866213\n",
      "테스트 정확도: 0.19473684\n",
      "[step: 671] cost: 0.34670937061309814\n",
      "[step: 672] cost: 0.34534597396850586\n",
      "[step: 673] cost: 0.3439842462539673\n",
      "[step: 674] cost: 0.34262388944625854\n",
      "[step: 675] cost: 0.3412649929523468\n",
      "[step: 676] cost: 0.3399078845977783\n",
      "[step: 677] cost: 0.3385527431964874\n",
      "[step: 678] cost: 0.33719974756240845\n",
      "[step: 679] cost: 0.3358488976955414\n",
      "[step: 680] cost: 0.33450067043304443\n",
      "학습 정확도: 0.89569163\n",
      "테스트 정확도: 0.19473684\n",
      "[step: 681] cost: 0.33315587043762207\n",
      "[step: 682] cost: 0.33181503415107727\n",
      "[step: 683] cost: 0.33047977089881897\n",
      "[step: 684] cost: 0.32915198802948\n",
      "[step: 685] cost: 0.3278357982635498\n",
      "[step: 686] cost: 0.32653751969337463\n",
      "[step: 687] cost: 0.32526907324790955\n",
      "[step: 688] cost: 0.3240484893321991\n",
      "[step: 689] cost: 0.32291287183761597\n",
      "[step: 690] cost: 0.3219135105609894\n",
      "학습 정확도: 0.9047619\n",
      "테스트 정확도: 0.18947369\n",
      "[step: 691] cost: 0.32115665078163147\n",
      "[step: 692] cost: 0.3207542896270752\n",
      "[step: 693] cost: 0.3209303915500641\n",
      "[step: 694] cost: 0.32168230414390564\n",
      "[step: 695] cost: 0.3230498731136322\n",
      "[step: 696] cost: 0.32372450828552246\n",
      "[step: 697] cost: 0.32287606596946716\n",
      "[step: 698] cost: 0.31711453199386597\n",
      "[step: 699] cost: 0.31120097637176514\n",
      "[step: 700] cost: 0.3111284375190735\n",
      "학습 정확도: 0.9047619\n",
      "테스트 정확도: 0.2\n",
      "[step: 701] cost: 0.3123921751976013\n",
      "[step: 702] cost: 0.30852624773979187\n",
      "[step: 703] cost: 0.30607590079307556\n",
      "[step: 704] cost: 0.3075636923313141\n",
      "[step: 705] cost: 0.3048606812953949\n",
      "[step: 706] cost: 0.3021123707294464\n",
      "[step: 707] cost: 0.30333811044692993\n",
      "[step: 708] cost: 0.3008926510810852\n",
      "[step: 709] cost: 0.29846757650375366\n",
      "[step: 710] cost: 0.299343466758728\n",
      "학습 정확도: 0.9047619\n",
      "테스트 정확도: 0.19473684\n",
      "[step: 711] cost: 0.29688072204589844\n",
      "[step: 712] cost: 0.2950151264667511\n",
      "[step: 713] cost: 0.29548180103302\n",
      "[step: 714] cost: 0.29290467500686646\n",
      "[step: 715] cost: 0.29173415899276733\n",
      "[step: 716] cost: 0.29162606596946716\n",
      "[step: 717] cost: 0.28914183378219604\n",
      "[step: 718] cost: 0.28847506642341614\n",
      "[step: 719] cost: 0.28776106238365173\n",
      "[step: 720] cost: 0.2856317162513733\n",
      "학습 정확도: 0.9206349\n",
      "테스트 정확도: 0.18947369\n",
      "[step: 721] cost: 0.2851817011833191\n",
      "[step: 722] cost: 0.2839963436126709\n",
      "[step: 723] cost: 0.28234565258026123\n",
      "[step: 724] cost: 0.2818121910095215\n",
      "[step: 725] cost: 0.28040406107902527\n",
      "[step: 726] cost: 0.2791770100593567\n",
      "[step: 727] cost: 0.27840694785118103\n",
      "[step: 728] cost: 0.2770019471645355\n",
      "[step: 729] cost: 0.2760249972343445\n",
      "[step: 730] cost: 0.2750151753425598\n",
      "학습 정확도: 0.92743766\n",
      "테스트 정확도: 0.18947369\n",
      "[step: 731] cost: 0.2737485468387604\n",
      "[step: 732] cost: 0.2728397846221924\n",
      "[step: 733] cost: 0.27168458700180054\n",
      "[step: 734] cost: 0.27058109641075134\n",
      "[step: 735] cost: 0.2696271538734436\n",
      "[step: 736] cost: 0.2684420049190521\n",
      "[step: 737] cost: 0.26745182275772095\n",
      "[step: 738] cost: 0.2664201557636261\n",
      "[step: 739] cost: 0.26528429985046387\n",
      "[step: 740] cost: 0.26433655619621277\n",
      "학습 정확도: 0.9297052\n",
      "테스트 정확도: 0.18947369\n",
      "[step: 741] cost: 0.26325199007987976\n",
      "[step: 742] cost: 0.26219087839126587\n",
      "[step: 743] cost: 0.26123347878456116\n",
      "[step: 744] cost: 0.26013892889022827\n",
      "[step: 745] cost: 0.2591380178928375\n",
      "[step: 746] cost: 0.2581512928009033\n",
      "[step: 747] cost: 0.2570800185203552\n",
      "[step: 748] cost: 0.25611013174057007\n",
      "[step: 749] cost: 0.25509941577911377\n",
      "[step: 750] cost: 0.2540659010410309\n",
      "학습 정확도: 0.9297052\n",
      "테스트 정확도: 0.18947369\n",
      "[step: 751] cost: 0.2531023919582367\n",
      "[step: 752] cost: 0.2520848512649536\n",
      "[step: 753] cost: 0.25108662247657776\n",
      "[step: 754] cost: 0.25011691451072693\n",
      "[step: 755] cost: 0.24910911917686462\n",
      "[step: 756] cost: 0.24813508987426758\n",
      "[step: 757] cost: 0.24715900421142578\n",
      "[step: 758] cost: 0.24617008864879608\n",
      "[step: 759] cost: 0.24520859122276306\n",
      "[step: 760] cost: 0.2442323863506317\n",
      "학습 정확도: 0.9297052\n",
      "테스트 정확도: 0.18947369\n",
      "[step: 761] cost: 0.24326370656490326\n",
      "[step: 762] cost: 0.24230730533599854\n",
      "[step: 763] cost: 0.24133852124214172\n",
      "[step: 764] cost: 0.24038603901863098\n",
      "[step: 765] cost: 0.23943285644054413\n",
      "[step: 766] cost: 0.23847649991512299\n",
      "[step: 767] cost: 0.2375350296497345\n",
      "[step: 768] cost: 0.23658689856529236\n",
      "[step: 769] cost: 0.23564423620700836\n",
      "[step: 770] cost: 0.2347099781036377\n",
      "학습 정확도: 0.9297052\n",
      "테스트 정확도: 0.18947369\n",
      "[step: 771] cost: 0.23377013206481934\n",
      "[step: 772] cost: 0.23283973336219788\n",
      "[step: 773] cost: 0.2319115698337555\n",
      "[step: 774] cost: 0.23098227381706238\n",
      "[step: 775] cost: 0.23006169497966766\n",
      "[step: 776] cost: 0.2291404902935028\n",
      "[step: 777] cost: 0.2282223403453827\n",
      "[step: 778] cost: 0.22730977833271027\n",
      "[step: 779] cost: 0.22639691829681396\n",
      "[step: 780] cost: 0.22548907995224\n",
      "학습 정확도: 0.93424034\n",
      "테스트 정확도: 0.18947369\n",
      "[step: 781] cost: 0.22458405792713165\n",
      "[step: 782] cost: 0.22368064522743225\n",
      "[step: 783] cost: 0.22278191149234772\n",
      "[step: 784] cost: 0.22188478708267212\n",
      "[step: 785] cost: 0.22099101543426514\n",
      "[step: 786] cost: 0.220100536942482\n",
      "[step: 787] cost: 0.21921202540397644\n",
      "[step: 788] cost: 0.21832741796970367\n",
      "[step: 789] cost: 0.2174449861049652\n",
      "[step: 790] cost: 0.21656541526317596\n",
      "학습 정확도: 0.93424034\n",
      "테스트 정확도: 0.18947369\n",
      "[step: 791] cost: 0.21568940579891205\n",
      "[step: 792] cost: 0.21481537818908691\n",
      "[step: 793] cost: 0.21394474804401398\n",
      "[step: 794] cost: 0.21307693421840668\n",
      "[step: 795] cost: 0.21221157908439636\n",
      "[step: 796] cost: 0.21134966611862183\n",
      "[step: 797] cost: 0.21049007773399353\n",
      "[step: 798] cost: 0.2096334546804428\n",
      "[step: 799] cost: 0.2087799608707428\n",
      "[step: 800] cost: 0.2079288363456726\n",
      "학습 정확도: 0.93877554\n",
      "테스트 정확도: 0.2\n",
      "[step: 801] cost: 0.2070808708667755\n",
      "[step: 802] cost: 0.2062356024980545\n",
      "[step: 803] cost: 0.20539315044879913\n",
      "[step: 804] cost: 0.20455369353294373\n",
      "[step: 805] cost: 0.20371682941913605\n",
      "[step: 806] cost: 0.20288290083408356\n",
      "[step: 807] cost: 0.2020517885684967\n",
      "[step: 808] cost: 0.2012234777212143\n",
      "[step: 809] cost: 0.2003980576992035\n",
      "[step: 810] cost: 0.19957542419433594\n",
      "학습 정확도: 0.9455782\n",
      "테스트 정확도: 0.2\n",
      "[step: 811] cost: 0.19875569641590118\n",
      "[step: 812] cost: 0.1979387253522873\n",
      "[step: 813] cost: 0.19712460041046143\n",
      "[step: 814] cost: 0.19631333649158478\n",
      "[step: 815] cost: 0.19550485908985138\n",
      "[step: 816] cost: 0.19469928741455078\n",
      "[step: 817] cost: 0.1938965618610382\n",
      "[step: 818] cost: 0.19309668242931366\n",
      "[step: 819] cost: 0.19229966402053833\n",
      "[step: 820] cost: 0.19150550663471222\n",
      "학습 정확도: 0.95011336\n",
      "테스트 정확도: 0.2\n",
      "[step: 821] cost: 0.19071419537067413\n",
      "[step: 822] cost: 0.18992576003074646\n",
      "[step: 823] cost: 0.1891401708126068\n",
      "[step: 824] cost: 0.18835753202438354\n",
      "[step: 825] cost: 0.18757769465446472\n",
      "[step: 826] cost: 0.1868007630109787\n",
      "[step: 827] cost: 0.18602673709392548\n",
      "[step: 828] cost: 0.18525557219982147\n",
      "[step: 829] cost: 0.18448728322982788\n",
      "[step: 830] cost: 0.18372197449207306\n",
      "학습 정확도: 0.95464855\n",
      "테스트 정확도: 0.2\n",
      "[step: 831] cost: 0.18295952677726746\n",
      "[step: 832] cost: 0.18219992518424988\n",
      "[step: 833] cost: 0.18144327402114868\n",
      "[step: 834] cost: 0.18068961799144745\n",
      "[step: 835] cost: 0.17993871867656708\n",
      "[step: 836] cost: 0.17919081449508667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 837] cost: 0.17844584584236145\n",
      "[step: 838] cost: 0.17770376801490784\n",
      "[step: 839] cost: 0.17696459591388702\n",
      "[step: 840] cost: 0.1762283891439438\n",
      "학습 정확도: 0.95464855\n",
      "테스트 정확도: 0.2\n",
      "[step: 841] cost: 0.17549508810043335\n",
      "[step: 842] cost: 0.17476478219032288\n",
      "[step: 843] cost: 0.17403730750083923\n",
      "[step: 844] cost: 0.1733127385377884\n",
      "[step: 845] cost: 0.17259114980697632\n",
      "[step: 846] cost: 0.17187246680259705\n",
      "[step: 847] cost: 0.17115676403045654\n",
      "[step: 848] cost: 0.17044392228126526\n",
      "[step: 849] cost: 0.16973403096199036\n",
      "[step: 850] cost: 0.16902709007263184\n",
      "학습 정확도: 0.9569161\n",
      "테스트 정확도: 0.2\n",
      "[step: 851] cost: 0.16832302510738373\n",
      "[step: 852] cost: 0.16762188076972961\n",
      "[step: 853] cost: 0.16692370176315308\n",
      "[step: 854] cost: 0.16622835397720337\n",
      "[step: 855] cost: 0.16553597152233124\n",
      "[step: 856] cost: 0.1648464798927307\n",
      "[step: 857] cost: 0.1641598492860794\n",
      "[step: 858] cost: 0.1634760946035385\n",
      "[step: 859] cost: 0.162795290350914\n",
      "[step: 860] cost: 0.1621173471212387\n",
      "학습 정확도: 0.9569161\n",
      "테스트 정확도: 0.2\n",
      "[step: 861] cost: 0.16144219040870667\n",
      "[step: 862] cost: 0.16077002882957458\n",
      "[step: 863] cost: 0.16010065376758575\n",
      "[step: 864] cost: 0.15943413972854614\n",
      "[step: 865] cost: 0.15877047181129456\n",
      "[step: 866] cost: 0.1581096202135086\n",
      "[step: 867] cost: 0.1574515700340271\n",
      "[step: 868] cost: 0.1567963808774948\n",
      "[step: 869] cost: 0.15614396333694458\n",
      "[step: 870] cost: 0.15549437701702118\n",
      "학습 정확도: 0.9569161\n",
      "테스트 정확도: 0.2\n",
      "[step: 871] cost: 0.15484750270843506\n",
      "[step: 872] cost: 0.15420345962047577\n",
      "[step: 873] cost: 0.15356212854385376\n",
      "[step: 874] cost: 0.15292353928089142\n",
      "[step: 875] cost: 0.15228769183158875\n",
      "[step: 876] cost: 0.15165461599826813\n",
      "[step: 877] cost: 0.15102417767047882\n",
      "[step: 878] cost: 0.15039649605751038\n",
      "[step: 879] cost: 0.14977142214775085\n",
      "[step: 880] cost: 0.14914903044700623\n",
      "학습 정확도: 0.9659864\n",
      "테스트 정확도: 0.2\n",
      "[step: 881] cost: 0.14852933585643768\n",
      "[step: 882] cost: 0.14791224896907806\n",
      "[step: 883] cost: 0.14729779958724976\n",
      "[step: 884] cost: 0.1466858834028244\n",
      "[step: 885] cost: 0.14607664942741394\n",
      "[step: 886] cost: 0.14546996355056763\n",
      "[step: 887] cost: 0.14486584067344666\n",
      "[step: 888] cost: 0.14426423609256744\n",
      "[step: 889] cost: 0.1436651647090912\n",
      "[step: 890] cost: 0.1430685967206955\n",
      "학습 정확도: 0.9659864\n",
      "테스트 정확도: 0.20526315\n",
      "[step: 891] cost: 0.14247454702854156\n",
      "[step: 892] cost: 0.14188295602798462\n",
      "[step: 893] cost: 0.14129382371902466\n",
      "[step: 894] cost: 0.14070716500282288\n",
      "[step: 895] cost: 0.14012287557125092\n",
      "[step: 896] cost: 0.13954107463359833\n",
      "[step: 897] cost: 0.1389615535736084\n",
      "[step: 898] cost: 0.13838450610637665\n",
      "[step: 899] cost: 0.13780973851680756\n",
      "[step: 900] cost: 0.13723735511302948\n",
      "학습 정확도: 0.96825397\n",
      "테스트 정확도: 0.20526315\n",
      "[step: 901] cost: 0.13666726648807526\n",
      "[step: 902] cost: 0.13609950244426727\n",
      "[step: 903] cost: 0.13553400337696075\n",
      "[step: 904] cost: 0.1349707692861557\n",
      "[step: 905] cost: 0.1344098150730133\n",
      "[step: 906] cost: 0.1338510364294052\n",
      "[step: 907] cost: 0.1332944929599762\n",
      "[step: 908] cost: 0.13274013996124268\n",
      "[step: 909] cost: 0.13218796253204346\n",
      "[step: 910] cost: 0.13163799047470093\n",
      "학습 정확도: 0.9727891\n",
      "테스트 정확도: 0.2\n",
      "[step: 911] cost: 0.13109007477760315\n",
      "[step: 912] cost: 0.13054433465003967\n",
      "[step: 913] cost: 0.13000069558620453\n",
      "[step: 914] cost: 0.12945911288261414\n",
      "[step: 915] cost: 0.1289195567369461\n",
      "[step: 916] cost: 0.1283821165561676\n",
      "[step: 917] cost: 0.12784670293331146\n",
      "[step: 918] cost: 0.12731331586837769\n",
      "[step: 919] cost: 0.12678183615207672\n",
      "[step: 920] cost: 0.1262524127960205\n",
      "학습 정확도: 0.9727891\n",
      "테스트 정확도: 0.2\n",
      "[step: 921] cost: 0.1257249265909195\n",
      "[step: 922] cost: 0.1251993626356125\n",
      "[step: 923] cost: 0.12467571347951889\n",
      "[step: 924] cost: 0.12415400892496109\n",
      "[step: 925] cost: 0.12363416701555252\n",
      "[step: 926] cost: 0.12311618775129318\n",
      "[step: 927] cost: 0.12260007113218307\n",
      "[step: 928] cost: 0.12208578735589981\n",
      "[step: 929] cost: 0.12157334387302399\n",
      "[step: 930] cost: 0.12106268107891083\n",
      "학습 정확도: 0.9727891\n",
      "테스트 정확도: 0.2\n",
      "[step: 931] cost: 0.12055379897356033\n",
      "[step: 932] cost: 0.1200466975569725\n",
      "[step: 933] cost: 0.11954133212566376\n",
      "[step: 934] cost: 0.1190376952290535\n",
      "[step: 935] cost: 0.11853578686714172\n",
      "[step: 936] cost: 0.11803561449050903\n",
      "[step: 937] cost: 0.11753711104393005\n",
      "[step: 938] cost: 0.1170402392745018\n",
      "[step: 939] cost: 0.11654507368803024\n",
      "[step: 940] cost: 0.11605149507522583\n",
      "학습 정확도: 0.9750567\n",
      "테스트 정확도: 0.2\n",
      "[step: 941] cost: 0.11555960029363632\n",
      "[step: 942] cost: 0.11506928503513336\n",
      "[step: 943] cost: 0.11458057910203934\n",
      "[step: 944] cost: 0.11409343779087067\n",
      "[step: 945] cost: 0.11360785365104675\n",
      "[step: 946] cost: 0.113123819231987\n",
      "[step: 947] cost: 0.1126413494348526\n",
      "[step: 948] cost: 0.11216036230325699\n",
      "[step: 949] cost: 0.11168093979358673\n",
      "[step: 950] cost: 0.11120295524597168\n",
      "학습 정확도: 0.97732425\n",
      "테스트 정확도: 0.2\n",
      "[step: 951] cost: 0.1107264831662178\n",
      "[step: 952] cost: 0.11025144904851913\n",
      "[step: 953] cost: 0.10977789014577866\n",
      "[step: 954] cost: 0.10930578410625458\n",
      "[step: 955] cost: 0.10883510112762451\n",
      "[step: 956] cost: 0.10836582630872726\n",
      "[step: 957] cost: 0.10789794474840164\n",
      "[step: 958] cost: 0.10743143409490585\n",
      "[step: 959] cost: 0.10696634650230408\n",
      "[step: 960] cost: 0.10650260001420975\n",
      "학습 정확도: 0.97732425\n",
      "테스트 정확도: 0.2\n",
      "[step: 961] cost: 0.10604020953178406\n",
      "[step: 962] cost: 0.10557915270328522\n",
      "[step: 963] cost: 0.10511946678161621\n",
      "[step: 964] cost: 0.10466109216213226\n",
      "[step: 965] cost: 0.10420399159193039\n",
      "[step: 966] cost: 0.10374822467565536\n",
      "[step: 967] cost: 0.1032937616109848\n",
      "[step: 968] cost: 0.10284056514501572\n",
      "[step: 969] cost: 0.10238861292600632\n",
      "[step: 970] cost: 0.10193796455860138\n",
      "학습 정확도: 0.97732425\n",
      "테스트 정확도: 0.2\n",
      "[step: 971] cost: 0.10148852318525314\n",
      "[step: 972] cost: 0.10104034841060638\n",
      "[step: 973] cost: 0.1005934402346611\n",
      "[step: 974] cost: 0.10014769434928894\n",
      "[step: 975] cost: 0.09970317035913467\n",
      "[step: 976] cost: 0.0992598682641983\n",
      "[step: 977] cost: 0.09881778806447983\n",
      "[step: 978] cost: 0.09837688505649567\n",
      "[step: 979] cost: 0.0979371890425682\n",
      "[step: 980] cost: 0.09749861806631088\n",
      "학습 정확도: 0.97732425\n",
      "테스트 정확도: 0.2\n",
      "[step: 981] cost: 0.09706125408411026\n",
      "[step: 982] cost: 0.09662506729364395\n",
      "[step: 983] cost: 0.09619001299142838\n",
      "[step: 984] cost: 0.09575612097978592\n",
      "[step: 985] cost: 0.09532338380813599\n",
      "[step: 986] cost: 0.0948917418718338\n",
      "[step: 987] cost: 0.09446129202842712\n",
      "[step: 988] cost: 0.09403195977210999\n",
      "[step: 989] cost: 0.09360371530056\n",
      "[step: 990] cost: 0.09317664802074432\n",
      "학습 정확도: 0.97732425\n",
      "테스트 정확도: 0.19473684\n",
      "[step: 991] cost: 0.09275060147047043\n",
      "[step: 992] cost: 0.09232577681541443\n",
      "[step: 993] cost: 0.0919019803404808\n",
      "[step: 994] cost: 0.0914793312549591\n",
      "[step: 995] cost: 0.09105777740478516\n",
      "[step: 996] cost: 0.09063731878995895\n",
      "[step: 997] cost: 0.0902179554104805\n",
      "[step: 998] cost: 0.0897996798157692\n",
      "[step: 999] cost: 0.08938249945640564\n",
      "[step: 1000] cost: 0.08896642923355103\n",
      "학습 정확도: 0.97732425\n",
      "테스트 정확도: 0.18947369\n",
      "학습 정확도: 0.97732425\n",
      "테스트 정확도: 0.18947369\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "ohe = OneHotEncoder()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "import_data = np.genfromtxt('../data/cct_swr_calculation_rnn.csv', delimiter=',', dtype='float')\n",
    "\n",
    "x_data = import_data[:, :25]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_data = scaler.fit_transform(x_data)\n",
    "x_data = x_data[:, :25]\n",
    "\n",
    "y_data = []\n",
    "test_y = []\n",
    "\n",
    "for i in range(len(import_data)):\n",
    "    temp = []\n",
    "    temp.append(import_data[i][25])\n",
    "    y_data.append(temp)\n",
    "\n",
    "raw_y = y_data\n",
    "    \n",
    "y_data = ohe.fit_transform(y_data)\n",
    "y_data = y_data.toarray();\n",
    "\n",
    "seq_length = 5\n",
    "data_dim = 25\n",
    "hidden_dim = 50\n",
    "output_dim = 5\n",
    "learning_rate = 0.001\n",
    "iterations = 1001\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(len(x_data) - seq_length):\n",
    "    _x = x_data[i:i + seq_length]\n",
    "    _y = y_data[i+seq_length]\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "\n",
    "train_size = int(len(dataY) * 0.7)\n",
    "test_size = len(dataY) - train_size\n",
    "trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:len(dataX)])\n",
    "trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:len(dataY)])\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, output_dim])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([hidden_dim, output_dim]))\n",
    "b = tf.Variable(tf.random_normal([output_dim]))\n",
    "\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_dim)\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "outputs = outputs[-1]\n",
    "model = tf.matmul(outputs, W) + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "writer = tf.summary.FileWriter(\"./logs/rnn_logs\", sess.graph)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "for i in range(iterations):\n",
    "    _, step_cost = sess.run([optimizer, cost], feed_dict={X:trainX, Y:trainY})\n",
    "    print(\"[step: {}] cost: {}\".format(i, step_cost))\n",
    "    summary, acc = sess.run([merged, accuracy], feed_dict={X:testX, Y:testY})\n",
    "    writer.add_summary(summary, i)\n",
    "    if(i % 10 == 0):\n",
    "        print('학습 정확도:', sess.run(accuracy, feed_dict={X:trainX, Y:trainY}))\n",
    "        print('테스트 정확도:', sess.run(accuracy, feed_dict={X:testX, Y:testY}))\n",
    "\n",
    "print('학습 정확도:', sess.run(accuracy, feed_dict={X:trainX, Y:trainY}))\n",
    "print('테스트 정확도:', sess.run(accuracy, feed_dict={X:testX, Y:testY}))\n",
    "\n",
    "# for i in range(len(trainX)):\n",
    "#     a = sess.run(model, feed_dict={X:[trainX[i]]})\n",
    "#     b = sess.run(Y, feed_dict={X:[trainX[i]]})\n",
    "#     print(sess.run(tf.argmax(a, 1)), \", \", sess.run(tf.argmax(b, 1)))\n",
    "\n",
    "# cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "# outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "# Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=tf.nn.softmax)\n",
    "\n",
    "# loss = tf.reduce_sum(tf.square(Y_pred - Y))\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "# train = optimizer.minimize(loss)\n",
    "\n",
    "# targets = tf.placeholder(tf.float32, [None, 5])\n",
    "# predictions = tf.placeholder(tf.float32, [None, 5])\n",
    "# rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     sess.run(init)\n",
    "    \n",
    "#     for i in range(iterations):\n",
    "#         _, step_loss = sess.run([train, loss], feed_dict={X:trainX, Y:trainY})\n",
    "#         print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "        \n",
    "#     test_predict = sess.run(Y_pred, feed_dict={X:testX})\n",
    "#     rmse_val = sess.run(rmse, feed_dict={targets:testY, predictions:test_predict})\n",
    "#     print(\"RMSE:{}\".format(rmse_val))\n",
    "    \n",
    "#     print(sess.run(tf.argmax(test_predict, 1)))\n",
    "#     print(len(dataX), len(trainX), len(testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f93e9d1daa44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;36m230\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RNN' is not defined"
     ]
    }
   ],
   "source": [
    "seq_length = 5\n",
    "RNN\n",
    "37.36\n",
    "\n",
    "LSTM\n",
    "34.21\n",
    "\n",
    "seq_length = 4\n",
    "RNN\n",
    "230\n",
    "37.37\n",
    "\n",
    "LSTM\n",
    "180\n",
    "36.12\n",
    "\n",
    "seq_length = 3\n",
    "RNN\n",
    "41.58\n",
    "\n",
    "LSTM\n",
    "36.31\n",
    "\n",
    "seq_length = 2\n",
    "460\n",
    "37.17\n",
    "\n",
    "404\n",
    "36.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
