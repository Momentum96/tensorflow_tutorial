{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 5, 1)\n",
      "[step: 0] cost: 2.2436654567718506\n",
      "[step: 1] cost: 2.2174129486083984\n",
      "[step: 2] cost: 2.1916768550872803\n",
      "[step: 3] cost: 2.1664621829986572\n",
      "[step: 4] cost: 2.141772508621216\n",
      "[step: 5] cost: 2.1176087856292725\n",
      "[step: 6] cost: 2.0939717292785645\n",
      "[step: 7] cost: 2.070859909057617\n",
      "[step: 8] cost: 2.0482702255249023\n",
      "[step: 9] cost: 2.026198625564575\n",
      "[step: 10] cost: 2.004639148712158\n",
      "[step: 11] cost: 1.9835847616195679\n",
      "[step: 12] cost: 1.9630266427993774\n",
      "[step: 13] cost: 1.9429551362991333\n",
      "[step: 14] cost: 1.9233583211898804\n",
      "[step: 15] cost: 1.904223918914795\n",
      "[step: 16] cost: 1.8855377435684204\n",
      "[step: 17] cost: 1.8672839403152466\n",
      "[step: 18] cost: 1.8494462966918945\n",
      "[step: 19] cost: 1.8320063352584839\n",
      "[step: 20] cost: 1.8149456977844238\n",
      "[step: 21] cost: 1.7982444763183594\n",
      "[step: 22] cost: 1.7818825244903564\n",
      "[step: 23] cost: 1.7658395767211914\n",
      "[step: 24] cost: 1.7500951290130615\n",
      "[step: 25] cost: 1.7346291542053223\n",
      "[step: 26] cost: 1.7194221019744873\n",
      "[step: 27] cost: 1.7044551372528076\n",
      "[step: 28] cost: 1.6897106170654297\n",
      "[step: 29] cost: 1.6751716136932373\n",
      "[step: 30] cost: 1.6608227491378784\n",
      "[step: 31] cost: 1.646649718284607\n",
      "[step: 32] cost: 1.6326388120651245\n",
      "[step: 33] cost: 1.6187783479690552\n",
      "[step: 34] cost: 1.6050573587417603\n",
      "[step: 35] cost: 1.5914654731750488\n",
      "[step: 36] cost: 1.5779943466186523\n",
      "[step: 37] cost: 1.5646353960037231\n",
      "[step: 38] cost: 1.5513815879821777\n",
      "[step: 39] cost: 1.53822660446167\n",
      "[step: 40] cost: 1.5251649618148804\n",
      "[step: 41] cost: 1.5121917724609375\n",
      "[step: 42] cost: 1.4993029832839966\n",
      "[step: 43] cost: 1.4864952564239502\n",
      "[step: 44] cost: 1.4737657308578491\n",
      "[step: 45] cost: 1.4611124992370605\n",
      "[step: 46] cost: 1.4485341310501099\n",
      "[step: 47] cost: 1.4360297918319702\n",
      "[step: 48] cost: 1.4235997200012207\n",
      "[step: 49] cost: 1.4112441539764404\n",
      "[step: 50] cost: 1.398964762687683\n",
      "[step: 51] cost: 1.3867629766464233\n",
      "[step: 52] cost: 1.3746415376663208\n",
      "[step: 53] cost: 1.3626036643981934\n",
      "[step: 54] cost: 1.3506529331207275\n",
      "[step: 55] cost: 1.3387938737869263\n",
      "[step: 56] cost: 1.3270312547683716\n",
      "[step: 57] cost: 1.3153706789016724\n",
      "[step: 58] cost: 1.303817868232727\n",
      "[step: 59] cost: 1.292379379272461\n",
      "[step: 60] cost: 1.281062126159668\n",
      "[step: 61] cost: 1.2698732614517212\n",
      "[step: 62] cost: 1.2588202953338623\n",
      "[step: 63] cost: 1.2479115724563599\n",
      "[step: 64] cost: 1.2371547222137451\n",
      "[step: 65] cost: 1.2265583276748657\n",
      "[step: 66] cost: 1.2161310911178589\n",
      "[step: 67] cost: 1.2058814764022827\n",
      "[step: 68] cost: 1.1958184242248535\n",
      "[step: 69] cost: 1.1859503984451294\n",
      "[step: 70] cost: 1.1762864589691162\n",
      "[step: 71] cost: 1.1668355464935303\n",
      "[step: 72] cost: 1.157605767250061\n",
      "[step: 73] cost: 1.1486060619354248\n",
      "[step: 74] cost: 1.139844298362732\n",
      "[step: 75] cost: 1.1313287019729614\n",
      "[step: 76] cost: 1.1230669021606445\n",
      "[step: 77] cost: 1.1150662899017334\n",
      "[step: 78] cost: 1.1073333024978638\n",
      "[step: 79] cost: 1.0998743772506714\n",
      "[step: 80] cost: 1.0926951169967651\n",
      "[step: 81] cost: 1.085800290107727\n",
      "[step: 82] cost: 1.0791938304901123\n",
      "[step: 83] cost: 1.0728789567947388\n",
      "[step: 84] cost: 1.0668575763702393\n",
      "[step: 85] cost: 1.0611306428909302\n",
      "[step: 86] cost: 1.0556979179382324\n",
      "[step: 87] cost: 1.0505577325820923\n",
      "[step: 88] cost: 1.045707106590271\n",
      "[step: 89] cost: 1.0411417484283447\n",
      "[step: 90] cost: 1.0368560552597046\n",
      "[step: 91] cost: 1.0328426361083984\n",
      "[step: 92] cost: 1.0290933847427368\n",
      "[step: 93] cost: 1.025598168373108\n",
      "[step: 94] cost: 1.0223466157913208\n",
      "[step: 95] cost: 1.0193268060684204\n",
      "[step: 96] cost: 1.016526460647583\n",
      "[step: 97] cost: 1.0139323472976685\n",
      "[step: 98] cost: 1.0115314722061157\n",
      "[step: 99] cost: 1.0093103647232056\n",
      "[step: 100] cost: 1.0072554349899292\n",
      "[step: 101] cost: 1.0053542852401733\n",
      "[step: 102] cost: 1.0035943984985352\n",
      "[step: 103] cost: 1.0019636154174805\n",
      "[step: 104] cost: 1.0004512071609497\n",
      "[step: 105] cost: 0.9990464448928833\n",
      "[step: 106] cost: 0.9977402687072754\n",
      "[step: 107] cost: 0.9965240955352783\n",
      "[step: 108] cost: 0.9953900575637817\n",
      "[step: 109] cost: 0.9943312406539917\n",
      "[step: 110] cost: 0.9933409690856934\n",
      "[step: 111] cost: 0.992414116859436\n",
      "[step: 112] cost: 0.9915453791618347\n",
      "[step: 113] cost: 0.9907301068305969\n",
      "[step: 114] cost: 0.9899640679359436\n",
      "[step: 115] cost: 0.98924320936203\n",
      "[step: 116] cost: 0.9885641932487488\n",
      "[step: 117] cost: 0.9879236221313477\n",
      "[step: 118] cost: 0.9873179793357849\n",
      "[step: 119] cost: 0.9867449402809143\n",
      "[step: 120] cost: 0.9862015247344971\n",
      "[step: 121] cost: 0.9856853485107422\n",
      "[step: 122] cost: 0.9851943850517273\n",
      "[step: 123] cost: 0.9847264289855957\n",
      "[step: 124] cost: 0.9842798709869385\n",
      "[step: 125] cost: 0.9838534593582153\n",
      "[step: 126] cost: 0.9834456443786621\n",
      "[step: 127] cost: 0.9830556511878967\n",
      "[step: 128] cost: 0.9826824069023132\n",
      "[step: 129] cost: 0.9823254942893982\n",
      "[step: 130] cost: 0.9819839000701904\n",
      "[step: 131] cost: 0.9816572666168213\n",
      "[step: 132] cost: 0.9813451766967773\n",
      "[step: 133] cost: 0.9810469746589661\n",
      "[step: 134] cost: 0.9807621240615845\n",
      "[step: 135] cost: 0.9804901480674744\n",
      "[step: 136] cost: 0.980230450630188\n",
      "[step: 137] cost: 0.9799821376800537\n",
      "[step: 138] cost: 0.979745090007782\n",
      "[step: 139] cost: 0.9795181155204773\n",
      "[step: 140] cost: 0.9793006181716919\n",
      "[step: 141] cost: 0.979092001914978\n",
      "[step: 142] cost: 0.9788914322853088\n",
      "[step: 143] cost: 0.9786984324455261\n",
      "[step: 144] cost: 0.978512167930603\n",
      "[step: 145] cost: 0.9783322215080261\n",
      "[step: 146] cost: 0.9781578183174133\n",
      "[step: 147] cost: 0.9779885411262512\n",
      "[step: 148] cost: 0.9778239727020264\n",
      "[step: 149] cost: 0.9776635766029358\n",
      "[step: 150] cost: 0.9775071144104004\n",
      "[step: 151] cost: 0.9773541688919067\n",
      "[step: 152] cost: 0.9772043228149414\n",
      "[step: 153] cost: 0.9770574569702148\n",
      "[step: 154] cost: 0.9769131541252136\n",
      "[step: 155] cost: 0.9767714142799377\n",
      "[step: 156] cost: 0.9766316413879395\n",
      "[step: 157] cost: 0.9764939546585083\n",
      "[step: 158] cost: 0.9763579368591309\n",
      "[step: 159] cost: 0.9762234091758728\n",
      "[step: 160] cost: 0.9760904908180237\n",
      "[step: 161] cost: 0.9759588241577148\n",
      "[step: 162] cost: 0.9758281111717224\n",
      "[step: 163] cost: 0.9756984710693359\n",
      "[step: 164] cost: 0.9755696654319763\n",
      "[step: 165] cost: 0.9754414558410645\n",
      "[step: 166] cost: 0.9753140807151794\n",
      "[step: 167] cost: 0.9751874208450317\n",
      "[step: 168] cost: 0.9750611782073975\n",
      "[step: 169] cost: 0.9749355316162109\n",
      "[step: 170] cost: 0.9748103022575378\n",
      "[step: 171] cost: 0.9746853709220886\n",
      "[step: 172] cost: 0.9745608568191528\n",
      "[step: 173] cost: 0.9744368195533752\n",
      "[step: 174] cost: 0.9743128418922424\n",
      "[step: 175] cost: 0.9741894006729126\n",
      "[step: 176] cost: 0.9740660786628723\n",
      "[step: 177] cost: 0.9739429950714111\n",
      "[step: 178] cost: 0.9738198518753052\n",
      "[step: 179] cost: 0.9736969470977783\n",
      "[step: 180] cost: 0.973574161529541\n",
      "[step: 181] cost: 0.9734513163566589\n",
      "[step: 182] cost: 0.9733286499977112\n",
      "[step: 183] cost: 0.9732060432434082\n",
      "[step: 184] cost: 0.9730831980705261\n",
      "[step: 185] cost: 0.9729602932929993\n",
      "[step: 186] cost: 0.9728373885154724\n",
      "[step: 187] cost: 0.9727142453193665\n",
      "[step: 188] cost: 0.972590982913971\n",
      "[step: 189] cost: 0.9724674820899963\n",
      "[step: 190] cost: 0.972343921661377\n",
      "[step: 191] cost: 0.9722199440002441\n",
      "[step: 192] cost: 0.9720957279205322\n",
      "[step: 193] cost: 0.9719710946083069\n",
      "[step: 194] cost: 0.9718462824821472\n",
      "[step: 195] cost: 0.9717208743095398\n",
      "[step: 196] cost: 0.9715951085090637\n",
      "[step: 197] cost: 0.9714691042900085\n",
      "[step: 198] cost: 0.9713423848152161\n",
      "[step: 199] cost: 0.9712151288986206\n",
      "학습 정확도: 0.6419753\n",
      "테스트 정확도: 0.7777778\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "ohe = OneHotEncoder()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "import_data = np.genfromtxt('../data/swr_ratio_classification_input_am.csv', delimiter=',', dtype='float')\n",
    "\n",
    "x_data = []\n",
    "\n",
    "for i in range(len(import_data)):\n",
    "    temp = []\n",
    "    temp.append(import_data[i][0])\n",
    "    x_data.append(temp)\n",
    "\n",
    "y_data = []\n",
    "test_y = []\n",
    "\n",
    "for i in range(len(import_data)):\n",
    "    temp = []\n",
    "    temp.append(import_data[i][1])\n",
    "    y_data.append(temp)\n",
    "\n",
    "raw_y = y_data\n",
    "    \n",
    "y_data = ohe.fit_transform(y_data)\n",
    "y_data = y_data.toarray();\n",
    "\n",
    "seq_length = 5\n",
    "data_dim = 1\n",
    "hidden_dim = 10\n",
    "output_dim = 4\n",
    "learning_rate = 0.001\n",
    "iterations = 200\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(len(x_data) - seq_length):\n",
    "    _x = x_data[i:i + seq_length]\n",
    "    _y = y_data[i+seq_length]\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "\n",
    "train_size = int(len(dataY) * 0.7)\n",
    "test_size = len(dataY) - train_size\n",
    "trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:len(dataX)])\n",
    "trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:len(dataY)])\n",
    "\n",
    "print(np.shape(trainX))\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, output_dim])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([hidden_dim, output_dim]))\n",
    "b = tf.Variable(tf.random_normal([output_dim]))\n",
    "\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_dim)\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "outputs = outputs[-1]\n",
    "model = tf.matmul(outputs, W) + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "    _, step_cost = sess.run([optimizer, cost], feed_dict={X:trainX, Y:trainY})\n",
    "    print(\"[step: {}] cost: {}\".format(i, step_cost))\n",
    "    \n",
    "is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "print('학습 정확도:', sess.run(accuracy, feed_dict={X:trainX, Y:trainY}))\n",
    "print('테스트 정확도:', sess.run(accuracy, feed_dict={X:testX, Y:testY}))\n",
    "\n",
    "# cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "# outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "# Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=tf.nn.softmax)\n",
    "\n",
    "# loss = tf.reduce_sum(tf.square(Y_pred - Y))\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "# train = optimizer.minimize(loss)\n",
    "\n",
    "# targets = tf.placeholder(tf.float32, [None, 5])\n",
    "# predictions = tf.placeholder(tf.float32, [None, 5])\n",
    "# rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     sess.run(init)\n",
    "    \n",
    "#     for i in range(iterations):\n",
    "#         _, step_loss = sess.run([train, loss], feed_dict={X:trainX, Y:trainY})\n",
    "#         print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "        \n",
    "#     test_predict = sess.run(Y_pred, feed_dict={X:testX})\n",
    "#     rmse_val = sess.run(rmse, feed_dict={targets:testY, predictions:test_predict})\n",
    "#     print(\"RMSE:{}\".format(rmse_val))\n",
    "    \n",
    "#     print(sess.run(tf.argmax(test_predict, 1)))\n",
    "#     print(len(dataX), len(trainX), len(testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100\n",
    "64\n",
    "77\n",
    "\n",
    "200\n",
    "64\n",
    "77"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
